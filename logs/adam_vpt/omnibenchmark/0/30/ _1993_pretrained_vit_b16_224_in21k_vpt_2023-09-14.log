2023-09-14 09:45:29,397 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 09:45:29,398 [trainer.py] => prefix:  
2023-09-14 09:45:29,398 [trainer.py] => dataset: omnibenchmark
2023-09-14 09:45:29,399 [trainer.py] => memory_size: 0
2023-09-14 09:45:29,399 [trainer.py] => memory_per_class: 0
2023-09-14 09:45:29,399 [trainer.py] => fixed_memory: False
2023-09-14 09:45:29,399 [trainer.py] => shuffle: True
2023-09-14 09:45:29,400 [trainer.py] => init_cls: 30
2023-09-14 09:45:29,400 [trainer.py] => increment: 30
2023-09-14 09:45:29,400 [trainer.py] => model_name: adam_vpt
2023-09-14 09:45:29,401 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 09:45:29,401 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 09:45:29,401 [trainer.py] => visible_device: 2,3
2023-09-14 09:45:29,401 [trainer.py] => seed: 1993
2023-09-14 09:45:29,402 [trainer.py] => tuned_epoch: 20
2023-09-14 09:45:29,402 [trainer.py] => init_lr: 0.02
2023-09-14 09:45:29,408 [trainer.py] => batch_size: 96
2023-09-14 09:45:29,408 [trainer.py] => weight_decay: 0.0005
2023-09-14 09:45:29,409 [trainer.py] => min_lr: 0
2023-09-14 09:45:29,409 [trainer.py] => optimizer: sgd
2023-09-14 09:45:29,409 [trainer.py] => vpt_type: deep
2023-09-14 09:45:29,410 [trainer.py] => prompt_token_num: 5
2023-09-14 09:45:29,410 [trainer.py] => use_A: False
2023-09-14 09:45:30,569 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 09:45:35,664 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 09:45:36,247 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 09:46:05,803 [trainer.py] => All params: 85844736
2023-09-14 09:46:05,809 [trainer.py] => Trainable params: 46080
2023-09-14 09:46:07,143 [adam_vpt.py] => Learning on 0-30
2023-09-14 09:48:10,262 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 2.879, Train_accy 39.39
2023-09-14 09:49:26,007 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 1.099, Train_accy 72.01, Test_accy 81.67
2023-09-14 09:50:36,341 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.795, Train_accy 74.95, Test_accy 88.00
2023-09-14 09:50:57,467 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 09:50:57,468 [trainer.py] => prefix:  
2023-09-14 09:50:57,469 [trainer.py] => dataset: omnibenchmark
2023-09-14 09:50:57,470 [trainer.py] => memory_size: 0
2023-09-14 09:50:57,470 [trainer.py] => memory_per_class: 0
2023-09-14 09:50:57,471 [trainer.py] => fixed_memory: False
2023-09-14 09:50:57,471 [trainer.py] => shuffle: True
2023-09-14 09:50:57,472 [trainer.py] => init_cls: 30
2023-09-14 09:50:57,472 [trainer.py] => increment: 30
2023-09-14 09:50:57,473 [trainer.py] => model_name: adam_vpt
2023-09-14 09:50:57,473 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 09:50:57,474 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 09:50:57,474 [trainer.py] => visible_device: 2,3
2023-09-14 09:50:57,475 [trainer.py] => seed: 1993
2023-09-14 09:50:57,475 [trainer.py] => tuned_epoch: 20
2023-09-14 09:50:57,476 [trainer.py] => init_lr: 0.02
2023-09-14 09:50:57,476 [trainer.py] => batch_size: 96
2023-09-14 09:50:57,477 [trainer.py] => weight_decay: 0.0005
2023-09-14 09:50:57,477 [trainer.py] => min_lr: 0
2023-09-14 09:50:57,478 [trainer.py] => optimizer: sgd
2023-09-14 09:50:57,479 [trainer.py] => vpt_type: deep
2023-09-14 09:50:57,479 [trainer.py] => prompt_token_num: 5
2023-09-14 09:50:57,480 [trainer.py] => use_A: False
2023-09-14 09:51:00,709 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 09:51:06,887 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 09:51:07,264 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 09:51:23,461 [trainer.py] => All params: 85844736
2023-09-14 09:51:23,484 [trainer.py] => Trainable params: 46080
2023-09-14 09:51:24,795 [adam_vpt.py] => Learning on 0-30
2023-09-14 09:57:16,272 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 09:57:16,273 [trainer.py] => prefix:  
2023-09-14 09:57:16,273 [trainer.py] => dataset: omnibenchmark
2023-09-14 09:57:16,273 [trainer.py] => memory_size: 0
2023-09-14 09:57:16,273 [trainer.py] => memory_per_class: 0
2023-09-14 09:57:16,274 [trainer.py] => fixed_memory: False
2023-09-14 09:57:16,274 [trainer.py] => shuffle: True
2023-09-14 09:57:16,274 [trainer.py] => init_cls: 30
2023-09-14 09:57:16,274 [trainer.py] => increment: 30
2023-09-14 09:57:16,275 [trainer.py] => model_name: adam_vpt
2023-09-14 09:57:16,275 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 09:57:16,275 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 09:57:16,276 [trainer.py] => visible_device: 2,3
2023-09-14 09:57:16,276 [trainer.py] => seed: 1993
2023-09-14 09:57:16,276 [trainer.py] => tuned_epoch: 20
2023-09-14 09:57:16,276 [trainer.py] => init_lr: 0.02
2023-09-14 09:57:16,277 [trainer.py] => batch_size: 96
2023-09-14 09:57:16,277 [trainer.py] => weight_decay: 0.0005
2023-09-14 09:57:16,277 [trainer.py] => min_lr: 0
2023-09-14 09:57:16,277 [trainer.py] => optimizer: sgd
2023-09-14 09:57:16,278 [trainer.py] => vpt_type: deep
2023-09-14 09:57:16,278 [trainer.py] => prompt_token_num: 16
2023-09-14 09:57:16,278 [trainer.py] => use_A: False
2023-09-14 09:57:17,504 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 09:57:22,501 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 09:57:22,864 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 09:57:32,871 [trainer.py] => All params: 85946112
2023-09-14 09:57:32,873 [trainer.py] => Trainable params: 147456
2023-09-14 09:57:34,132 [adam_vpt.py] => Learning on 0-30
2023-09-14 09:59:08,786 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 2.996, Train_accy 26.96
2023-09-14 10:00:21,129 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 1.283, Train_accy 69.15, Test_accy 79.67
2023-09-14 10:01:31,799 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.827, Train_accy 74.42, Test_accy 86.50
2023-09-14 10:02:42,173 [adam_vpt.py] => Task 0, Epoch 4/20 => Loss 0.732, Train_accy 76.76, Test_accy 85.67
2023-09-14 10:03:52,426 [adam_vpt.py] => Task 0, Epoch 5/20 => Loss 0.688, Train_accy 77.48, Test_accy 87.00
2023-09-14 10:04:56,331 [adam_vpt.py] => Task 0, Epoch 6/20 => Loss 0.670, Train_accy 78.16
2023-09-14 10:06:06,738 [adam_vpt.py] => Task 0, Epoch 7/20 => Loss 0.649, Train_accy 79.19, Test_accy 89.00
2023-09-14 10:07:16,673 [adam_vpt.py] => Task 0, Epoch 8/20 => Loss 0.637, Train_accy 79.56, Test_accy 89.83
2023-09-14 10:08:26,286 [adam_vpt.py] => Task 0, Epoch 9/20 => Loss 0.616, Train_accy 80.85, Test_accy 90.17
2023-09-14 10:09:36,107 [adam_vpt.py] => Task 0, Epoch 10/20 => Loss 0.587, Train_accy 81.36, Test_accy 91.00
2023-09-14 10:10:40,010 [adam_vpt.py] => Task 0, Epoch 11/20 => Loss 0.581, Train_accy 81.86
2023-09-14 10:11:47,866 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 10:11:47,867 [trainer.py] => prefix:  
2023-09-14 10:11:47,867 [trainer.py] => dataset: omnibenchmark
2023-09-14 10:11:47,867 [trainer.py] => memory_size: 0
2023-09-14 10:11:47,868 [trainer.py] => memory_per_class: 0
2023-09-14 10:11:47,868 [trainer.py] => fixed_memory: False
2023-09-14 10:11:47,868 [trainer.py] => shuffle: True
2023-09-14 10:11:47,869 [trainer.py] => init_cls: 30
2023-09-14 10:11:47,869 [trainer.py] => increment: 30
2023-09-14 10:11:47,869 [trainer.py] => model_name: adam_vpt
2023-09-14 10:11:47,869 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 10:11:47,870 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 10:11:47,870 [trainer.py] => visible_device: 2,3
2023-09-14 10:11:47,870 [trainer.py] => seed: 1993
2023-09-14 10:11:47,871 [trainer.py] => tuned_epoch: 20
2023-09-14 10:11:47,871 [trainer.py] => init_lr: 0.02
2023-09-14 10:11:47,871 [trainer.py] => batch_size: 96
2023-09-14 10:11:47,871 [trainer.py] => weight_decay: 0.0005
2023-09-14 10:11:47,872 [trainer.py] => min_lr: 0
2023-09-14 10:11:47,872 [trainer.py] => optimizer: sgd
2023-09-14 10:11:47,872 [trainer.py] => vpt_type: deep
2023-09-14 10:11:47,873 [trainer.py] => prompt_token_num: 16
2023-09-14 10:11:47,873 [trainer.py] => use_A: False
2023-09-14 10:11:49,052 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 10:11:52,736 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 10:11:53,145 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 10:12:04,479 [trainer.py] => All params: 85946112
2023-09-14 10:12:04,480 [trainer.py] => Trainable params: 147456
2023-09-14 10:12:05,863 [adam_vpt.py] => Learning on 0-30
2023-09-14 10:13:11,206 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 2.773, Train_accy 39.68
2023-09-14 10:14:20,993 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 1.150, Train_accy 69.91, Test_accy 79.83
2023-09-14 10:15:30,014 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.818, Train_accy 73.95, Test_accy 86.33
2023-09-14 10:16:39,043 [adam_vpt.py] => Task 0, Epoch 4/20 => Loss 0.748, Train_accy 75.85, Test_accy 84.17
2023-09-14 10:17:48,005 [adam_vpt.py] => Task 0, Epoch 5/20 => Loss 0.685, Train_accy 77.71, Test_accy 84.50
2023-09-14 10:18:51,797 [adam_vpt.py] => Task 0, Epoch 6/20 => Loss 0.682, Train_accy 77.78
2023-09-14 10:20:01,770 [adam_vpt.py] => Task 0, Epoch 7/20 => Loss 0.659, Train_accy 78.85, Test_accy 86.33
2023-09-14 10:21:10,017 [adam_vpt.py] => Task 0, Epoch 8/20 => Loss 0.648, Train_accy 79.04, Test_accy 88.67
2023-09-14 10:22:16,409 [adam_vpt.py] => Task 0, Epoch 9/20 => Loss 0.623, Train_accy 80.09, Test_accy 88.00
2023-09-14 10:23:22,786 [adam_vpt.py] => Task 0, Epoch 10/20 => Loss 0.617, Train_accy 80.30, Test_accy 90.83
2023-09-14 10:24:24,507 [adam_vpt.py] => Task 0, Epoch 11/20 => Loss 0.603, Train_accy 80.75
2023-09-14 10:25:30,952 [adam_vpt.py] => Task 0, Epoch 12/20 => Loss 0.582, Train_accy 81.57, Test_accy 89.17
2023-09-14 10:26:37,520 [adam_vpt.py] => Task 0, Epoch 13/20 => Loss 0.586, Train_accy 81.19, Test_accy 91.50
2023-09-14 10:27:43,855 [adam_vpt.py] => Task 0, Epoch 14/20 => Loss 0.580, Train_accy 81.64, Test_accy 91.00
2023-09-14 10:28:50,213 [adam_vpt.py] => Task 0, Epoch 15/20 => Loss 0.565, Train_accy 82.09, Test_accy 89.67
2023-09-14 10:29:51,950 [adam_vpt.py] => Task 0, Epoch 16/20 => Loss 0.579, Train_accy 81.73
2023-09-14 10:30:58,711 [adam_vpt.py] => Task 0, Epoch 17/20 => Loss 0.569, Train_accy 82.25, Test_accy 90.67
2023-09-14 10:32:05,226 [adam_vpt.py] => Task 0, Epoch 18/20 => Loss 0.554, Train_accy 82.39, Test_accy 90.83
2023-09-14 10:33:11,776 [adam_vpt.py] => Task 0, Epoch 19/20 => Loss 0.560, Train_accy 82.32, Test_accy 90.50
2023-09-14 10:34:18,769 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.564, Train_accy 82.59, Test_accy 90.67
2023-09-14 10:34:20,750 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 10:34:21,079 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 10:34:31,856 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 10:34:32,130 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 10:35:55,570 [trainer.py] => No NME accuracy.
2023-09-14 10:35:55,570 [trainer.py] => CNN: {'total': 89.5, '00-09': 91.5, '10-19': 91.0, '20-29': 86.0, 'old': 0, 'new': 89.5}
2023-09-14 10:35:55,571 [trainer.py] => CNN top1 curve: [89.5]
2023-09-14 10:35:55,571 [trainer.py] => CNN top5 curve: [98.83]

2023-09-14 10:35:55,572 [trainer.py] => Average Accuracy (CNN): 89.5
2023-09-14 10:35:55,574 [trainer.py] => All params: 171790849
2023-09-14 10:35:55,576 [trainer.py] => Trainable params: 85992193
2023-09-14 10:35:55,582 [adam_vpt.py] => Learning on 30-60
2023-09-14 10:38:19,718 [trainer.py] => No NME accuracy.
2023-09-14 10:38:19,718 [trainer.py] => CNN: {'total': 88.57, '00-09': 85.5, '10-19': 90.5, '20-29': 83.0, '30-39': 89.5, '40-49': 88.0, '50-59': 94.97, 'old': 86.33, 'new': 90.82}
2023-09-14 10:38:19,719 [trainer.py] => CNN top1 curve: [89.5, 88.57]
2023-09-14 10:38:19,719 [trainer.py] => CNN top5 curve: [98.83, 98.42]

2023-09-14 10:38:19,719 [trainer.py] => Average Accuracy (CNN): 89.035
2023-09-14 10:38:19,722 [trainer.py] => All params: 171836929
2023-09-14 10:38:19,725 [trainer.py] => Trainable params: 86038273
2023-09-14 10:38:19,730 [adam_vpt.py] => Learning on 60-90
2023-09-14 10:40:41,871 [trainer.py] => No NME accuracy.
2023-09-14 10:40:41,872 [trainer.py] => CNN: {'total': 85.48, '00-09': 83.0, '10-19': 85.0, '20-29': 80.0, '30-39': 87.0, '40-49': 86.0, '50-59': 92.96, '60-69': 92.0, '70-79': 87.0, '80-89': 76.38, 'old': 85.65, 'new': 85.14}
2023-09-14 10:40:41,872 [trainer.py] => CNN top1 curve: [89.5, 88.57, 85.48]
2023-09-14 10:40:41,872 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.27]

2023-09-14 10:40:41,873 [trainer.py] => Average Accuracy (CNN): 87.85000000000001
2023-09-14 10:40:41,876 [trainer.py] => All params: 171883009
2023-09-14 10:40:41,878 [trainer.py] => Trainable params: 86084353
2023-09-14 10:40:41,882 [adam_vpt.py] => Learning on 90-120
2023-09-14 10:43:43,044 [trainer.py] => No NME accuracy.
2023-09-14 10:43:43,045 [trainer.py] => CNN: {'total': 81.67, '00-09': 80.0, '10-19': 80.5, '20-29': 79.5, '30-39': 86.5, '40-49': 84.0, '50-59': 86.93, '60-69': 87.5, '70-79': 84.0, '80-89': 74.37, '90-99': 79.9, '100-109': 79.9, '110-119': 76.88, 'old': 82.59, 'new': 78.89}
2023-09-14 10:43:43,045 [trainer.py] => CNN top1 curve: [89.5, 88.57, 85.48, 81.67]
2023-09-14 10:43:43,046 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.27, 96.28]

2023-09-14 10:43:43,046 [trainer.py] => Average Accuracy (CNN): 86.305
2023-09-14 10:43:43,048 [trainer.py] => All params: 171929089
2023-09-14 10:43:43,051 [trainer.py] => Trainable params: 86130433
2023-09-14 10:43:43,059 [adam_vpt.py] => Learning on 120-150
2023-09-14 10:46:21,706 [trainer.py] => No NME accuracy.
2023-09-14 10:46:21,710 [trainer.py] => CNN: {'total': 79.36, '00-09': 77.5, '10-19': 80.5, '20-29': 78.0, '30-39': 86.0, '40-49': 80.5, '50-59': 80.9, '60-69': 83.0, '70-79': 83.5, '80-89': 72.36, '90-99': 78.39, '100-109': 76.88, '110-119': 71.86, '120-129': 82.0, '130-139': 79.0, '140-149': 79.9, 'old': 79.12, 'new': 80.3}
2023-09-14 10:46:21,719 [trainer.py] => CNN top1 curve: [89.5, 88.57, 85.48, 81.67, 79.36]
2023-09-14 10:46:21,719 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.27, 96.28, 94.76]

2023-09-14 10:46:21,720 [trainer.py] => Average Accuracy (CNN): 84.91600000000001
2023-09-14 10:46:21,723 [trainer.py] => All params: 171975169
2023-09-14 10:46:21,727 [trainer.py] => Trainable params: 86176513
2023-09-14 10:46:21,740 [adam_vpt.py] => Learning on 150-180
2023-09-14 10:49:02,979 [trainer.py] => No NME accuracy.
2023-09-14 10:49:02,980 [trainer.py] => CNN: {'total': 76.78, '00-09': 75.5, '10-19': 76.5, '20-29': 75.0, '30-39': 83.5, '40-49': 77.5, '50-59': 79.4, '60-69': 81.5, '70-79': 80.5, '80-89': 69.85, '90-99': 78.39, '100-109': 74.37, '110-119': 71.36, '120-129': 81.5, '130-139': 78.0, '140-149': 78.89, '150-159': 78.89, '160-169': 74.87, '170-179': 66.5, 'old': 77.45, 'new': 73.41}
2023-09-14 10:49:02,981 [trainer.py] => CNN top1 curve: [89.5, 88.57, 85.48, 81.67, 79.36, 76.78]
2023-09-14 10:49:02,981 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.27, 96.28, 94.76, 93.49]

2023-09-14 10:49:02,982 [trainer.py] => Average Accuracy (CNN): 83.56
2023-09-14 10:49:02,984 [trainer.py] => All params: 172021249
2023-09-14 10:49:02,986 [trainer.py] => Trainable params: 86222593
2023-09-14 10:49:02,995 [adam_vpt.py] => Learning on 180-210
2023-09-14 10:51:52,418 [trainer.py] => No NME accuracy.
2023-09-14 10:51:52,508 [trainer.py] => CNN: {'total': 75.65, '00-09': 75.0, '10-19': 76.5, '20-29': 74.5, '30-39': 83.5, '40-49': 76.5, '50-59': 78.89, '60-69': 78.0, '70-79': 80.5, '80-89': 69.85, '90-99': 75.88, '100-109': 73.87, '110-119': 66.83, '120-129': 81.5, '130-139': 76.5, '140-149': 78.89, '150-159': 75.88, '160-169': 73.87, '170-179': 65.5, '180-189': 72.22, '190-199': 83.5, '200-209': 70.85, 'old': 75.67, 'new': 75.54}
2023-09-14 10:51:52,548 [trainer.py] => CNN top1 curve: [89.5, 88.57, 85.48, 81.67, 79.36, 76.78, 75.65]
2023-09-14 10:51:52,574 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.27, 96.28, 94.76, 93.49, 92.93]

2023-09-14 10:51:52,584 [trainer.py] => Average Accuracy (CNN): 82.42999999999999
2023-09-14 10:51:52,590 [trainer.py] => All params: 172067329
2023-09-14 10:51:52,593 [trainer.py] => Trainable params: 86268673
2023-09-14 10:51:52,600 [adam_vpt.py] => Learning on 210-240
2023-09-14 10:54:51,286 [trainer.py] => No NME accuracy.
2023-09-14 10:54:51,287 [trainer.py] => CNN: {'total': 74.08, '00-09': 74.5, '10-19': 74.0, '20-29': 73.5, '30-39': 83.5, '40-49': 75.0, '50-59': 78.89, '60-69': 77.5, '70-79': 80.5, '80-89': 69.85, '90-99': 75.88, '100-109': 66.33, '110-119': 66.33, '120-129': 79.0, '130-139': 76.5, '140-149': 77.89, '150-159': 75.38, '160-169': 73.87, '170-179': 62.5, '180-189': 72.22, '190-199': 83.0, '200-209': 66.83, '210-219': 49.5, '220-229': 85.93, '230-239': 79.5, 'old': 74.43, 'new': 71.62}
2023-09-14 10:54:51,287 [trainer.py] => CNN top1 curve: [89.5, 88.57, 85.48, 81.67, 79.36, 76.78, 75.65, 74.08]
2023-09-14 10:54:51,288 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.27, 96.28, 94.76, 93.49, 92.93, 91.94]

2023-09-14 10:54:51,288 [trainer.py] => Average Accuracy (CNN): 81.38625
2023-09-14 10:54:51,292 [trainer.py] => All params: 172113409
2023-09-14 10:54:51,295 [trainer.py] => Trainable params: 86314753
2023-09-14 10:54:51,307 [adam_vpt.py] => Learning on 240-270
2023-09-14 10:57:53,961 [trainer.py] => No NME accuracy.
2023-09-14 10:57:53,962 [trainer.py] => CNN: {'total': 73.16, '00-09': 74.0, '10-19': 73.5, '20-29': 73.5, '30-39': 82.5, '40-49': 75.0, '50-59': 78.89, '60-69': 75.5, '70-79': 80.0, '80-89': 69.35, '90-99': 74.37, '100-109': 65.83, '110-119': 63.32, '120-129': 74.0, '130-139': 75.0, '140-149': 73.37, '150-159': 72.36, '160-169': 73.87, '170-179': 62.5, '180-189': 72.22, '190-199': 81.0, '200-209': 64.32, '210-219': 49.5, '220-229': 85.93, '230-239': 79.5, '240-249': 69.0, '250-259': 74.87, '260-269': 82.0, 'old': 72.89, 'new': 75.29}
2023-09-14 10:57:53,963 [trainer.py] => CNN top1 curve: [89.5, 88.57, 85.48, 81.67, 79.36, 76.78, 75.65, 74.08, 73.16]
2023-09-14 10:57:53,965 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.27, 96.28, 94.76, 93.49, 92.93, 91.94, 91.96]

2023-09-14 10:57:53,966 [trainer.py] => Average Accuracy (CNN): 80.47222222222223
2023-09-14 10:57:53,970 [trainer.py] => All params: 172159489
2023-09-14 10:57:53,973 [trainer.py] => Trainable params: 86360833
2023-09-14 10:57:53,988 [adam_vpt.py] => Learning on 270-300
2023-09-14 11:00:47,312 [trainer.py] => No NME accuracy.
2023-09-14 11:00:47,313 [trainer.py] => CNN: {'total': 73.12, '00-09': 73.5, '10-19': 73.5, '20-29': 73.0, '30-39': 82.5, '40-49': 72.0, '50-59': 77.89, '60-69': 75.0, '70-79': 79.5, '80-89': 68.34, '90-99': 70.85, '100-109': 65.33, '110-119': 62.81, '120-129': 74.0, '130-139': 74.5, '140-149': 73.37, '150-159': 72.36, '160-169': 71.86, '170-179': 61.0, '180-189': 71.21, '190-199': 80.5, '200-209': 62.31, '210-219': 49.0, '220-229': 85.93, '230-239': 79.5, '240-249': 66.5, '250-259': 74.37, '260-269': 81.0, '270-279': 79.4, '280-289': 82.0, '290-299': 80.4, 'old': 72.29, 'new': 80.6}
2023-09-14 11:00:47,313 [trainer.py] => CNN top1 curve: [89.5, 88.57, 85.48, 81.67, 79.36, 76.78, 75.65, 74.08, 73.16, 73.12]
2023-09-14 11:00:47,314 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.27, 96.28, 94.76, 93.49, 92.93, 91.94, 91.96, 91.71]

2023-09-14 11:00:47,314 [trainer.py] => Average Accuracy (CNN): 79.737
2023-09-14 11:03:20,870 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 11:03:20,871 [trainer.py] => prefix:  
2023-09-14 11:03:20,871 [trainer.py] => dataset: omnibenchmark
2023-09-14 11:03:20,871 [trainer.py] => memory_size: 0
2023-09-14 11:03:20,872 [trainer.py] => memory_per_class: 0
2023-09-14 11:03:20,872 [trainer.py] => fixed_memory: False
2023-09-14 11:03:20,872 [trainer.py] => shuffle: True
2023-09-14 11:03:20,872 [trainer.py] => init_cls: 30
2023-09-14 11:03:20,872 [trainer.py] => increment: 30
2023-09-14 11:03:20,873 [trainer.py] => model_name: adam_vpt
2023-09-14 11:03:20,873 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 11:03:20,873 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 11:03:20,874 [trainer.py] => visible_device: 2,3
2023-09-14 11:03:20,874 [trainer.py] => seed: 1993
2023-09-14 11:03:20,874 [trainer.py] => tuned_epoch: 20
2023-09-14 11:03:20,874 [trainer.py] => init_lr: 0.02
2023-09-14 11:03:20,875 [trainer.py] => batch_size: 96
2023-09-14 11:03:20,875 [trainer.py] => weight_decay: 0.0005
2023-09-14 11:03:20,875 [trainer.py] => min_lr: 0
2023-09-14 11:03:20,875 [trainer.py] => optimizer: sgd
2023-09-14 11:03:20,876 [trainer.py] => vpt_type: deep
2023-09-14 11:03:20,876 [trainer.py] => prompt_token_num: 5
2023-09-14 11:03:20,876 [trainer.py] => use_A: False
2023-09-14 11:03:21,946 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 11:03:26,861 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 11:03:27,245 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 11:03:48,661 [trainer.py] => All params: 85844736
2023-09-14 11:03:48,663 [trainer.py] => Trainable params: 46080
2023-09-14 11:03:50,189 [adam_vpt.py] => Learning on 0-30
2023-09-14 11:05:56,525 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 2.888, Train_accy 35.76
2023-09-14 11:07:02,731 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 1.133, Train_accy 71.33, Test_accy 81.67
2023-09-14 11:08:08,619 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.813, Train_accy 74.72, Test_accy 87.83
2023-09-14 11:09:15,726 [adam_vpt.py] => Task 0, Epoch 4/20 => Loss 0.745, Train_accy 76.15, Test_accy 86.33
2023-09-14 11:10:22,470 [adam_vpt.py] => Task 0, Epoch 5/20 => Loss 0.695, Train_accy 77.76, Test_accy 88.00
2023-09-14 11:11:22,220 [adam_vpt.py] => Task 0, Epoch 6/20 => Loss 0.666, Train_accy 78.27
2023-09-14 11:12:26,312 [adam_vpt.py] => Task 0, Epoch 7/20 => Loss 0.649, Train_accy 79.29, Test_accy 88.83
2023-09-14 11:13:30,635 [adam_vpt.py] => Task 0, Epoch 8/20 => Loss 0.641, Train_accy 79.78, Test_accy 89.33
2023-09-14 11:14:35,044 [adam_vpt.py] => Task 0, Epoch 9/20 => Loss 0.623, Train_accy 80.74, Test_accy 91.00
2023-09-14 11:15:39,322 [adam_vpt.py] => Task 0, Epoch 10/20 => Loss 0.598, Train_accy 81.30, Test_accy 90.67
2023-09-14 11:16:38,928 [adam_vpt.py] => Task 0, Epoch 11/20 => Loss 0.604, Train_accy 81.32
2023-09-14 11:17:45,446 [adam_vpt.py] => Task 0, Epoch 12/20 => Loss 0.594, Train_accy 81.22, Test_accy 90.00
2023-09-14 11:18:51,886 [adam_vpt.py] => Task 0, Epoch 13/20 => Loss 0.584, Train_accy 81.75, Test_accy 92.67
2023-09-14 11:19:58,000 [adam_vpt.py] => Task 0, Epoch 14/20 => Loss 0.578, Train_accy 81.85, Test_accy 92.17
2023-09-14 11:21:04,394 [adam_vpt.py] => Task 0, Epoch 15/20 => Loss 0.566, Train_accy 81.90, Test_accy 91.83
2023-09-14 11:22:04,879 [adam_vpt.py] => Task 0, Epoch 16/20 => Loss 0.577, Train_accy 82.31
2023-09-14 11:23:10,900 [adam_vpt.py] => Task 0, Epoch 17/20 => Loss 0.574, Train_accy 81.98, Test_accy 92.17
2023-09-14 11:24:17,480 [adam_vpt.py] => Task 0, Epoch 18/20 => Loss 0.555, Train_accy 82.44, Test_accy 92.67
2023-09-14 11:25:24,246 [adam_vpt.py] => Task 0, Epoch 19/20 => Loss 0.573, Train_accy 81.89, Test_accy 92.33
2023-09-14 11:26:30,949 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.562, Train_accy 82.62, Test_accy 92.67
2023-09-14 11:26:34,382 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 11:26:34,836 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 11:27:06,235 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 11:27:06,527 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 11:28:55,781 [trainer.py] => No NME accuracy.
2023-09-14 11:28:55,796 [trainer.py] => CNN: {'total': 90.0, '00-09': 91.0, '10-19': 91.0, '20-29': 88.0, 'old': 0, 'new': 90.0}
2023-09-14 11:28:55,801 [trainer.py] => CNN top1 curve: [90.0]
2023-09-14 11:28:55,801 [trainer.py] => CNN top5 curve: [98.83]

2023-09-14 11:28:55,802 [trainer.py] => Average Accuracy (CNN): 90.0
2023-09-14 11:28:55,811 [trainer.py] => All params: 171689473
2023-09-14 11:28:55,813 [trainer.py] => Trainable params: 85890817
2023-09-14 11:28:55,819 [adam_vpt.py] => Learning on 30-60
2023-09-14 11:31:11,995 [trainer.py] => No NME accuracy.
2023-09-14 11:31:11,996 [trainer.py] => CNN: {'total': 88.74, '00-09': 85.5, '10-19': 90.0, '20-29': 85.0, '30-39': 90.5, '40-49': 88.0, '50-59': 93.47, 'old': 86.83, 'new': 90.65}
2023-09-14 11:31:11,996 [trainer.py] => CNN top1 curve: [90.0, 88.74]
2023-09-14 11:31:11,996 [trainer.py] => CNN top5 curve: [98.83, 98.42]

2023-09-14 11:31:11,997 [trainer.py] => Average Accuracy (CNN): 89.37
2023-09-14 11:31:11,999 [trainer.py] => All params: 171735553
2023-09-14 11:31:12,003 [trainer.py] => Trainable params: 85936897
2023-09-14 11:31:12,017 [adam_vpt.py] => Learning on 60-90
2023-09-14 11:33:24,255 [trainer.py] => No NME accuracy.
2023-09-14 11:33:24,256 [trainer.py] => CNN: {'total': 86.32, '00-09': 83.5, '10-19': 85.0, '20-29': 81.5, '30-39': 87.5, '40-49': 85.5, '50-59': 91.96, '60-69': 94.0, '70-79': 88.0, '80-89': 79.9, 'old': 85.82, 'new': 87.31}
2023-09-14 11:33:24,256 [trainer.py] => CNN top1 curve: [90.0, 88.74, 86.32]
2023-09-14 11:33:24,256 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.55]

2023-09-14 11:33:24,256 [trainer.py] => Average Accuracy (CNN): 88.35333333333334
2023-09-14 11:33:24,260 [trainer.py] => All params: 171781633
2023-09-14 11:33:24,263 [trainer.py] => Trainable params: 85982977
2023-09-14 11:33:24,271 [adam_vpt.py] => Learning on 90-120
2023-09-14 11:35:45,176 [trainer.py] => No NME accuracy.
2023-09-14 11:35:45,177 [trainer.py] => CNN: {'total': 82.46, '00-09': 80.5, '10-19': 81.5, '20-29': 80.5, '30-39': 86.5, '40-49': 84.0, '50-59': 85.43, '60-69': 88.5, '70-79': 86.0, '80-89': 76.88, '90-99': 79.9, '100-109': 80.4, '110-119': 79.4, 'old': 83.31, 'new': 79.9}
2023-09-14 11:35:45,178 [trainer.py] => CNN top1 curve: [90.0, 88.74, 86.32, 82.46]
2023-09-14 11:35:45,178 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.55, 96.49]

2023-09-14 11:35:45,178 [trainer.py] => Average Accuracy (CNN): 86.88
2023-09-14 11:35:45,181 [trainer.py] => All params: 171827713
2023-09-14 11:35:45,184 [trainer.py] => Trainable params: 86029057
2023-09-14 11:35:45,194 [adam_vpt.py] => Learning on 120-150
2023-09-14 11:38:08,127 [trainer.py] => No NME accuracy.
2023-09-14 11:38:08,128 [trainer.py] => CNN: {'total': 80.59, '00-09': 78.0, '10-19': 81.5, '20-29': 79.5, '30-39': 86.0, '40-49': 82.5, '50-59': 80.9, '60-69': 84.5, '70-79': 84.5, '80-89': 75.38, '90-99': 78.89, '100-109': 76.88, '110-119': 73.87, '120-129': 83.5, '130-139': 80.5, '140-149': 82.41, 'old': 80.21, 'new': 82.14}
2023-09-14 11:38:08,128 [trainer.py] => CNN top1 curve: [90.0, 88.74, 86.32, 82.46, 80.59]
2023-09-14 11:38:08,128 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.55, 96.49, 95.32]

2023-09-14 11:38:08,129 [trainer.py] => Average Accuracy (CNN): 85.622
2023-09-14 11:38:08,132 [trainer.py] => All params: 171873793
2023-09-14 11:38:08,134 [trainer.py] => Trainable params: 86075137
2023-09-14 11:38:08,141 [adam_vpt.py] => Learning on 150-180
2023-09-14 11:40:32,964 [trainer.py] => No NME accuracy.
2023-09-14 11:40:32,965 [trainer.py] => CNN: {'total': 77.95, '00-09': 75.5, '10-19': 78.0, '20-29': 76.5, '30-39': 84.0, '40-49': 79.5, '50-59': 79.4, '60-69': 83.5, '70-79': 81.5, '80-89': 72.86, '90-99': 78.89, '100-109': 73.37, '110-119': 72.86, '120-129': 83.5, '130-139': 80.0, '140-149': 81.41, '150-159': 79.9, '160-169': 73.87, '170-179': 68.5, 'old': 78.72, 'new': 74.08}
2023-09-14 11:40:32,965 [trainer.py] => CNN top1 curve: [90.0, 88.74, 86.32, 82.46, 80.59, 77.95]
2023-09-14 11:40:32,965 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.55, 96.49, 95.32, 94.07]

2023-09-14 11:40:32,966 [trainer.py] => Average Accuracy (CNN): 84.34333333333333
2023-09-14 11:40:32,969 [trainer.py] => All params: 171919873
2023-09-14 11:40:32,973 [trainer.py] => Trainable params: 86121217
2023-09-14 11:40:32,986 [adam_vpt.py] => Learning on 180-210
2023-09-14 11:43:06,880 [trainer.py] => No NME accuracy.
2023-09-14 11:43:06,881 [trainer.py] => CNN: {'total': 76.94, '00-09': 74.5, '10-19': 78.0, '20-29': 75.5, '30-39': 84.0, '40-49': 78.5, '50-59': 78.89, '60-69': 81.5, '70-79': 81.0, '80-89': 71.86, '90-99': 76.38, '100-109': 72.86, '110-119': 69.85, '120-129': 83.5, '130-139': 78.5, '140-149': 81.41, '150-159': 76.88, '160-169': 73.37, '170-179': 68.0, '180-189': 73.74, '190-199': 84.5, '200-209': 72.86, 'old': 76.92, 'new': 77.05}
2023-09-14 11:43:06,881 [trainer.py] => CNN top1 curve: [90.0, 88.74, 86.32, 82.46, 80.59, 77.95, 76.94]
2023-09-14 11:43:06,884 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.55, 96.49, 95.32, 94.07, 93.58]

2023-09-14 11:43:06,885 [trainer.py] => Average Accuracy (CNN): 83.28571428571429
2023-09-14 11:43:06,887 [trainer.py] => All params: 171965953
2023-09-14 11:43:06,889 [trainer.py] => Trainable params: 86167297
2023-09-14 11:43:06,897 [adam_vpt.py] => Learning on 210-240
2023-09-14 11:45:50,324 [trainer.py] => No NME accuracy.
2023-09-14 11:45:50,325 [trainer.py] => CNN: {'total': 75.46, '00-09': 74.0, '10-19': 76.0, '20-29': 75.0, '30-39': 84.0, '40-49': 77.0, '50-59': 78.89, '60-69': 80.5, '70-79': 81.0, '80-89': 71.86, '90-99': 76.38, '100-109': 65.33, '110-119': 68.84, '120-129': 82.0, '130-139': 78.5, '140-149': 80.9, '150-159': 76.38, '160-169': 73.37, '170-179': 65.5, '180-189': 73.74, '190-199': 84.5, '200-209': 68.84, '210-219': 49.0, '220-229': 88.44, '230-239': 81.0, 'old': 75.84, 'new': 72.79}
2023-09-14 11:45:50,325 [trainer.py] => CNN top1 curve: [90.0, 88.74, 86.32, 82.46, 80.59, 77.95, 76.94, 75.46]
2023-09-14 11:45:50,326 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.55, 96.49, 95.32, 94.07, 93.58, 92.36]

2023-09-14 11:45:50,326 [trainer.py] => Average Accuracy (CNN): 82.3075
2023-09-14 11:45:50,328 [trainer.py] => All params: 172012033
2023-09-14 11:45:50,331 [trainer.py] => Trainable params: 86213377
2023-09-14 11:45:50,340 [adam_vpt.py] => Learning on 240-270
2023-09-14 11:48:33,484 [trainer.py] => No NME accuracy.
2023-09-14 11:48:33,485 [trainer.py] => CNN: {'total': 74.48, '00-09': 74.0, '10-19': 75.5, '20-29': 75.0, '30-39': 84.0, '40-49': 77.0, '50-59': 77.89, '60-69': 78.5, '70-79': 80.5, '80-89': 70.85, '90-99': 74.87, '100-109': 64.32, '110-119': 65.83, '120-129': 77.0, '130-139': 77.5, '140-149': 76.88, '150-159': 73.37, '160-169': 73.37, '170-179': 65.5, '180-189': 73.74, '190-199': 82.5, '200-209': 65.83, '210-219': 49.0, '220-229': 88.44, '230-239': 81.0, '240-249': 69.0, '250-259': 77.39, '260-269': 82.0, 'old': 74.27, 'new': 76.13}
2023-09-14 11:48:33,485 [trainer.py] => CNN top1 curve: [90.0, 88.74, 86.32, 82.46, 80.59, 77.95, 76.94, 75.46, 74.48]
2023-09-14 11:48:33,486 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.55, 96.49, 95.32, 94.07, 93.58, 92.36, 92.46]

2023-09-14 11:48:33,486 [trainer.py] => Average Accuracy (CNN): 81.43777777777778
2023-09-14 11:48:33,490 [trainer.py] => All params: 172058113
2023-09-14 11:48:33,493 [trainer.py] => Trainable params: 86259457
2023-09-14 11:48:33,503 [adam_vpt.py] => Learning on 270-300
2023-09-14 11:51:15,494 [trainer.py] => No NME accuracy.
2023-09-14 11:51:15,495 [trainer.py] => CNN: {'total': 74.42, '00-09': 73.5, '10-19': 75.0, '20-29': 74.5, '30-39': 84.0, '40-49': 74.0, '50-59': 76.88, '60-69': 78.0, '70-79': 80.0, '80-89': 69.85, '90-99': 71.86, '100-109': 63.82, '110-119': 65.33, '120-129': 77.0, '130-139': 77.5, '140-149': 76.88, '150-159': 73.37, '160-169': 71.86, '170-179': 64.5, '180-189': 72.73, '190-199': 82.0, '200-209': 65.83, '210-219': 48.5, '220-229': 88.44, '230-239': 80.0, '240-249': 67.0, '250-259': 76.88, '260-269': 81.0, '270-279': 80.9, '280-289': 80.5, '290-299': 80.9, 'old': 73.71, 'new': 80.77}
2023-09-14 11:51:15,496 [trainer.py] => CNN top1 curve: [90.0, 88.74, 86.32, 82.46, 80.59, 77.95, 76.94, 75.46, 74.48, 74.42]
2023-09-14 11:51:15,496 [trainer.py] => CNN top5 curve: [98.83, 98.42, 97.55, 96.49, 95.32, 94.07, 93.58, 92.36, 92.46, 92.18]

2023-09-14 11:51:15,496 [trainer.py] => Average Accuracy (CNN): 80.736
2023-09-14 17:32:38,950 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 17:32:38,951 [trainer.py] => prefix:  
2023-09-14 17:32:38,952 [trainer.py] => dataset: omnibenchmark
2023-09-14 17:32:38,952 [trainer.py] => memory_size: 0
2023-09-14 17:32:38,953 [trainer.py] => memory_per_class: 0
2023-09-14 17:32:38,953 [trainer.py] => fixed_memory: False
2023-09-14 17:32:38,953 [trainer.py] => shuffle: True
2023-09-14 17:32:38,954 [trainer.py] => init_cls: 30
2023-09-14 17:32:38,954 [trainer.py] => increment: 30
2023-09-14 17:32:38,955 [trainer.py] => model_name: adam_vpt
2023-09-14 17:32:38,955 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 17:32:38,956 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 17:32:38,956 [trainer.py] => visible_device: 2,3
2023-09-14 17:32:38,957 [trainer.py] => seed: 1993
2023-09-14 17:32:38,957 [trainer.py] => tuned_epoch: 20
2023-09-14 17:32:38,958 [trainer.py] => init_lr: 0.02
2023-09-14 17:32:38,958 [trainer.py] => batch_size: 96
2023-09-14 17:32:38,958 [trainer.py] => weight_decay: 0.0005
2023-09-14 17:32:38,959 [trainer.py] => min_lr: 0
2023-09-14 17:32:38,959 [trainer.py] => optimizer: sgd
2023-09-14 17:32:38,960 [trainer.py] => vpt_type: deep
2023-09-14 17:32:38,960 [trainer.py] => prompt_token_num: 5
2023-09-14 17:32:38,960 [trainer.py] => use_A: False
2023-09-14 17:32:41,136 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 17:32:45,935 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 17:32:46,289 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 17:40:11,656 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 17:40:11,656 [trainer.py] => prefix:  
2023-09-14 17:40:11,657 [trainer.py] => dataset: omnibenchmark
2023-09-14 17:40:11,657 [trainer.py] => memory_size: 0
2023-09-14 17:40:11,657 [trainer.py] => memory_per_class: 0
2023-09-14 17:40:11,658 [trainer.py] => fixed_memory: False
2023-09-14 17:40:11,658 [trainer.py] => shuffle: True
2023-09-14 17:40:11,658 [trainer.py] => init_cls: 30
2023-09-14 17:40:11,659 [trainer.py] => increment: 30
2023-09-14 17:40:11,659 [trainer.py] => model_name: adam_vpt
2023-09-14 17:40:11,660 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 17:40:11,660 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 17:40:11,660 [trainer.py] => visible_device: 2,3
2023-09-14 17:40:11,661 [trainer.py] => seed: 1993
2023-09-14 17:40:11,661 [trainer.py] => tuned_epoch: 20
2023-09-14 17:40:11,662 [trainer.py] => init_lr: 0.02
2023-09-14 17:40:11,662 [trainer.py] => batch_size: 96
2023-09-14 17:40:11,664 [trainer.py] => weight_decay: 0.0005
2023-09-14 17:40:11,666 [trainer.py] => min_lr: 0
2023-09-14 17:40:11,666 [trainer.py] => optimizer: sgd
2023-09-14 17:40:11,666 [trainer.py] => vpt_type: deep
2023-09-14 17:40:11,667 [trainer.py] => prompt_token_num: 5
2023-09-14 17:40:11,667 [trainer.py] => use_A: False
2023-09-14 17:40:14,585 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 17:40:22,290 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 17:40:22,760 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 17:44:32,590 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 17:44:32,591 [trainer.py] => prefix:  
2023-09-14 17:44:32,591 [trainer.py] => dataset: omnibenchmark
2023-09-14 17:44:32,592 [trainer.py] => memory_size: 0
2023-09-14 17:44:32,592 [trainer.py] => memory_per_class: 0
2023-09-14 17:44:32,593 [trainer.py] => fixed_memory: False
2023-09-14 17:44:32,593 [trainer.py] => shuffle: True
2023-09-14 17:44:32,593 [trainer.py] => init_cls: 30
2023-09-14 17:44:32,594 [trainer.py] => increment: 30
2023-09-14 17:44:32,594 [trainer.py] => model_name: adam_vpt
2023-09-14 17:44:32,594 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 17:44:32,594 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 17:44:32,595 [trainer.py] => visible_device: 2,3
2023-09-14 17:44:32,595 [trainer.py] => seed: 1993
2023-09-14 17:44:32,595 [trainer.py] => tuned_epoch: 20
2023-09-14 17:44:32,596 [trainer.py] => init_lr: 0.02
2023-09-14 17:44:32,596 [trainer.py] => batch_size: 96
2023-09-14 17:44:32,596 [trainer.py] => weight_decay: 0.0005
2023-09-14 17:44:32,596 [trainer.py] => min_lr: 0
2023-09-14 17:44:32,597 [trainer.py] => optimizer: sgd
2023-09-14 17:44:32,597 [trainer.py] => vpt_type: deep
2023-09-14 17:44:32,597 [trainer.py] => prompt_token_num: 5
2023-09-14 17:44:32,597 [trainer.py] => use_A: False
2023-09-14 17:44:33,803 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 17:44:40,950 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 17:44:41,293 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 17:44:50,350 [trainer.py] => All params: 86761192
2023-09-14 17:44:50,353 [trainer.py] => Trainable params: 962536
2023-09-14 17:44:51,659 [adam_vpt.py] => Learning on 0-30
2023-09-14 17:47:55,097 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 17:47:55,097 [trainer.py] => prefix:  
2023-09-14 17:47:55,097 [trainer.py] => dataset: omnibenchmark
2023-09-14 17:47:55,098 [trainer.py] => memory_size: 0
2023-09-14 17:47:55,098 [trainer.py] => memory_per_class: 0
2023-09-14 17:47:55,098 [trainer.py] => fixed_memory: False
2023-09-14 17:47:55,098 [trainer.py] => shuffle: True
2023-09-14 17:47:55,099 [trainer.py] => init_cls: 30
2023-09-14 17:47:55,099 [trainer.py] => increment: 30
2023-09-14 17:47:55,099 [trainer.py] => model_name: adam_vpt
2023-09-14 17:47:55,099 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 17:47:55,099 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 17:47:55,100 [trainer.py] => visible_device: 2,3
2023-09-14 17:47:55,100 [trainer.py] => seed: 1993
2023-09-14 17:47:55,100 [trainer.py] => tuned_epoch: 20
2023-09-14 17:47:55,100 [trainer.py] => init_lr: 0.02
2023-09-14 17:47:55,101 [trainer.py] => batch_size: 96
2023-09-14 17:47:55,101 [trainer.py] => weight_decay: 0.0005
2023-09-14 17:47:55,101 [trainer.py] => min_lr: 0
2023-09-14 17:47:55,101 [trainer.py] => optimizer: sgd
2023-09-14 17:47:55,102 [trainer.py] => vpt_type: deep
2023-09-14 17:47:55,102 [trainer.py] => prompt_token_num: 5
2023-09-14 17:47:55,109 [trainer.py] => use_A: False
2023-09-14 17:47:56,082 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 17:48:02,415 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 17:48:02,770 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 17:48:07,106 [trainer.py] => All params: 85992192
2023-09-14 17:48:07,119 [trainer.py] => Trainable params: 46080
2023-09-14 17:48:08,588 [adam_vpt.py] => Learning on 0-30
2023-09-14 17:52:40,715 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 17:52:40,716 [trainer.py] => prefix:  
2023-09-14 17:52:40,716 [trainer.py] => dataset: omnibenchmark
2023-09-14 17:52:40,717 [trainer.py] => memory_size: 0
2023-09-14 17:52:40,717 [trainer.py] => memory_per_class: 0
2023-09-14 17:52:40,718 [trainer.py] => fixed_memory: False
2023-09-14 17:52:40,718 [trainer.py] => shuffle: True
2023-09-14 17:52:40,719 [trainer.py] => init_cls: 30
2023-09-14 17:52:40,719 [trainer.py] => increment: 30
2023-09-14 17:52:40,720 [trainer.py] => model_name: adam_vpt
2023-09-14 17:52:40,720 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 17:52:40,721 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 17:52:40,721 [trainer.py] => visible_device: 2,3
2023-09-14 17:52:40,722 [trainer.py] => seed: 1993
2023-09-14 17:52:40,724 [trainer.py] => tuned_epoch: 20
2023-09-14 17:52:40,725 [trainer.py] => init_lr: 0.02
2023-09-14 17:52:40,725 [trainer.py] => batch_size: 96
2023-09-14 17:52:40,726 [trainer.py] => weight_decay: 0.0005
2023-09-14 17:52:40,726 [trainer.py] => min_lr: 0
2023-09-14 17:52:40,727 [trainer.py] => optimizer: sgd
2023-09-14 17:52:40,727 [trainer.py] => vpt_type: deep
2023-09-14 17:52:40,728 [trainer.py] => prompt_token_num: 5
2023-09-14 17:52:40,728 [trainer.py] => use_A: False
2023-09-14 17:52:43,950 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 17:52:51,926 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 17:52:52,332 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 17:54:38,486 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-14 17:54:38,487 [trainer.py] => prefix:  
2023-09-14 17:54:38,487 [trainer.py] => dataset: omnibenchmark
2023-09-14 17:54:38,487 [trainer.py] => memory_size: 0
2023-09-14 17:54:38,488 [trainer.py] => memory_per_class: 0
2023-09-14 17:54:38,488 [trainer.py] => fixed_memory: False
2023-09-14 17:54:38,488 [trainer.py] => shuffle: True
2023-09-14 17:54:38,488 [trainer.py] => init_cls: 30
2023-09-14 17:54:38,488 [trainer.py] => increment: 30
2023-09-14 17:54:38,489 [trainer.py] => model_name: adam_vpt
2023-09-14 17:54:38,489 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-14 17:54:38,489 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-14 17:54:38,489 [trainer.py] => visible_device: 2,3
2023-09-14 17:54:38,489 [trainer.py] => seed: 1993
2023-09-14 17:54:38,490 [trainer.py] => tuned_epoch: 20
2023-09-14 17:54:38,490 [trainer.py] => init_lr: 0.02
2023-09-14 17:54:38,490 [trainer.py] => batch_size: 96
2023-09-14 17:54:38,490 [trainer.py] => weight_decay: 0.0005
2023-09-14 17:54:38,491 [trainer.py] => min_lr: 0
2023-09-14 17:54:38,491 [trainer.py] => optimizer: sgd
2023-09-14 17:54:38,491 [trainer.py] => vpt_type: deep
2023-09-14 17:54:38,491 [trainer.py] => prompt_token_num: 5
2023-09-14 17:54:38,491 [trainer.py] => use_A: False
2023-09-14 17:54:39,754 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-14 17:54:45,896 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-14 17:54:46,681 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-14 17:55:00,140 [trainer.py] => All params: 85992192
2023-09-14 17:55:00,142 [trainer.py] => Trainable params: 193536
2023-09-14 17:55:01,614 [adam_vpt.py] => Learning on 0-30
