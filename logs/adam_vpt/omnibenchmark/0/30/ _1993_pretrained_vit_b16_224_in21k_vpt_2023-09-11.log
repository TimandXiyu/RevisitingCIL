2023-09-11 07:04:27,553 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 07:04:27,553 [trainer.py] => prefix:  
2023-09-11 07:04:27,553 [trainer.py] => dataset: omnibenchmark
2023-09-11 07:04:27,553 [trainer.py] => memory_size: 0
2023-09-11 07:04:27,553 [trainer.py] => memory_per_class: 0
2023-09-11 07:04:27,553 [trainer.py] => fixed_memory: False
2023-09-11 07:04:27,553 [trainer.py] => shuffle: True
2023-09-11 07:04:27,553 [trainer.py] => init_cls: 30
2023-09-11 07:04:27,553 [trainer.py] => increment: 30
2023-09-11 07:04:27,553 [trainer.py] => model_name: adam_vpt
2023-09-11 07:04:27,553 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 07:04:27,553 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 07:04:27,553 [trainer.py] => seed: 1993
2023-09-11 07:04:27,553 [trainer.py] => tuned_epoch: 20
2023-09-11 07:04:27,553 [trainer.py] => init_lr: 0.01
2023-09-11 07:04:27,553 [trainer.py] => batch_size: 96
2023-09-11 07:04:27,553 [trainer.py] => weight_decay: 0.0005
2023-09-11 07:04:27,553 [trainer.py] => min_lr: 0
2023-09-11 07:04:27,553 [trainer.py] => optimizer: sgd
2023-09-11 07:04:27,553 [trainer.py] => vpt_type: deep
2023-09-11 07:04:27,553 [trainer.py] => prompt_token_num: 5
2023-09-11 07:04:27,744 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 07:04:29,380 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:04:29,886 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:04:30,708 [trainer.py] => All params: 85844736
2023-09-11 07:04:30,709 [trainer.py] => Trainable params: 46080
2023-09-11 07:04:30,853 [adam_vpt.py] => Learning on 0-30
2023-09-11 07:07:15,886 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 07:07:15,886 [trainer.py] => prefix:  
2023-09-11 07:07:15,886 [trainer.py] => dataset: omnibenchmark
2023-09-11 07:07:15,887 [trainer.py] => memory_size: 0
2023-09-11 07:07:15,887 [trainer.py] => memory_per_class: 0
2023-09-11 07:07:15,887 [trainer.py] => fixed_memory: False
2023-09-11 07:07:15,887 [trainer.py] => shuffle: True
2023-09-11 07:07:15,887 [trainer.py] => init_cls: 30
2023-09-11 07:07:15,887 [trainer.py] => increment: 30
2023-09-11 07:07:15,887 [trainer.py] => model_name: adam_vpt
2023-09-11 07:07:15,887 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 07:07:15,887 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 07:07:15,887 [trainer.py] => seed: 1993
2023-09-11 07:07:15,887 [trainer.py] => tuned_epoch: 20
2023-09-11 07:07:15,888 [trainer.py] => init_lr: 0.01
2023-09-11 07:07:15,888 [trainer.py] => batch_size: 96
2023-09-11 07:07:15,888 [trainer.py] => weight_decay: 0.0005
2023-09-11 07:07:15,888 [trainer.py] => min_lr: 0
2023-09-11 07:07:15,888 [trainer.py] => optimizer: sgd
2023-09-11 07:07:15,888 [trainer.py] => vpt_type: deep
2023-09-11 07:07:15,888 [trainer.py] => prompt_token_num: 5
2023-09-11 07:07:16,571 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 07:07:18,758 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:07:19,278 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:07:20,129 [trainer.py] => All params: 85844736
2023-09-11 07:07:20,139 [trainer.py] => Trainable params: 46080
2023-09-11 07:07:20,274 [adam_vpt.py] => Learning on 0-30
2023-09-11 07:07:41,841 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 07:07:41,841 [trainer.py] => prefix:  
2023-09-11 07:07:41,842 [trainer.py] => dataset: omnibenchmark
2023-09-11 07:07:41,842 [trainer.py] => memory_size: 0
2023-09-11 07:07:41,842 [trainer.py] => memory_per_class: 0
2023-09-11 07:07:41,842 [trainer.py] => fixed_memory: False
2023-09-11 07:07:41,842 [trainer.py] => shuffle: True
2023-09-11 07:07:41,842 [trainer.py] => init_cls: 30
2023-09-11 07:07:41,842 [trainer.py] => increment: 30
2023-09-11 07:07:41,842 [trainer.py] => model_name: adam_vpt
2023-09-11 07:07:41,842 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 07:07:41,842 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 07:07:41,842 [trainer.py] => seed: 1993
2023-09-11 07:07:41,843 [trainer.py] => tuned_epoch: 20
2023-09-11 07:07:41,843 [trainer.py] => init_lr: 0.01
2023-09-11 07:07:41,843 [trainer.py] => batch_size: 96
2023-09-11 07:07:41,843 [trainer.py] => weight_decay: 0.0005
2023-09-11 07:07:41,843 [trainer.py] => min_lr: 0
2023-09-11 07:07:41,843 [trainer.py] => optimizer: sgd
2023-09-11 07:07:41,843 [trainer.py] => vpt_type: deep
2023-09-11 07:07:41,843 [trainer.py] => prompt_token_num: 5
2023-09-11 07:07:42,538 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 07:07:44,766 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:07:45,279 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:07:46,206 [trainer.py] => All params: 85844736
2023-09-11 07:07:46,216 [trainer.py] => Trainable params: 46080
2023-09-11 07:07:46,351 [adam_vpt.py] => Learning on 0-30
2023-09-11 07:08:51,291 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 07:08:51,291 [trainer.py] => prefix:  
2023-09-11 07:08:51,291 [trainer.py] => dataset: omnibenchmark
2023-09-11 07:08:51,291 [trainer.py] => memory_size: 0
2023-09-11 07:08:51,292 [trainer.py] => memory_per_class: 0
2023-09-11 07:08:51,292 [trainer.py] => fixed_memory: False
2023-09-11 07:08:51,292 [trainer.py] => shuffle: True
2023-09-11 07:08:51,292 [trainer.py] => init_cls: 30
2023-09-11 07:08:51,292 [trainer.py] => increment: 30
2023-09-11 07:08:51,292 [trainer.py] => model_name: adam_vpt
2023-09-11 07:08:51,292 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 07:08:51,292 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 07:08:51,292 [trainer.py] => seed: 1993
2023-09-11 07:08:51,292 [trainer.py] => tuned_epoch: 20
2023-09-11 07:08:51,292 [trainer.py] => init_lr: 0.01
2023-09-11 07:08:51,292 [trainer.py] => batch_size: 96
2023-09-11 07:08:51,292 [trainer.py] => weight_decay: 0.0005
2023-09-11 07:08:51,292 [trainer.py] => min_lr: 0
2023-09-11 07:08:51,292 [trainer.py] => optimizer: sgd
2023-09-11 07:08:51,292 [trainer.py] => vpt_type: deep
2023-09-11 07:08:51,292 [trainer.py] => prompt_token_num: 5
2023-09-11 07:08:51,483 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 07:08:53,145 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:08:53,654 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:08:54,418 [trainer.py] => All params: 85844736
2023-09-11 07:08:54,418 [trainer.py] => Trainable params: 46080
2023-09-11 07:08:54,564 [adam_vpt.py] => Learning on 0-30
2023-09-11 07:26:32,512 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.378, Train_accy 89.15, Test_accy 93.00
2023-09-11 07:26:33,377 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:26:33,881 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:26:35,321 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:26:35,571 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:37:58,264 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 08:37:58,264 [trainer.py] => prefix:  
2023-09-11 08:37:58,265 [trainer.py] => dataset: omnibenchmark
2023-09-11 08:37:58,265 [trainer.py] => memory_size: 0
2023-09-11 08:37:58,265 [trainer.py] => memory_per_class: 0
2023-09-11 08:37:58,265 [trainer.py] => fixed_memory: False
2023-09-11 08:37:58,265 [trainer.py] => shuffle: True
2023-09-11 08:37:58,265 [trainer.py] => init_cls: 30
2023-09-11 08:37:58,265 [trainer.py] => increment: 30
2023-09-11 08:37:58,265 [trainer.py] => model_name: adam_vpt
2023-09-11 08:37:58,265 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 08:37:58,265 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 08:37:58,265 [trainer.py] => seed: 1993
2023-09-11 08:37:58,265 [trainer.py] => tuned_epoch: 20
2023-09-11 08:37:58,265 [trainer.py] => init_lr: 0.01
2023-09-11 08:37:58,265 [trainer.py] => batch_size: 96
2023-09-11 08:37:58,265 [trainer.py] => weight_decay: 0.0005
2023-09-11 08:37:58,265 [trainer.py] => min_lr: 0
2023-09-11 08:37:58,265 [trainer.py] => optimizer: sgd
2023-09-11 08:37:58,265 [trainer.py] => vpt_type: deep
2023-09-11 08:37:58,265 [trainer.py] => prompt_token_num: 5
2023-09-11 08:37:58,642 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 08:38:00,231 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 08:38:00,758 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:38:01,511 [trainer.py] => All params: 85844736
2023-09-11 08:38:01,511 [trainer.py] => Trainable params: 46080
2023-09-11 08:38:01,644 [adam_vpt.py] => Learning on 0-30
2023-09-11 08:38:59,130 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 08:38:59,130 [trainer.py] => prefix:  
2023-09-11 08:38:59,130 [trainer.py] => dataset: omnibenchmark
2023-09-11 08:38:59,130 [trainer.py] => memory_size: 0
2023-09-11 08:38:59,130 [trainer.py] => memory_per_class: 0
2023-09-11 08:38:59,130 [trainer.py] => fixed_memory: False
2023-09-11 08:38:59,130 [trainer.py] => shuffle: True
2023-09-11 08:38:59,130 [trainer.py] => init_cls: 30
2023-09-11 08:38:59,130 [trainer.py] => increment: 30
2023-09-11 08:38:59,130 [trainer.py] => model_name: adam_vpt
2023-09-11 08:38:59,130 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 08:38:59,130 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 08:38:59,130 [trainer.py] => seed: 1993
2023-09-11 08:38:59,130 [trainer.py] => tuned_epoch: 20
2023-09-11 08:38:59,130 [trainer.py] => init_lr: 0.01
2023-09-11 08:38:59,130 [trainer.py] => batch_size: 96
2023-09-11 08:38:59,130 [trainer.py] => weight_decay: 0.0005
2023-09-11 08:38:59,130 [trainer.py] => min_lr: 0
2023-09-11 08:38:59,130 [trainer.py] => optimizer: sgd
2023-09-11 08:38:59,130 [trainer.py] => vpt_type: deep
2023-09-11 08:38:59,130 [trainer.py] => prompt_token_num: 5
2023-09-11 08:38:59,326 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 08:39:00,951 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 08:39:01,452 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:39:02,268 [trainer.py] => All params: 85844736
2023-09-11 08:39:02,269 [trainer.py] => Trainable params: 46080
2023-09-11 08:39:02,408 [adam_vpt.py] => Learning on 0-30
2023-09-11 08:39:49,911 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 3.087, Train_accy 45.49
2023-09-11 08:40:44,170 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 1.425, Train_accy 77.17, Test_accy 81.83
2023-09-11 08:41:38,553 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.701, Train_accy 80.62, Test_accy 87.67
2023-09-11 08:42:32,703 [adam_vpt.py] => Task 0, Epoch 4/20 => Loss 0.585, Train_accy 83.26, Test_accy 87.83
2023-09-11 08:43:26,592 [adam_vpt.py] => Task 0, Epoch 5/20 => Loss 0.530, Train_accy 84.10, Test_accy 89.17
2023-09-11 08:44:14,527 [adam_vpt.py] => Task 0, Epoch 6/20 => Loss 0.503, Train_accy 84.90
2023-09-11 08:45:08,324 [adam_vpt.py] => Task 0, Epoch 7/20 => Loss 0.473, Train_accy 86.17, Test_accy 91.00
2023-09-11 08:46:02,358 [adam_vpt.py] => Task 0, Epoch 8/20 => Loss 0.456, Train_accy 86.15, Test_accy 90.17
2023-09-11 08:46:56,444 [adam_vpt.py] => Task 0, Epoch 9/20 => Loss 0.435, Train_accy 87.25, Test_accy 91.33
2023-09-11 08:47:50,553 [adam_vpt.py] => Task 0, Epoch 10/20 => Loss 0.417, Train_accy 87.58, Test_accy 91.17
2023-09-11 08:48:38,324 [adam_vpt.py] => Task 0, Epoch 11/20 => Loss 0.419, Train_accy 87.35
2023-09-11 08:49:32,672 [adam_vpt.py] => Task 0, Epoch 12/20 => Loss 0.401, Train_accy 88.24, Test_accy 91.00
2023-09-11 08:50:27,115 [adam_vpt.py] => Task 0, Epoch 13/20 => Loss 0.397, Train_accy 88.29, Test_accy 92.50
2023-09-11 08:51:21,256 [adam_vpt.py] => Task 0, Epoch 14/20 => Loss 0.394, Train_accy 88.33, Test_accy 92.17
2023-09-11 08:52:15,690 [adam_vpt.py] => Task 0, Epoch 15/20 => Loss 0.385, Train_accy 88.55, Test_accy 91.17
2023-09-11 08:53:03,692 [adam_vpt.py] => Task 0, Epoch 16/20 => Loss 0.388, Train_accy 88.50
2023-09-11 08:53:58,240 [adam_vpt.py] => Task 0, Epoch 17/20 => Loss 0.376, Train_accy 88.90, Test_accy 92.67
2023-09-11 08:54:52,546 [adam_vpt.py] => Task 0, Epoch 18/20 => Loss 0.374, Train_accy 88.95, Test_accy 92.33
2023-09-11 08:55:46,680 [adam_vpt.py] => Task 0, Epoch 19/20 => Loss 0.380, Train_accy 88.79, Test_accy 92.00
2023-09-11 08:56:41,228 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.378, Train_accy 89.04, Test_accy 92.17
2023-09-11 08:56:42,055 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 08:56:42,584 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:56:44,123 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 08:56:44,376 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:57:58,950 [trainer.py] => No NME accuracy.
2023-09-11 08:57:58,950 [trainer.py] => CNN: {'total': 90.5, '00-09': 93.0, '10-19': 91.0, '20-29': 87.5, 'old': 0, 'new': 90.5}
2023-09-11 08:57:58,950 [trainer.py] => CNN top1 curve: [90.5]
2023-09-11 08:57:58,950 [trainer.py] => CNN top5 curve: [99.0]

2023-09-11 08:57:58,950 [trainer.py] => Average Accuracy (CNN): 90.5
2023-09-11 08:57:58,952 [trainer.py] => All params: 171689473
2023-09-11 08:57:58,953 [trainer.py] => Trainable params: 85890817
2023-09-11 08:57:58,955 [adam_vpt.py] => Learning on 30-60
2023-09-11 08:59:17,822 [trainer.py] => No NME accuracy.
2023-09-11 08:59:17,823 [trainer.py] => CNN: {'total': 89.66, '00-09': 88.5, '10-19': 91.0, '20-29': 84.5, '30-39': 91.0, '40-49': 87.5, '50-59': 95.48, 'old': 88.0, 'new': 91.32}
2023-09-11 08:59:17,823 [trainer.py] => CNN top1 curve: [90.5, 89.66]
2023-09-11 08:59:17,823 [trainer.py] => CNN top5 curve: [99.0, 98.67]

2023-09-11 08:59:17,823 [trainer.py] => Average Accuracy (CNN): 90.08
2023-09-11 08:59:17,824 [trainer.py] => All params: 171735553
2023-09-11 08:59:17,825 [trainer.py] => Trainable params: 85936897
2023-09-11 08:59:17,827 [adam_vpt.py] => Learning on 60-90
2023-09-11 09:00:39,429 [trainer.py] => No NME accuracy.
2023-09-11 09:00:39,430 [trainer.py] => CNN: {'total': 86.76, '00-09': 85.5, '10-19': 85.5, '20-29': 81.5, '30-39': 88.5, '40-49': 85.5, '50-59': 94.47, '60-69': 93.5, '70-79': 88.0, '80-89': 78.39, 'old': 86.82, 'new': 86.64}
2023-09-11 09:00:39,430 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76]
2023-09-11 09:00:39,430 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16]

2023-09-11 09:00:39,430 [trainer.py] => Average Accuracy (CNN): 88.97333333333334
2023-09-11 09:00:39,431 [trainer.py] => All params: 171781633
2023-09-11 09:00:39,432 [trainer.py] => Trainable params: 85982977
2023-09-11 09:00:39,433 [adam_vpt.py] => Learning on 90-120
2023-09-11 09:02:10,587 [trainer.py] => No NME accuracy.
2023-09-11 09:02:10,587 [trainer.py] => CNN: {'total': 82.76, '00-09': 82.5, '10-19': 82.0, '20-29': 81.0, '30-39': 87.5, '40-49': 83.5, '50-59': 86.93, '60-69': 89.0, '70-79': 86.0, '80-89': 74.37, '90-99': 80.9, '100-109': 79.9, '110-119': 79.4, 'old': 83.65, 'new': 80.07}
2023-09-11 09:02:10,587 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76]
2023-09-11 09:02:10,587 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7]

2023-09-11 09:02:10,587 [trainer.py] => Average Accuracy (CNN): 87.42
2023-09-11 09:02:10,588 [trainer.py] => All params: 171827713
2023-09-11 09:02:10,589 [trainer.py] => Trainable params: 86029057
2023-09-11 09:02:10,590 [adam_vpt.py] => Learning on 120-150
2023-09-11 09:03:44,178 [trainer.py] => No NME accuracy.
2023-09-11 09:03:44,178 [trainer.py] => CNN: {'total': 80.53, '00-09': 79.5, '10-19': 82.0, '20-29': 80.5, '30-39': 87.0, '40-49': 80.5, '50-59': 79.9, '60-69': 84.0, '70-79': 85.5, '80-89': 73.37, '90-99': 80.4, '100-109': 76.88, '110-119': 72.86, '120-129': 83.5, '130-139': 81.5, '140-149': 80.4, 'old': 80.21, 'new': 81.8}
2023-09-11 09:03:44,178 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53]
2023-09-11 09:03:44,178 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46]

2023-09-11 09:03:44,178 [trainer.py] => Average Accuracy (CNN): 86.042
2023-09-11 09:03:44,180 [trainer.py] => All params: 171873793
2023-09-11 09:03:44,182 [trainer.py] => Trainable params: 86075137
2023-09-11 09:03:44,185 [adam_vpt.py] => Learning on 150-180
2023-09-11 09:05:18,442 [trainer.py] => No NME accuracy.
2023-09-11 09:05:18,442 [trainer.py] => CNN: {'total': 77.76, '00-09': 77.0, '10-19': 79.0, '20-29': 78.0, '30-39': 84.5, '40-49': 77.5, '50-59': 78.39, '60-69': 82.0, '70-79': 82.5, '80-89': 70.35, '90-99': 80.4, '100-109': 74.37, '110-119': 71.36, '120-129': 82.0, '130-139': 81.0, '140-149': 79.4, '150-159': 77.39, '160-169': 76.88, '170-179': 67.5, 'old': 78.52, 'new': 73.91}
2023-09-11 09:05:18,442 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76]
2023-09-11 09:05:18,442 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07]

2023-09-11 09:05:18,442 [trainer.py] => Average Accuracy (CNN): 84.66166666666668
2023-09-11 09:05:18,444 [trainer.py] => All params: 171919873
2023-09-11 09:05:18,445 [trainer.py] => Trainable params: 86121217
2023-09-11 09:05:18,448 [adam_vpt.py] => Learning on 180-210
2023-09-11 09:06:57,934 [trainer.py] => No NME accuracy.
2023-09-11 09:06:57,934 [trainer.py] => CNN: {'total': 76.84, '00-09': 76.0, '10-19': 79.0, '20-29': 77.5, '30-39': 84.5, '40-49': 76.5, '50-59': 77.89, '60-69': 81.0, '70-79': 82.0, '80-89': 69.35, '90-99': 77.89, '100-109': 73.87, '110-119': 68.84, '120-129': 82.0, '130-139': 80.0, '140-149': 79.4, '150-159': 75.38, '160-169': 75.88, '170-179': 67.0, '180-189': 73.23, '190-199': 84.0, '200-209': 72.36, 'old': 76.89, 'new': 76.55}
2023-09-11 09:06:57,934 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76, 76.84]
2023-09-11 09:06:57,934 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07, 93.67]

2023-09-11 09:06:57,935 [trainer.py] => Average Accuracy (CNN): 83.54428571428572
2023-09-11 09:06:57,936 [trainer.py] => All params: 171965953
2023-09-11 09:06:57,937 [trainer.py] => Trainable params: 86167297
2023-09-11 09:06:57,940 [adam_vpt.py] => Learning on 210-240
2023-09-11 09:08:45,632 [trainer.py] => No NME accuracy.
2023-09-11 09:08:45,632 [trainer.py] => CNN: {'total': 75.04, '00-09': 73.5, '10-19': 76.5, '20-29': 76.5, '30-39': 84.5, '40-49': 75.0, '50-59': 77.89, '60-69': 80.0, '70-79': 82.0, '80-89': 69.35, '90-99': 77.39, '100-109': 66.33, '110-119': 67.84, '120-129': 78.5, '130-139': 80.0, '140-149': 78.39, '150-159': 75.38, '160-169': 75.88, '170-179': 65.0, '180-189': 72.73, '190-199': 83.5, '200-209': 69.35, '210-219': 48.5, '220-229': 86.43, '230-239': 80.5, 'old': 75.51, 'new': 71.79}
2023-09-11 09:08:45,632 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76, 76.84, 75.04]
2023-09-11 09:08:45,632 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07, 93.67, 92.38]

2023-09-11 09:08:45,632 [trainer.py] => Average Accuracy (CNN): 82.48125
2023-09-11 09:08:45,634 [trainer.py] => All params: 172012033
2023-09-11 09:08:45,635 [trainer.py] => Trainable params: 86213377
2023-09-11 09:08:45,638 [adam_vpt.py] => Learning on 240-270
2023-09-11 09:10:36,951 [trainer.py] => No NME accuracy.
2023-09-11 09:10:36,951 [trainer.py] => CNN: {'total': 74.01, '00-09': 73.5, '10-19': 76.0, '20-29': 76.5, '30-39': 84.0, '40-49': 75.0, '50-59': 76.88, '60-69': 78.5, '70-79': 81.0, '80-89': 68.84, '90-99': 75.88, '100-109': 65.33, '110-119': 65.33, '120-129': 74.0, '130-139': 79.0, '140-149': 73.87, '150-159': 72.36, '160-169': 75.88, '170-179': 65.0, '180-189': 72.73, '190-199': 82.5, '200-209': 65.33, '210-219': 48.5, '220-229': 86.43, '230-239': 80.5, '240-249': 69.0, '250-259': 75.88, '260-269': 80.5, 'old': 73.87, 'new': 75.13}
2023-09-11 09:10:36,952 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76, 76.84, 75.04, 74.01]
2023-09-11 09:10:36,952 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07, 93.67, 92.38, 92.28]

2023-09-11 09:10:36,952 [trainer.py] => Average Accuracy (CNN): 81.54
2023-09-11 09:10:36,953 [trainer.py] => All params: 172058113
2023-09-11 09:10:36,955 [trainer.py] => Trainable params: 86259457
2023-09-11 09:10:36,960 [adam_vpt.py] => Learning on 270-300
2023-09-11 09:12:27,025 [trainer.py] => No NME accuracy.
2023-09-11 09:12:27,025 [trainer.py] => CNN: {'total': 74.05, '00-09': 73.5, '10-19': 76.0, '20-29': 76.0, '30-39': 83.5, '40-49': 71.0, '50-59': 76.88, '60-69': 78.5, '70-79': 80.5, '80-89': 68.84, '90-99': 72.36, '100-109': 64.82, '110-119': 64.82, '120-129': 74.0, '130-139': 78.5, '140-149': 73.37, '150-159': 72.36, '160-169': 73.37, '170-179': 64.0, '180-189': 72.22, '190-199': 82.0, '200-209': 64.82, '210-219': 48.0, '220-229': 86.43, '230-239': 79.0, '240-249': 67.0, '250-259': 75.38, '260-269': 79.5, '270-279': 82.91, '280-289': 80.5, '290-299': 81.41, 'old': 73.21, 'new': 81.61}
2023-09-11 09:12:27,025 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76, 76.84, 75.04, 74.01, 74.05]
2023-09-11 09:12:27,025 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07, 93.67, 92.38, 92.28, 92.08]

2023-09-11 09:12:27,025 [trainer.py] => Average Accuracy (CNN): 80.791
2023-09-11 09:13:42,517 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 09:13:42,517 [trainer.py] => prefix:  
2023-09-11 09:13:42,518 [trainer.py] => dataset: omnibenchmark
2023-09-11 09:13:42,518 [trainer.py] => memory_size: 0
2023-09-11 09:13:42,518 [trainer.py] => memory_per_class: 0
2023-09-11 09:13:42,518 [trainer.py] => fixed_memory: False
2023-09-11 09:13:42,518 [trainer.py] => shuffle: True
2023-09-11 09:13:42,518 [trainer.py] => init_cls: 30
2023-09-11 09:13:42,518 [trainer.py] => increment: 30
2023-09-11 09:13:42,518 [trainer.py] => model_name: adam_vpt
2023-09-11 09:13:42,518 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 09:13:42,518 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 09:13:42,518 [trainer.py] => seed: 1993
2023-09-11 09:13:42,518 [trainer.py] => tuned_epoch: 20
2023-09-11 09:13:42,518 [trainer.py] => init_lr: 0.02
2023-09-11 09:13:42,518 [trainer.py] => batch_size: 96
2023-09-11 09:13:42,518 [trainer.py] => weight_decay: 0.0005
2023-09-11 09:13:42,518 [trainer.py] => min_lr: 0
2023-09-11 09:13:42,518 [trainer.py] => optimizer: sgd
2023-09-11 09:13:42,518 [trainer.py] => vpt_type: deep
2023-09-11 09:13:42,518 [trainer.py] => prompt_token_num: 5
2023-09-11 09:13:42,709 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 09:13:44,322 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 09:13:44,831 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 09:13:45,637 [trainer.py] => All params: 85844736
2023-09-11 09:13:45,637 [trainer.py] => Trainable params: 46080
2023-09-11 09:13:45,783 [adam_vpt.py] => Learning on 0-30
2023-09-11 09:14:33,477 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 2.560, Train_accy 55.99
2023-09-11 09:15:27,719 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 0.686, Train_accy 80.63, Test_accy 86.17
2023-09-11 09:16:22,101 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.530, Train_accy 84.18, Test_accy 89.33
2023-09-11 09:17:16,396 [adam_vpt.py] => Task 0, Epoch 4/20 => Loss 0.474, Train_accy 85.31, Test_accy 89.33
2023-09-11 09:18:10,830 [adam_vpt.py] => Task 0, Epoch 5/20 => Loss 0.438, Train_accy 86.43, Test_accy 90.67
2023-09-11 09:18:58,713 [adam_vpt.py] => Task 0, Epoch 6/20 => Loss 0.425, Train_accy 87.02
2023-09-11 09:19:52,874 [adam_vpt.py] => Task 0, Epoch 7/20 => Loss 0.402, Train_accy 88.03, Test_accy 91.00
2023-09-11 09:20:47,089 [adam_vpt.py] => Task 0, Epoch 8/20 => Loss 0.389, Train_accy 88.19, Test_accy 91.33
2023-09-11 09:21:41,347 [adam_vpt.py] => Task 0, Epoch 9/20 => Loss 0.373, Train_accy 88.69, Test_accy 91.83
2023-09-11 09:22:35,290 [adam_vpt.py] => Task 0, Epoch 10/20 => Loss 0.356, Train_accy 89.31, Test_accy 91.50
2023-09-11 09:23:23,088 [adam_vpt.py] => Task 0, Epoch 11/20 => Loss 0.363, Train_accy 88.89
2023-09-11 09:24:17,562 [adam_vpt.py] => Task 0, Epoch 12/20 => Loss 0.343, Train_accy 89.64, Test_accy 91.83
2023-09-11 09:25:11,942 [adam_vpt.py] => Task 0, Epoch 13/20 => Loss 0.344, Train_accy 89.65, Test_accy 92.67
2023-09-11 09:26:06,338 [adam_vpt.py] => Task 0, Epoch 14/20 => Loss 0.340, Train_accy 89.57, Test_accy 92.67
2023-09-11 09:27:00,852 [adam_vpt.py] => Task 0, Epoch 15/20 => Loss 0.331, Train_accy 89.92, Test_accy 92.00
2023-09-11 09:27:48,499 [adam_vpt.py] => Task 0, Epoch 16/20 => Loss 0.334, Train_accy 89.69
2023-09-11 09:28:42,749 [adam_vpt.py] => Task 0, Epoch 17/20 => Loss 0.324, Train_accy 90.06, Test_accy 92.17
2023-09-11 09:29:36,963 [adam_vpt.py] => Task 0, Epoch 18/20 => Loss 0.321, Train_accy 90.35, Test_accy 92.33
2023-09-11 09:30:31,436 [adam_vpt.py] => Task 0, Epoch 19/20 => Loss 0.327, Train_accy 89.96, Test_accy 92.50
2023-09-11 09:31:25,971 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.322, Train_accy 90.47, Test_accy 92.67
2023-09-11 09:31:26,791 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 09:31:27,336 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 09:31:28,868 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 09:31:29,130 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 09:32:43,471 [trainer.py] => No NME accuracy.
2023-09-11 09:32:43,471 [trainer.py] => CNN: {'total': 91.5, '00-09': 93.0, '10-19': 91.5, '20-29': 90.0, 'old': 0, 'new': 91.5}
2023-09-11 09:32:43,471 [trainer.py] => CNN top1 curve: [91.5]
2023-09-11 09:32:43,471 [trainer.py] => CNN top5 curve: [99.0]

2023-09-11 09:32:43,472 [trainer.py] => Average Accuracy (CNN): 91.5
2023-09-11 09:32:43,473 [trainer.py] => All params: 171689473
2023-09-11 09:32:43,475 [trainer.py] => Trainable params: 85890817
2023-09-11 09:32:43,476 [adam_vpt.py] => Learning on 30-60
2023-09-11 09:34:01,927 [trainer.py] => No NME accuracy.
2023-09-11 09:34:01,927 [trainer.py] => CNN: {'total': 89.66, '00-09': 88.0, '10-19': 91.5, '20-29': 86.0, '30-39': 91.0, '40-49': 88.0, '50-59': 93.47, 'old': 88.5, 'new': 90.82}
2023-09-11 09:34:01,927 [trainer.py] => CNN top1 curve: [91.5, 89.66]
2023-09-11 09:34:01,927 [trainer.py] => CNN top5 curve: [99.0, 98.5]

2023-09-11 09:34:01,927 [trainer.py] => Average Accuracy (CNN): 90.58
2023-09-11 09:34:01,929 [trainer.py] => All params: 171735553
2023-09-11 09:34:01,930 [trainer.py] => Trainable params: 85936897
2023-09-11 09:34:01,932 [adam_vpt.py] => Learning on 60-90
2023-09-11 09:35:23,571 [trainer.py] => No NME accuracy.
2023-09-11 09:35:23,571 [trainer.py] => CNN: {'total': 86.65, '00-09': 85.5, '10-19': 85.5, '20-29': 82.5, '30-39': 88.5, '40-49': 85.0, '50-59': 92.46, '60-69': 92.0, '70-79': 88.5, '80-89': 79.9, 'old': 86.57, 'new': 86.81}
2023-09-11 09:35:23,571 [trainer.py] => CNN top1 curve: [91.5, 89.66, 86.65]
2023-09-11 09:35:23,571 [trainer.py] => CNN top5 curve: [99.0, 98.5, 98.0]

2023-09-11 09:35:23,571 [trainer.py] => Average Accuracy (CNN): 89.27
2023-09-11 09:35:23,573 [trainer.py] => All params: 171781633
2023-09-11 09:35:23,574 [trainer.py] => Trainable params: 85982977
2023-09-11 09:35:23,577 [adam_vpt.py] => Learning on 90-120
2023-09-11 09:36:52,912 [trainer.py] => No NME accuracy.
2023-09-11 09:36:52,913 [trainer.py] => CNN: {'total': 83.17, '00-09': 84.0, '10-19': 83.0, '20-29': 82.0, '30-39': 87.0, '40-49': 83.5, '50-59': 86.93, '60-69': 88.0, '70-79': 86.5, '80-89': 76.38, '90-99': 80.9, '100-109': 78.89, '110-119': 80.9, 'old': 84.15, 'new': 80.23}
2023-09-11 09:36:52,913 [trainer.py] => CNN top1 curve: [91.5, 89.66, 86.65, 83.17]
2023-09-11 09:36:52,913 [trainer.py] => CNN top5 curve: [99.0, 98.5, 98.0, 96.74]

2023-09-11 09:36:52,913 [trainer.py] => Average Accuracy (CNN): 87.745
2023-09-11 09:36:52,914 [trainer.py] => All params: 171827713
2023-09-11 09:36:52,916 [trainer.py] => Trainable params: 86029057
2023-09-11 09:36:52,918 [adam_vpt.py] => Learning on 120-150
2023-09-11 09:38:26,458 [trainer.py] => No NME accuracy.
2023-09-11 09:38:26,458 [trainer.py] => CNN: {'total': 81.03, '00-09': 81.5, '10-19': 83.0, '20-29': 81.0, '30-39': 86.5, '40-49': 81.0, '50-59': 80.4, '60-69': 83.5, '70-79': 85.5, '80-89': 74.87, '90-99': 79.9, '100-109': 75.38, '110-119': 74.87, '120-129': 84.5, '130-139': 82.5, '140-149': 80.9, 'old': 80.63, 'new': 82.64}
2023-09-11 09:38:26,458 [trainer.py] => CNN top1 curve: [91.5, 89.66, 86.65, 83.17, 81.03]
2023-09-11 09:38:26,458 [trainer.py] => CNN top5 curve: [99.0, 98.5, 98.0, 96.74, 95.49]

2023-09-11 09:38:26,458 [trainer.py] => Average Accuracy (CNN): 86.402
2023-09-11 09:38:26,460 [trainer.py] => All params: 171873793
2023-09-11 09:38:26,461 [trainer.py] => Trainable params: 86075137
2023-09-11 09:38:26,464 [adam_vpt.py] => Learning on 150-180
2023-09-11 09:40:00,922 [trainer.py] => No NME accuracy.
2023-09-11 09:40:00,922 [trainer.py] => CNN: {'total': 78.34, '00-09': 79.5, '10-19': 80.0, '20-29': 77.5, '30-39': 84.5, '40-49': 79.0, '50-59': 78.89, '60-69': 82.5, '70-79': 82.5, '80-89': 71.86, '90-99': 79.4, '100-109': 72.36, '110-119': 72.86, '120-129': 83.5, '130-139': 82.5, '140-149': 78.89, '150-159': 77.39, '160-169': 75.88, '170-179': 71.0, 'old': 79.06, 'new': 74.75}
2023-09-11 09:40:00,922 [trainer.py] => CNN top1 curve: [91.5, 89.66, 86.65, 83.17, 81.03, 78.34]
2023-09-11 09:40:00,922 [trainer.py] => CNN top5 curve: [99.0, 98.5, 98.0, 96.74, 95.49, 94.27]

2023-09-11 09:40:00,923 [trainer.py] => Average Accuracy (CNN): 85.05833333333334
2023-09-11 09:40:00,923 [trainer.py] => All params: 171919873
2023-09-11 09:40:00,924 [trainer.py] => Trainable params: 86121217
2023-09-11 09:40:00,926 [adam_vpt.py] => Learning on 180-210
2023-09-11 09:41:40,268 [trainer.py] => No NME accuracy.
2023-09-11 09:41:40,268 [trainer.py] => CNN: {'total': 77.25, '00-09': 79.0, '10-19': 80.0, '20-29': 77.0, '30-39': 84.5, '40-49': 78.0, '50-59': 78.39, '60-69': 81.5, '70-79': 81.5, '80-89': 69.85, '90-99': 77.39, '100-109': 71.86, '110-119': 69.85, '120-129': 83.5, '130-139': 81.0, '140-149': 78.39, '150-159': 74.37, '160-169': 73.87, '170-179': 70.5, '180-189': 74.24, '190-199': 84.5, '200-209': 72.86, 'old': 77.26, 'new': 77.22}
2023-09-11 09:41:40,268 [trainer.py] => CNN top1 curve: [91.5, 89.66, 86.65, 83.17, 81.03, 78.34, 77.25]
2023-09-11 09:41:40,268 [trainer.py] => CNN top5 curve: [99.0, 98.5, 98.0, 96.74, 95.49, 94.27, 93.91]

2023-09-11 09:41:40,268 [trainer.py] => Average Accuracy (CNN): 83.94285714285715
2023-09-11 09:41:40,270 [trainer.py] => All params: 171965953
2023-09-11 09:41:40,271 [trainer.py] => Trainable params: 86167297
2023-09-11 09:41:40,273 [adam_vpt.py] => Learning on 210-240
2023-09-11 09:43:27,533 [trainer.py] => No NME accuracy.
2023-09-11 09:43:27,533 [trainer.py] => CNN: {'total': 75.63, '00-09': 76.5, '10-19': 78.0, '20-29': 75.5, '30-39': 84.5, '40-49': 76.5, '50-59': 78.39, '60-69': 80.5, '70-79': 81.5, '80-89': 69.35, '90-99': 77.39, '100-109': 64.32, '110-119': 69.35, '120-129': 80.5, '130-139': 81.0, '140-149': 77.89, '150-159': 74.37, '160-169': 73.87, '170-179': 68.5, '180-189': 73.74, '190-199': 84.0, '200-209': 69.35, '210-219': 51.0, '220-229': 88.44, '230-239': 80.5, 'old': 75.96, 'new': 73.29}
2023-09-11 09:43:27,533 [trainer.py] => CNN top1 curve: [91.5, 89.66, 86.65, 83.17, 81.03, 78.34, 77.25, 75.63]
2023-09-11 09:43:27,533 [trainer.py] => CNN top5 curve: [99.0, 98.5, 98.0, 96.74, 95.49, 94.27, 93.91, 92.73]

2023-09-11 09:43:27,533 [trainer.py] => Average Accuracy (CNN): 82.90375
2023-09-11 09:43:27,535 [trainer.py] => All params: 172012033
2023-09-11 09:43:27,535 [trainer.py] => Trainable params: 86213377
2023-09-11 09:43:27,538 [adam_vpt.py] => Learning on 240-270
2023-09-11 09:45:19,032 [trainer.py] => No NME accuracy.
2023-09-11 09:45:19,032 [trainer.py] => CNN: {'total': 74.61, '00-09': 76.5, '10-19': 77.5, '20-29': 75.5, '30-39': 83.5, '40-49': 76.5, '50-59': 77.89, '60-69': 79.5, '70-79': 81.0, '80-89': 68.84, '90-99': 75.88, '100-109': 63.82, '110-119': 65.83, '120-129': 76.0, '130-139': 79.5, '140-149': 73.87, '150-159': 71.36, '160-169': 73.87, '170-179': 68.5, '180-189': 73.74, '190-199': 82.0, '200-209': 67.34, '210-219': 51.0, '220-229': 88.44, '230-239': 80.5, '240-249': 69.0, '250-259': 76.38, '260-269': 80.5, 'old': 74.52, 'new': 75.29}
2023-09-11 09:45:19,032 [trainer.py] => CNN top1 curve: [91.5, 89.66, 86.65, 83.17, 81.03, 78.34, 77.25, 75.63, 74.61]
2023-09-11 09:45:19,032 [trainer.py] => CNN top5 curve: [99.0, 98.5, 98.0, 96.74, 95.49, 94.27, 93.91, 92.73, 92.7]

2023-09-11 09:45:19,033 [trainer.py] => Average Accuracy (CNN): 81.98222222222222
2023-09-11 09:45:19,034 [trainer.py] => All params: 172058113
2023-09-11 09:45:19,035 [trainer.py] => Trainable params: 86259457
2023-09-11 09:45:19,040 [adam_vpt.py] => Learning on 270-300
2023-09-11 09:47:08,988 [trainer.py] => No NME accuracy.
2023-09-11 09:47:08,988 [trainer.py] => CNN: {'total': 74.55, '00-09': 76.0, '10-19': 77.5, '20-29': 75.0, '30-39': 83.5, '40-49': 72.5, '50-59': 77.89, '60-69': 79.5, '70-79': 80.0, '80-89': 68.84, '90-99': 71.86, '100-109': 63.32, '110-119': 65.33, '120-129': 76.0, '130-139': 78.5, '140-149': 73.87, '150-159': 71.36, '160-169': 72.36, '170-179': 68.0, '180-189': 72.73, '190-199': 81.5, '200-209': 66.83, '210-219': 49.5, '220-229': 88.44, '230-239': 79.5, '240-249': 67.0, '250-259': 75.88, '260-269': 79.5, '270-279': 83.42, '280-289': 80.0, '290-299': 80.9, 'old': 73.79, 'new': 81.44}
2023-09-11 09:47:08,988 [trainer.py] => CNN top1 curve: [91.5, 89.66, 86.65, 83.17, 81.03, 78.34, 77.25, 75.63, 74.61, 74.55]
2023-09-11 09:47:08,988 [trainer.py] => CNN top5 curve: [99.0, 98.5, 98.0, 96.74, 95.49, 94.27, 93.91, 92.73, 92.7, 92.46]

2023-09-11 09:47:08,988 [trainer.py] => Average Accuracy (CNN): 81.239
2023-09-11 10:06:58,483 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 10:06:58,483 [trainer.py] => prefix:  
2023-09-11 10:06:58,483 [trainer.py] => dataset: omnibenchmark
2023-09-11 10:06:58,483 [trainer.py] => memory_size: 0
2023-09-11 10:06:58,483 [trainer.py] => memory_per_class: 0
2023-09-11 10:06:58,483 [trainer.py] => fixed_memory: False
2023-09-11 10:06:58,483 [trainer.py] => shuffle: True
2023-09-11 10:06:58,483 [trainer.py] => init_cls: 30
2023-09-11 10:06:58,483 [trainer.py] => increment: 30
2023-09-11 10:06:58,484 [trainer.py] => model_name: adam_vpt
2023-09-11 10:06:58,484 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 10:06:58,484 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 10:06:58,484 [trainer.py] => seed: 1993
2023-09-11 10:06:58,484 [trainer.py] => tuned_epoch: 20
2023-09-11 10:06:58,484 [trainer.py] => init_lr: 0.02
2023-09-11 10:06:58,484 [trainer.py] => batch_size: 96
2023-09-11 10:06:58,484 [trainer.py] => weight_decay: 0.0005
2023-09-11 10:06:58,484 [trainer.py] => min_lr: 0
2023-09-11 10:06:58,484 [trainer.py] => optimizer: sgd
2023-09-11 10:06:58,484 [trainer.py] => vpt_type: deep
2023-09-11 10:06:58,484 [trainer.py] => prompt_token_num: 5
2023-09-11 10:06:58,484 [trainer.py] => use_A: True
2023-09-11 10:06:58,676 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 10:07:00,271 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 10:07:00,770 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 10:07:01,563 [trainer.py] => All params: 85844736
2023-09-11 10:07:01,563 [trainer.py] => Trainable params: 46080
2023-09-11 10:07:01,706 [adam_vpt.py] => Learning on 0-30
2023-09-11 10:09:20,048 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 10:09:20,048 [trainer.py] => prefix:  
2023-09-11 10:09:20,048 [trainer.py] => dataset: omnibenchmark
2023-09-11 10:09:20,048 [trainer.py] => memory_size: 0
2023-09-11 10:09:20,048 [trainer.py] => memory_per_class: 0
2023-09-11 10:09:20,048 [trainer.py] => fixed_memory: False
2023-09-11 10:09:20,048 [trainer.py] => shuffle: True
2023-09-11 10:09:20,048 [trainer.py] => init_cls: 30
2023-09-11 10:09:20,048 [trainer.py] => increment: 30
2023-09-11 10:09:20,048 [trainer.py] => model_name: adam_vpt
2023-09-11 10:09:20,048 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 10:09:20,048 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 10:09:20,048 [trainer.py] => seed: 1993
2023-09-11 10:09:20,048 [trainer.py] => tuned_epoch: 20
2023-09-11 10:09:20,048 [trainer.py] => init_lr: 0.02
2023-09-11 10:09:20,048 [trainer.py] => batch_size: 96
2023-09-11 10:09:20,048 [trainer.py] => weight_decay: 0.0005
2023-09-11 10:09:20,048 [trainer.py] => min_lr: 0
2023-09-11 10:09:20,048 [trainer.py] => optimizer: sgd
2023-09-11 10:09:20,048 [trainer.py] => vpt_type: deep
2023-09-11 10:09:20,048 [trainer.py] => prompt_token_num: 5
2023-09-11 10:09:20,048 [trainer.py] => use_A: True
2023-09-11 10:09:20,237 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 10:09:21,851 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 10:09:22,374 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 10:09:23,205 [trainer.py] => All params: 85844736
2023-09-11 10:09:23,205 [trainer.py] => Trainable params: 46080
2023-09-11 10:09:23,346 [adam_vpt.py] => Learning on 0-30
2023-09-11 10:10:18,379 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 2.625, Train_accy 53.01
2023-09-11 10:11:20,456 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 0.756, Train_accy 78.73, Test_accy 84.17
2023-09-11 10:12:22,120 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.575, Train_accy 81.76, Test_accy 89.00
2023-09-11 10:12:57,207 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 10:12:57,207 [trainer.py] => prefix:  
2023-09-11 10:12:57,207 [trainer.py] => dataset: omnibenchmark
2023-09-11 10:12:57,207 [trainer.py] => memory_size: 0
2023-09-11 10:12:57,208 [trainer.py] => memory_per_class: 0
2023-09-11 10:12:57,208 [trainer.py] => fixed_memory: False
2023-09-11 10:12:57,208 [trainer.py] => shuffle: True
2023-09-11 10:12:57,208 [trainer.py] => init_cls: 30
2023-09-11 10:12:57,208 [trainer.py] => increment: 30
2023-09-11 10:12:57,208 [trainer.py] => model_name: adam_vpt
2023-09-11 10:12:57,208 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 10:12:57,208 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 10:12:57,208 [trainer.py] => seed: 1993
2023-09-11 10:12:57,208 [trainer.py] => tuned_epoch: 20
2023-09-11 10:12:57,208 [trainer.py] => init_lr: 0.02
2023-09-11 10:12:57,209 [trainer.py] => batch_size: 96
2023-09-11 10:12:57,209 [trainer.py] => weight_decay: 0.0005
2023-09-11 10:12:57,209 [trainer.py] => min_lr: 0
2023-09-11 10:12:57,209 [trainer.py] => optimizer: sgd
2023-09-11 10:12:57,209 [trainer.py] => vpt_type: deep
2023-09-11 10:12:57,209 [trainer.py] => prompt_token_num: 5
2023-09-11 10:12:57,209 [trainer.py] => use_A: True
2023-09-11 10:13:52,116 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 10:13:52,116 [trainer.py] => prefix:  
2023-09-11 10:13:52,116 [trainer.py] => dataset: omnibenchmark
2023-09-11 10:13:52,116 [trainer.py] => memory_size: 0
2023-09-11 10:13:52,116 [trainer.py] => memory_per_class: 0
2023-09-11 10:13:52,116 [trainer.py] => fixed_memory: False
2023-09-11 10:13:52,116 [trainer.py] => shuffle: True
2023-09-11 10:13:52,116 [trainer.py] => init_cls: 30
2023-09-11 10:13:52,116 [trainer.py] => increment: 30
2023-09-11 10:13:52,116 [trainer.py] => model_name: adam_vpt
2023-09-11 10:13:52,116 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 10:13:52,116 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 10:13:52,116 [trainer.py] => seed: 1993
2023-09-11 10:13:52,116 [trainer.py] => tuned_epoch: 1
2023-09-11 10:13:52,116 [trainer.py] => init_lr: 0.02
2023-09-11 10:13:52,116 [trainer.py] => batch_size: 96
2023-09-11 10:13:52,116 [trainer.py] => weight_decay: 0.0005
2023-09-11 10:13:52,116 [trainer.py] => min_lr: 0
2023-09-11 10:13:52,116 [trainer.py] => optimizer: sgd
2023-09-11 10:13:52,116 [trainer.py] => vpt_type: deep
2023-09-11 10:13:52,116 [trainer.py] => prompt_token_num: 5
2023-09-11 10:13:52,116 [trainer.py] => use_A: True
2023-09-11 10:13:52,311 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 10:13:53,968 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 10:13:54,470 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 10:13:55,269 [trainer.py] => All params: 85844736
2023-09-11 10:13:55,269 [trainer.py] => Trainable params: 46080
2023-09-11 10:13:55,416 [adam_vpt.py] => Learning on 0-30
2023-09-11 10:14:50,488 [adam_vpt.py] => Task 0, Epoch 1/1 => Loss 2.670, Train_accy 50.49
2023-09-11 10:14:51,315 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 10:14:51,577 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 10:14:53,099 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 10:14:53,360 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 10:16:07,935 [trainer.py] => No NME accuracy.
2023-09-11 10:16:07,935 [trainer.py] => CNN: {'total': 87.67, '00-09': 91.0, '10-19': 88.5, '20-29': 83.5, 'old': 0, 'new': 87.67}
2023-09-11 10:16:07,935 [trainer.py] => CNN top1 curve: [87.67]
2023-09-11 10:16:07,935 [trainer.py] => CNN top5 curve: [99.33]

2023-09-11 10:16:07,935 [trainer.py] => Average Accuracy (CNN): 87.67
2023-09-11 10:16:07,937 [trainer.py] => All params: 171689473
2023-09-11 10:16:07,938 [trainer.py] => Trainable params: 85890817
2023-09-11 10:16:07,940 [adam_vpt.py] => Learning on 30-60
2023-09-11 10:17:26,445 [trainer.py] => No NME accuracy.
2023-09-11 10:17:26,445 [trainer.py] => CNN: {'total': 87.49, '00-09': 85.5, '10-19': 87.5, '20-29': 80.5, '30-39': 90.0, '40-49': 88.0, '50-59': 93.47, 'old': 84.5, 'new': 90.48}
2023-09-11 10:17:26,445 [trainer.py] => CNN top1 curve: [87.67, 87.49]
2023-09-11 10:17:26,445 [trainer.py] => CNN top5 curve: [99.33, 97.66]

2023-09-11 10:17:26,445 [trainer.py] => Average Accuracy (CNN): 87.58
2023-09-11 10:17:26,447 [trainer.py] => All params: 171735553
2023-09-11 10:17:26,448 [trainer.py] => Trainable params: 85936897
2023-09-11 10:17:26,451 [adam_vpt.py] => Learning on 60-90
2023-09-11 10:18:47,980 [trainer.py] => No NME accuracy.
2023-09-11 10:18:47,981 [trainer.py] => CNN: {'total': 84.76, '00-09': 82.0, '10-19': 82.5, '20-29': 76.0, '30-39': 87.5, '40-49': 86.0, '50-59': 92.96, '60-69': 91.0, '70-79': 86.5, '80-89': 78.39, 'old': 84.49, 'new': 85.31}
2023-09-11 10:18:47,981 [trainer.py] => CNN top1 curve: [87.67, 87.49, 84.76]
2023-09-11 10:18:47,981 [trainer.py] => CNN top5 curve: [99.33, 97.66, 96.89]

2023-09-11 10:18:47,981 [trainer.py] => Average Accuracy (CNN): 86.64
2023-09-11 10:18:47,982 [trainer.py] => All params: 171781633
2023-09-11 10:18:47,983 [trainer.py] => Trainable params: 85982977
2023-09-11 10:18:47,985 [adam_vpt.py] => Learning on 90-120
2023-09-11 10:19:35,573 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 10:19:35,573 [trainer.py] => prefix:  
2023-09-11 10:19:35,573 [trainer.py] => dataset: omnibenchmark
2023-09-11 10:19:35,573 [trainer.py] => memory_size: 0
2023-09-11 10:19:35,573 [trainer.py] => memory_per_class: 0
2023-09-11 10:19:35,573 [trainer.py] => fixed_memory: False
2023-09-11 10:19:35,573 [trainer.py] => shuffle: True
2023-09-11 10:19:35,573 [trainer.py] => init_cls: 30
2023-09-11 10:19:35,573 [trainer.py] => increment: 30
2023-09-11 10:19:35,573 [trainer.py] => model_name: adam_vpt
2023-09-11 10:19:35,573 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 10:19:35,573 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 10:19:35,573 [trainer.py] => seed: 1993
2023-09-11 10:19:35,573 [trainer.py] => tuned_epoch: 20
2023-09-11 10:19:35,574 [trainer.py] => init_lr: 0.02
2023-09-11 10:19:35,574 [trainer.py] => batch_size: 96
2023-09-11 10:19:35,574 [trainer.py] => weight_decay: 0.0005
2023-09-11 10:19:35,574 [trainer.py] => min_lr: 0
2023-09-11 10:19:35,574 [trainer.py] => optimizer: sgd
2023-09-11 10:19:35,574 [trainer.py] => vpt_type: deep
2023-09-11 10:19:35,574 [trainer.py] => prompt_token_num: 5
2023-09-11 10:19:35,574 [trainer.py] => use_A: True
2023-09-11 10:19:35,764 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 10:19:37,381 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 10:19:37,876 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 10:19:38,669 [trainer.py] => All params: 85844736
2023-09-11 10:19:38,669 [trainer.py] => Trainable params: 46080
2023-09-11 10:19:38,815 [adam_vpt.py] => Learning on 0-30
2023-09-11 10:20:33,967 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 2.609, Train_accy 53.29
2023-09-11 10:21:36,279 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 0.758, Train_accy 78.48, Test_accy 84.33
2023-09-11 10:22:38,215 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.578, Train_accy 81.94, Test_accy 89.83
2023-09-11 10:23:40,295 [adam_vpt.py] => Task 0, Epoch 4/20 => Loss 0.527, Train_accy 82.97, Test_accy 87.50
2023-09-11 10:24:42,794 [adam_vpt.py] => Task 0, Epoch 5/20 => Loss 0.502, Train_accy 84.60, Test_accy 89.17
2023-09-11 10:25:38,407 [adam_vpt.py] => Task 0, Epoch 6/20 => Loss 0.468, Train_accy 85.46
2023-09-11 10:26:39,954 [adam_vpt.py] => Task 0, Epoch 7/20 => Loss 0.448, Train_accy 86.16, Test_accy 90.67
2023-09-11 10:27:41,775 [adam_vpt.py] => Task 0, Epoch 8/20 => Loss 0.435, Train_accy 86.41, Test_accy 91.00
2023-09-11 10:28:43,790 [adam_vpt.py] => Task 0, Epoch 9/20 => Loss 0.425, Train_accy 86.74, Test_accy 89.83
2023-09-11 10:29:45,659 [adam_vpt.py] => Task 0, Epoch 10/20 => Loss 0.418, Train_accy 86.49, Test_accy 91.00
2023-09-11 10:30:41,721 [adam_vpt.py] => Task 0, Epoch 11/20 => Loss 0.408, Train_accy 87.18
2023-09-11 10:31:43,796 [adam_vpt.py] => Task 0, Epoch 12/20 => Loss 0.404, Train_accy 87.56, Test_accy 90.83
2023-09-11 10:32:45,367 [adam_vpt.py] => Task 0, Epoch 13/20 => Loss 0.387, Train_accy 87.76, Test_accy 92.33
2023-09-11 10:33:47,337 [adam_vpt.py] => Task 0, Epoch 14/20 => Loss 0.390, Train_accy 87.97, Test_accy 91.83
2023-09-11 10:34:49,312 [adam_vpt.py] => Task 0, Epoch 15/20 => Loss 0.381, Train_accy 87.83, Test_accy 91.00
2023-09-11 10:35:44,300 [adam_vpt.py] => Task 0, Epoch 16/20 => Loss 0.382, Train_accy 87.79
2023-09-11 10:36:46,103 [adam_vpt.py] => Task 0, Epoch 17/20 => Loss 0.387, Train_accy 88.23, Test_accy 92.83
2023-09-11 10:37:47,640 [adam_vpt.py] => Task 0, Epoch 18/20 => Loss 0.383, Train_accy 87.94, Test_accy 92.83
2023-09-11 10:38:49,825 [adam_vpt.py] => Task 0, Epoch 19/20 => Loss 0.383, Train_accy 88.34, Test_accy 92.17
2023-09-11 10:39:51,872 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.371, Train_accy 88.51, Test_accy 92.33
2023-09-11 10:39:52,836 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 10:39:53,342 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 10:39:54,789 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 10:39:55,044 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 10:41:09,119 [trainer.py] => No NME accuracy.
2023-09-11 10:41:09,119 [trainer.py] => CNN: {'total': 89.33, '00-09': 91.0, '10-19': 90.5, '20-29': 86.5, 'old': 0, 'new': 89.33}
2023-09-11 10:41:09,119 [trainer.py] => CNN top1 curve: [89.33]
2023-09-11 10:41:09,119 [trainer.py] => CNN top5 curve: [99.17]

2023-09-11 10:41:09,119 [trainer.py] => Average Accuracy (CNN): 89.33
2023-09-11 10:41:09,121 [trainer.py] => All params: 171689473
2023-09-11 10:41:09,122 [trainer.py] => Trainable params: 85890817
2023-09-11 10:41:09,124 [adam_vpt.py] => Learning on 30-60
2023-09-11 10:42:27,469 [trainer.py] => No NME accuracy.
2023-09-11 10:42:27,469 [trainer.py] => CNN: {'total': 88.82, '00-09': 85.5, '10-19': 90.0, '20-29': 84.0, '30-39': 91.0, '40-49': 88.0, '50-59': 94.47, 'old': 86.5, 'new': 91.15}
2023-09-11 10:42:27,469 [trainer.py] => CNN top1 curve: [89.33, 88.82]
2023-09-11 10:42:27,469 [trainer.py] => CNN top5 curve: [99.17, 98.67]

2023-09-11 10:42:27,469 [trainer.py] => Average Accuracy (CNN): 89.07499999999999
2023-09-11 10:42:27,471 [trainer.py] => All params: 171735553
2023-09-11 10:42:27,472 [trainer.py] => Trainable params: 85936897
2023-09-11 10:42:27,474 [adam_vpt.py] => Learning on 60-90
2023-09-11 10:43:48,910 [trainer.py] => No NME accuracy.
2023-09-11 10:43:48,910 [trainer.py] => CNN: {'total': 86.26, '00-09': 83.5, '10-19': 83.5, '20-29': 82.0, '30-39': 89.5, '40-49': 86.5, '50-59': 92.96, '60-69': 92.0, '70-79': 87.0, '80-89': 79.4, 'old': 86.32, 'new': 86.14}
2023-09-11 10:43:48,910 [trainer.py] => CNN top1 curve: [89.33, 88.82, 86.26]
2023-09-11 10:43:48,910 [trainer.py] => CNN top5 curve: [99.17, 98.67, 97.72]

2023-09-11 10:43:48,910 [trainer.py] => Average Accuracy (CNN): 88.13666666666666
2023-09-11 10:43:48,912 [trainer.py] => All params: 171781633
2023-09-11 10:43:48,913 [trainer.py] => Trainable params: 85982977
2023-09-11 10:43:48,915 [adam_vpt.py] => Learning on 90-120
2023-09-11 10:45:17,999 [trainer.py] => No NME accuracy.
2023-09-11 10:45:17,999 [trainer.py] => CNN: {'total': 82.59, '00-09': 80.5, '10-19': 81.0, '20-29': 81.5, '30-39': 87.5, '40-49': 84.5, '50-59': 86.43, '60-69': 87.5, '70-79': 84.5, '80-89': 75.88, '90-99': 80.9, '100-109': 79.9, '110-119': 80.9, 'old': 83.26, 'new': 80.57}
2023-09-11 10:45:17,999 [trainer.py] => CNN top1 curve: [89.33, 88.82, 86.26, 82.59]
2023-09-11 10:45:17,999 [trainer.py] => CNN top5 curve: [99.17, 98.67, 97.72, 96.49]

2023-09-11 10:45:17,999 [trainer.py] => Average Accuracy (CNN): 86.75
2023-09-11 10:45:18,000 [trainer.py] => All params: 171827713
2023-09-11 10:45:18,001 [trainer.py] => Trainable params: 86029057
2023-09-11 10:45:18,002 [adam_vpt.py] => Learning on 120-150
2023-09-11 10:46:51,140 [trainer.py] => No NME accuracy.
2023-09-11 10:46:51,141 [trainer.py] => CNN: {'total': 80.53, '00-09': 78.0, '10-19': 80.5, '20-29': 81.0, '30-39': 87.0, '40-49': 82.0, '50-59': 80.4, '60-69': 83.5, '70-79': 83.0, '80-89': 74.87, '90-99': 80.4, '100-109': 75.88, '110-119': 74.87, '120-129': 84.0, '130-139': 80.0, '140-149': 82.41, 'old': 80.13, 'new': 82.14}
2023-09-11 10:46:51,141 [trainer.py] => CNN top1 curve: [89.33, 88.82, 86.26, 82.59, 80.53]
2023-09-11 10:46:51,141 [trainer.py] => CNN top5 curve: [99.17, 98.67, 97.72, 96.49, 95.16]

2023-09-11 10:46:51,141 [trainer.py] => Average Accuracy (CNN): 85.506
2023-09-11 10:46:51,142 [trainer.py] => All params: 171873793
2023-09-11 10:46:51,144 [trainer.py] => Trainable params: 86075137
2023-09-11 10:46:51,147 [adam_vpt.py] => Learning on 150-180
2023-09-11 10:48:24,999 [trainer.py] => No NME accuracy.
2023-09-11 10:48:24,999 [trainer.py] => CNN: {'total': 77.84, '00-09': 75.5, '10-19': 77.0, '20-29': 79.5, '30-39': 85.5, '40-49': 78.0, '50-59': 78.89, '60-69': 83.0, '70-79': 80.0, '80-89': 72.36, '90-99': 80.4, '100-109': 73.37, '110-119': 72.86, '120-129': 83.5, '130-139': 80.0, '140-149': 80.4, '150-159': 78.39, '160-169': 75.38, '170-179': 67.0, 'old': 78.69, 'new': 73.58}
2023-09-11 10:48:24,999 [trainer.py] => CNN top1 curve: [89.33, 88.82, 86.26, 82.59, 80.53, 77.84]
2023-09-11 10:48:25,000 [trainer.py] => CNN top5 curve: [99.17, 98.67, 97.72, 96.49, 95.16, 94.01]

2023-09-11 10:48:25,000 [trainer.py] => Average Accuracy (CNN): 84.22833333333334
2023-09-11 10:48:25,001 [trainer.py] => All params: 171919873
2023-09-11 10:48:25,003 [trainer.py] => Trainable params: 86121217
2023-09-11 10:48:25,005 [adam_vpt.py] => Learning on 180-210
2023-09-11 10:50:03,960 [trainer.py] => No NME accuracy.
2023-09-11 10:50:03,960 [trainer.py] => CNN: {'total': 76.87, '00-09': 75.0, '10-19': 77.0, '20-29': 79.0, '30-39': 85.5, '40-49': 76.5, '50-59': 78.39, '60-69': 81.5, '70-79': 79.5, '80-89': 71.36, '90-99': 77.39, '100-109': 72.86, '110-119': 70.35, '120-129': 83.5, '130-139': 79.0, '140-149': 79.9, '150-159': 75.88, '160-169': 74.37, '170-179': 66.5, '180-189': 73.23, '190-199': 84.5, '200-209': 72.86, 'old': 76.87, 'new': 76.88}
2023-09-11 10:50:03,960 [trainer.py] => CNN top1 curve: [89.33, 88.82, 86.26, 82.59, 80.53, 77.84, 76.87]
2023-09-11 10:50:03,960 [trainer.py] => CNN top5 curve: [99.17, 98.67, 97.72, 96.49, 95.16, 94.01, 93.6]

2023-09-11 10:50:03,960 [trainer.py] => Average Accuracy (CNN): 83.17714285714285
2023-09-11 10:50:03,962 [trainer.py] => All params: 171965953
2023-09-11 10:50:03,963 [trainer.py] => Trainable params: 86167297
2023-09-11 10:50:03,966 [adam_vpt.py] => Learning on 210-240
2023-09-11 10:51:50,683 [trainer.py] => No NME accuracy.
2023-09-11 10:51:50,683 [trainer.py] => CNN: {'total': 75.17, '00-09': 74.0, '10-19': 75.0, '20-29': 77.0, '30-39': 85.5, '40-49': 74.5, '50-59': 78.39, '60-69': 81.0, '70-79': 79.5, '80-89': 70.85, '90-99': 76.88, '100-109': 65.83, '110-119': 69.35, '120-129': 80.0, '130-139': 79.0, '140-149': 78.89, '150-159': 75.38, '160-169': 74.37, '170-179': 64.0, '180-189': 72.73, '190-199': 84.5, '200-209': 68.84, '210-219': 50.5, '220-229': 86.43, '230-239': 81.5, 'old': 75.51, 'new': 72.79}
2023-09-11 10:51:50,683 [trainer.py] => CNN top1 curve: [89.33, 88.82, 86.26, 82.59, 80.53, 77.84, 76.87, 75.17]
2023-09-11 10:51:50,683 [trainer.py] => CNN top5 curve: [99.17, 98.67, 97.72, 96.49, 95.16, 94.01, 93.6, 92.38]

2023-09-11 10:51:50,683 [trainer.py] => Average Accuracy (CNN): 82.17625
2023-09-11 10:51:50,684 [trainer.py] => All params: 172012033
2023-09-11 10:51:50,685 [trainer.py] => Trainable params: 86213377
2023-09-11 10:51:50,687 [adam_vpt.py] => Learning on 240-270
2023-09-11 10:53:41,827 [trainer.py] => No NME accuracy.
2023-09-11 10:53:41,828 [trainer.py] => CNN: {'total': 74.23, '00-09': 74.0, '10-19': 74.5, '20-29': 77.0, '30-39': 85.5, '40-49': 74.5, '50-59': 77.39, '60-69': 79.5, '70-79': 79.0, '80-89': 69.85, '90-99': 75.38, '100-109': 64.32, '110-119': 66.33, '120-129': 75.0, '130-139': 78.0, '140-149': 74.37, '150-159': 72.36, '160-169': 74.37, '170-179': 64.0, '180-189': 72.73, '190-199': 82.5, '200-209': 65.83, '210-219': 50.5, '220-229': 86.43, '230-239': 81.5, '240-249': 70.0, '250-259': 77.39, '260-269': 82.0, 'old': 73.96, 'new': 76.46}
2023-09-11 10:53:41,828 [trainer.py] => CNN top1 curve: [89.33, 88.82, 86.26, 82.59, 80.53, 77.84, 76.87, 75.17, 74.23]
2023-09-11 10:53:41,828 [trainer.py] => CNN top5 curve: [99.17, 98.67, 97.72, 96.49, 95.16, 94.01, 93.6, 92.38, 92.39]

2023-09-11 10:53:41,828 [trainer.py] => Average Accuracy (CNN): 81.29333333333334
2023-09-11 10:53:41,829 [trainer.py] => All params: 172058113
2023-09-11 10:53:41,831 [trainer.py] => Trainable params: 86259457
2023-09-11 10:53:41,833 [adam_vpt.py] => Learning on 270-300
2023-09-11 10:55:31,904 [trainer.py] => No NME accuracy.
2023-09-11 10:55:31,905 [trainer.py] => CNN: {'total': 74.3, '00-09': 74.0, '10-19': 74.5, '20-29': 76.5, '30-39': 85.5, '40-49': 71.5, '50-59': 76.88, '60-69': 79.0, '70-79': 79.0, '80-89': 69.35, '90-99': 72.36, '100-109': 63.32, '110-119': 65.33, '120-129': 75.0, '130-139': 78.0, '140-149': 74.37, '150-159': 72.36, '160-169': 72.86, '170-179': 63.5, '180-189': 72.22, '190-199': 82.0, '200-209': 65.33, '210-219': 50.0, '220-229': 86.43, '230-239': 80.0, '240-249': 68.0, '250-259': 76.88, '260-269': 81.0, '270-279': 81.91, '280-289': 81.5, '290-299': 80.4, 'old': 73.53, 'new': 81.27}
2023-09-11 10:55:31,905 [trainer.py] => CNN top1 curve: [89.33, 88.82, 86.26, 82.59, 80.53, 77.84, 76.87, 75.17, 74.23, 74.3]
2023-09-11 10:55:31,905 [trainer.py] => CNN top5 curve: [99.17, 98.67, 97.72, 96.49, 95.16, 94.01, 93.6, 92.38, 92.39, 92.35]

2023-09-11 10:55:31,905 [trainer.py] => Average Accuracy (CNN): 80.594
2023-09-11 13:07:58,853 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 13:07:58,853 [trainer.py] => prefix:  
2023-09-11 13:07:58,853 [trainer.py] => dataset: omnibenchmark
2023-09-11 13:07:58,853 [trainer.py] => memory_size: 0
2023-09-11 13:07:58,853 [trainer.py] => memory_per_class: 0
2023-09-11 13:07:58,853 [trainer.py] => fixed_memory: False
2023-09-11 13:07:58,853 [trainer.py] => shuffle: True
2023-09-11 13:07:58,853 [trainer.py] => init_cls: 30
2023-09-11 13:07:58,853 [trainer.py] => increment: 30
2023-09-11 13:07:58,853 [trainer.py] => model_name: adam_vpt
2023-09-11 13:07:58,853 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 13:07:58,853 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 13:07:58,853 [trainer.py] => seed: 1993
2023-09-11 13:07:58,853 [trainer.py] => tuned_epoch: 20
2023-09-11 13:07:58,853 [trainer.py] => init_lr: 0.02
2023-09-11 13:07:58,853 [trainer.py] => batch_size: 96
2023-09-11 13:07:58,853 [trainer.py] => weight_decay: 0.0005
2023-09-11 13:07:58,853 [trainer.py] => min_lr: 0
2023-09-11 13:07:58,854 [trainer.py] => optimizer: sgd
2023-09-11 13:07:58,854 [trainer.py] => vpt_type: deep
2023-09-11 13:07:58,854 [trainer.py] => prompt_token_num: 5
2023-09-11 13:07:58,854 [trainer.py] => use_A: False
2023-09-11 13:07:59,045 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 13:08:00,682 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:08:01,192 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:08:02,000 [trainer.py] => All params: 85844736
2023-09-11 13:08:02,001 [trainer.py] => Trainable params: 46080
2023-09-11 13:08:02,150 [adam_vpt.py] => Learning on 0-30
2023-09-11 13:09:01,576 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 13:09:01,576 [trainer.py] => prefix:  
2023-09-11 13:09:01,576 [trainer.py] => dataset: omnibenchmark
2023-09-11 13:09:01,576 [trainer.py] => memory_size: 0
2023-09-11 13:09:01,576 [trainer.py] => memory_per_class: 0
2023-09-11 13:09:01,576 [trainer.py] => fixed_memory: False
2023-09-11 13:09:01,576 [trainer.py] => shuffle: True
2023-09-11 13:09:01,576 [trainer.py] => init_cls: 30
2023-09-11 13:09:01,576 [trainer.py] => increment: 30
2023-09-11 13:09:01,576 [trainer.py] => model_name: adam_vpt
2023-09-11 13:09:01,576 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 13:09:01,576 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 13:09:01,576 [trainer.py] => seed: 1993
2023-09-11 13:09:01,576 [trainer.py] => tuned_epoch: 20
2023-09-11 13:09:01,576 [trainer.py] => init_lr: 0.02
2023-09-11 13:09:01,576 [trainer.py] => batch_size: 96
2023-09-11 13:09:01,576 [trainer.py] => weight_decay: 0.0005
2023-09-11 13:09:01,576 [trainer.py] => min_lr: 0
2023-09-11 13:09:01,576 [trainer.py] => optimizer: sgd
2023-09-11 13:09:01,576 [trainer.py] => vpt_type: deep
2023-09-11 13:09:01,576 [trainer.py] => prompt_token_num: 5
2023-09-11 13:09:01,576 [trainer.py] => use_A: False
2023-09-11 13:09:01,767 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 13:09:03,390 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:09:03,899 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:09:04,689 [trainer.py] => All params: 85844736
2023-09-11 13:09:04,690 [trainer.py] => Trainable params: 46080
2023-09-11 13:09:04,821 [adam_vpt.py] => Learning on 0-30
2023-09-11 13:11:43,637 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 13:11:43,637 [trainer.py] => prefix:  
2023-09-11 13:11:43,637 [trainer.py] => dataset: omnibenchmark
2023-09-11 13:11:43,637 [trainer.py] => memory_size: 0
2023-09-11 13:11:43,637 [trainer.py] => memory_per_class: 0
2023-09-11 13:11:43,637 [trainer.py] => fixed_memory: False
2023-09-11 13:11:43,637 [trainer.py] => shuffle: True
2023-09-11 13:11:43,637 [trainer.py] => init_cls: 30
2023-09-11 13:11:43,637 [trainer.py] => increment: 30
2023-09-11 13:11:43,637 [trainer.py] => model_name: adam_vpt
2023-09-11 13:11:43,637 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 13:11:43,637 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 13:11:43,637 [trainer.py] => seed: 1993
2023-09-11 13:11:43,637 [trainer.py] => tuned_epoch: 20
2023-09-11 13:11:43,637 [trainer.py] => init_lr: 0.02
2023-09-11 13:11:43,637 [trainer.py] => batch_size: 96
2023-09-11 13:11:43,637 [trainer.py] => weight_decay: 0.0005
2023-09-11 13:11:43,637 [trainer.py] => min_lr: 0
2023-09-11 13:11:43,637 [trainer.py] => optimizer: sgd
2023-09-11 13:11:43,637 [trainer.py] => vpt_type: deep
2023-09-11 13:11:43,637 [trainer.py] => prompt_token_num: 5
2023-09-11 13:11:43,637 [trainer.py] => use_A: False
2023-09-11 13:11:43,828 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 13:11:45,436 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:11:45,943 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:11:46,737 [trainer.py] => All params: 85844736
2023-09-11 13:11:46,737 [trainer.py] => Trainable params: 46080
2023-09-11 13:11:46,870 [adam_vpt.py] => Learning on 0-30
2023-09-11 13:14:45,826 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 13:14:45,826 [trainer.py] => prefix:  
2023-09-11 13:14:45,826 [trainer.py] => dataset: omnibenchmark
2023-09-11 13:14:45,826 [trainer.py] => memory_size: 0
2023-09-11 13:14:45,826 [trainer.py] => memory_per_class: 0
2023-09-11 13:14:45,827 [trainer.py] => fixed_memory: False
2023-09-11 13:14:45,827 [trainer.py] => shuffle: True
2023-09-11 13:14:45,827 [trainer.py] => init_cls: 30
2023-09-11 13:14:45,827 [trainer.py] => increment: 30
2023-09-11 13:14:45,827 [trainer.py] => model_name: adam_vpt
2023-09-11 13:14:45,827 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 13:14:45,827 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 13:14:45,827 [trainer.py] => seed: 1993
2023-09-11 13:14:45,827 [trainer.py] => tuned_epoch: 20
2023-09-11 13:14:45,827 [trainer.py] => init_lr: 0.02
2023-09-11 13:14:45,827 [trainer.py] => batch_size: 96
2023-09-11 13:14:45,828 [trainer.py] => weight_decay: 0.0005
2023-09-11 13:14:45,828 [trainer.py] => min_lr: 0
2023-09-11 13:14:45,828 [trainer.py] => optimizer: sgd
2023-09-11 13:14:45,828 [trainer.py] => vpt_type: deep
2023-09-11 13:14:45,828 [trainer.py] => prompt_token_num: 5
2023-09-11 13:14:45,828 [trainer.py] => use_A: False
2023-09-11 13:14:46,501 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 13:14:48,753 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:14:49,280 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:14:50,087 [trainer.py] => All params: 85844736
2023-09-11 13:14:50,096 [trainer.py] => Trainable params: 46080
2023-09-11 13:14:50,241 [adam_vpt.py] => Learning on 0-30
2023-09-11 13:17:02,137 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 13:17:02,137 [trainer.py] => prefix:  
2023-09-11 13:17:02,137 [trainer.py] => dataset: omnibenchmark
2023-09-11 13:17:02,137 [trainer.py] => memory_size: 0
2023-09-11 13:17:02,137 [trainer.py] => memory_per_class: 0
2023-09-11 13:17:02,137 [trainer.py] => fixed_memory: False
2023-09-11 13:17:02,137 [trainer.py] => shuffle: True
2023-09-11 13:17:02,138 [trainer.py] => init_cls: 30
2023-09-11 13:17:02,138 [trainer.py] => increment: 30
2023-09-11 13:17:02,138 [trainer.py] => model_name: adam_vpt
2023-09-11 13:17:02,138 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 13:17:02,138 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 13:17:02,138 [trainer.py] => seed: 1993
2023-09-11 13:17:02,138 [trainer.py] => tuned_epoch: 20
2023-09-11 13:17:02,138 [trainer.py] => init_lr: 0.02
2023-09-11 13:17:02,138 [trainer.py] => batch_size: 96
2023-09-11 13:17:02,138 [trainer.py] => weight_decay: 0.0005
2023-09-11 13:17:02,138 [trainer.py] => min_lr: 0
2023-09-11 13:17:02,139 [trainer.py] => optimizer: sgd
2023-09-11 13:17:02,139 [trainer.py] => vpt_type: deep
2023-09-11 13:17:02,139 [trainer.py] => prompt_token_num: 5
2023-09-11 13:17:02,139 [trainer.py] => use_A: False
2023-09-11 13:17:02,818 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 13:17:05,025 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:17:05,534 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:17:06,349 [trainer.py] => All params: 85844736
2023-09-11 13:17:06,359 [trainer.py] => Trainable params: 46080
2023-09-11 13:17:06,497 [adam_vpt.py] => Learning on 0-30
2023-09-11 13:18:41,309 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 13:18:41,309 [trainer.py] => prefix:  
2023-09-11 13:18:41,309 [trainer.py] => dataset: omnibenchmark
2023-09-11 13:18:41,309 [trainer.py] => memory_size: 0
2023-09-11 13:18:41,310 [trainer.py] => memory_per_class: 0
2023-09-11 13:18:41,310 [trainer.py] => fixed_memory: False
2023-09-11 13:18:41,310 [trainer.py] => shuffle: True
2023-09-11 13:18:41,310 [trainer.py] => init_cls: 30
2023-09-11 13:18:41,310 [trainer.py] => increment: 30
2023-09-11 13:18:41,310 [trainer.py] => model_name: adam_vpt
2023-09-11 13:18:41,310 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 13:18:41,310 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 13:18:41,310 [trainer.py] => seed: 1993
2023-09-11 13:18:41,310 [trainer.py] => tuned_epoch: 20
2023-09-11 13:18:41,311 [trainer.py] => init_lr: 0.02
2023-09-11 13:18:41,311 [trainer.py] => batch_size: 96
2023-09-11 13:18:41,311 [trainer.py] => weight_decay: 0.0005
2023-09-11 13:18:41,311 [trainer.py] => min_lr: 0
2023-09-11 13:18:41,311 [trainer.py] => optimizer: sgd
2023-09-11 13:18:41,311 [trainer.py] => vpt_type: deep
2023-09-11 13:18:41,311 [trainer.py] => prompt_token_num: 5
2023-09-11 13:18:41,311 [trainer.py] => use_A: False
2023-09-11 13:18:42,019 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 13:18:44,202 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:18:44,713 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:18:45,634 [trainer.py] => All params: 85844736
2023-09-11 13:18:45,644 [trainer.py] => Trainable params: 46080
2023-09-11 13:18:45,785 [adam_vpt.py] => Learning on 0-30
2023-09-11 13:23:39,655 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 13:23:39,655 [trainer.py] => prefix:  
2023-09-11 13:23:39,656 [trainer.py] => dataset: omnibenchmark
2023-09-11 13:23:39,656 [trainer.py] => memory_size: 0
2023-09-11 13:23:39,656 [trainer.py] => memory_per_class: 0
2023-09-11 13:23:39,656 [trainer.py] => fixed_memory: False
2023-09-11 13:23:39,656 [trainer.py] => shuffle: True
2023-09-11 13:23:39,656 [trainer.py] => init_cls: 30
2023-09-11 13:23:39,656 [trainer.py] => increment: 30
2023-09-11 13:23:39,656 [trainer.py] => model_name: adam_vpt
2023-09-11 13:23:39,656 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 13:23:39,656 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 13:23:39,656 [trainer.py] => seed: 1993
2023-09-11 13:23:39,656 [trainer.py] => tuned_epoch: 20
2023-09-11 13:23:39,656 [trainer.py] => init_lr: 0.02
2023-09-11 13:23:39,656 [trainer.py] => batch_size: 96
2023-09-11 13:23:39,656 [trainer.py] => weight_decay: 0.0005
2023-09-11 13:23:39,656 [trainer.py] => min_lr: 0
2023-09-11 13:23:39,656 [trainer.py] => optimizer: sgd
2023-09-11 13:23:39,656 [trainer.py] => vpt_type: deep
2023-09-11 13:23:39,656 [trainer.py] => prompt_token_num: 5
2023-09-11 13:23:39,656 [trainer.py] => use_A: False
2023-09-11 13:23:39,847 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 13:23:41,454 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:23:41,948 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:23:42,800 [trainer.py] => All params: 85844736
2023-09-11 13:23:42,800 [trainer.py] => Trainable params: 46080
2023-09-11 13:23:42,937 [adam_vpt.py] => Learning on 0-30
2023-09-11 13:23:43,950 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 13:23:43,950 [trainer.py] => prefix:  
2023-09-11 13:23:43,950 [trainer.py] => dataset: omnibenchmark
2023-09-11 13:23:43,950 [trainer.py] => memory_size: 0
2023-09-11 13:23:43,950 [trainer.py] => memory_per_class: 0
2023-09-11 13:23:43,950 [trainer.py] => fixed_memory: False
2023-09-11 13:23:43,950 [trainer.py] => shuffle: True
2023-09-11 13:23:43,950 [trainer.py] => init_cls: 30
2023-09-11 13:23:43,950 [trainer.py] => increment: 30
2023-09-11 13:23:43,950 [trainer.py] => model_name: adam_vpt
2023-09-11 13:23:43,950 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 13:23:43,950 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 13:23:43,950 [trainer.py] => seed: 1993
2023-09-11 13:23:43,950 [trainer.py] => tuned_epoch: 20
2023-09-11 13:23:43,950 [trainer.py] => init_lr: 0.02
2023-09-11 13:23:43,950 [trainer.py] => batch_size: 96
2023-09-11 13:23:43,950 [trainer.py] => weight_decay: 0.0005
2023-09-11 13:23:43,951 [trainer.py] => min_lr: 0
2023-09-11 13:23:43,951 [trainer.py] => optimizer: sgd
2023-09-11 13:23:43,951 [trainer.py] => vpt_type: deep
2023-09-11 13:23:43,951 [trainer.py] => prompt_token_num: 5
2023-09-11 13:23:43,951 [trainer.py] => use_A: False
2023-09-11 13:23:44,147 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 13:23:45,774 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:23:46,285 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:23:47,079 [trainer.py] => All params: 85844736
2023-09-11 13:23:47,079 [trainer.py] => Trainable params: 46080
2023-09-11 13:23:47,226 [adam_vpt.py] => Learning on 0-30
2023-09-11 13:24:50,528 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 3.318, Train_accy 3.92
2023-09-11 13:26:00,388 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 3.313, Train_accy 4.43, Test_accy 6.50
2023-09-11 13:30:07,007 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 13:30:07,007 [trainer.py] => prefix:  
2023-09-11 13:30:07,007 [trainer.py] => dataset: omnibenchmark
2023-09-11 13:30:07,007 [trainer.py] => memory_size: 0
2023-09-11 13:30:07,007 [trainer.py] => memory_per_class: 0
2023-09-11 13:30:07,007 [trainer.py] => fixed_memory: False
2023-09-11 13:30:07,007 [trainer.py] => shuffle: True
2023-09-11 13:30:07,007 [trainer.py] => init_cls: 30
2023-09-11 13:30:07,007 [trainer.py] => increment: 30
2023-09-11 13:30:07,007 [trainer.py] => model_name: adam_vpt
2023-09-11 13:30:07,007 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 13:30:07,007 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 13:30:07,007 [trainer.py] => seed: 1993
2023-09-11 13:30:07,007 [trainer.py] => tuned_epoch: 20
2023-09-11 13:30:07,007 [trainer.py] => init_lr: 0.02
2023-09-11 13:30:07,007 [trainer.py] => batch_size: 96
2023-09-11 13:30:07,007 [trainer.py] => weight_decay: 0.0005
2023-09-11 13:30:07,007 [trainer.py] => min_lr: 0
2023-09-11 13:30:07,007 [trainer.py] => optimizer: sgd
2023-09-11 13:30:07,007 [trainer.py] => vpt_type: deep
2023-09-11 13:30:07,007 [trainer.py] => prompt_token_num: 5
2023-09-11 13:30:07,007 [trainer.py] => use_A: False
2023-09-11 13:30:07,198 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 13:30:08,783 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:30:09,282 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:30:10,051 [trainer.py] => All params: 85844736
2023-09-11 13:30:10,051 [trainer.py] => Trainable params: 46080
2023-09-11 13:30:10,187 [adam_vpt.py] => Learning on 0-30
2023-09-11 13:31:13,328 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 3.072, Train_accy 28.97
2023-09-11 13:32:23,331 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 1.252, Train_accy 70.14, Test_accy 80.33
2023-09-11 13:33:33,544 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.831, Train_accy 73.95, Test_accy 87.33
2023-09-11 13:34:43,579 [adam_vpt.py] => Task 0, Epoch 4/20 => Loss 0.747, Train_accy 76.05, Test_accy 86.33
2023-09-11 13:35:53,861 [adam_vpt.py] => Task 0, Epoch 5/20 => Loss 0.693, Train_accy 77.75, Test_accy 86.83
2023-09-11 13:36:57,580 [adam_vpt.py] => Task 0, Epoch 6/20 => Loss 0.674, Train_accy 78.09
2023-09-11 13:38:07,406 [adam_vpt.py] => Task 0, Epoch 7/20 => Loss 0.654, Train_accy 78.93, Test_accy 88.67
2023-09-11 13:39:16,974 [adam_vpt.py] => Task 0, Epoch 8/20 => Loss 0.640, Train_accy 79.38, Test_accy 89.17
2023-09-11 13:40:27,115 [adam_vpt.py] => Task 0, Epoch 9/20 => Loss 0.626, Train_accy 80.57, Test_accy 90.00
2023-09-11 13:41:37,122 [adam_vpt.py] => Task 0, Epoch 10/20 => Loss 0.605, Train_accy 80.71, Test_accy 90.50
2023-09-11 13:42:40,537 [adam_vpt.py] => Task 0, Epoch 11/20 => Loss 0.605, Train_accy 81.35
2023-09-11 13:43:50,442 [adam_vpt.py] => Task 0, Epoch 12/20 => Loss 0.587, Train_accy 81.58, Test_accy 88.83
2023-09-11 13:45:00,606 [adam_vpt.py] => Task 0, Epoch 13/20 => Loss 0.577, Train_accy 81.75, Test_accy 93.00
2023-09-11 13:46:10,732 [adam_vpt.py] => Task 0, Epoch 14/20 => Loss 0.572, Train_accy 81.59, Test_accy 91.50
2023-09-11 13:47:21,062 [adam_vpt.py] => Task 0, Epoch 15/20 => Loss 0.563, Train_accy 82.22, Test_accy 90.50
2023-09-11 13:48:24,688 [adam_vpt.py] => Task 0, Epoch 16/20 => Loss 0.572, Train_accy 82.35
2023-09-11 13:49:34,590 [adam_vpt.py] => Task 0, Epoch 17/20 => Loss 0.563, Train_accy 82.36, Test_accy 91.17
2023-09-11 13:50:44,983 [adam_vpt.py] => Task 0, Epoch 18/20 => Loss 0.547, Train_accy 82.97, Test_accy 91.67
2023-09-11 13:51:55,137 [adam_vpt.py] => Task 0, Epoch 19/20 => Loss 0.552, Train_accy 83.13, Test_accy 91.33
2023-09-11 13:53:05,398 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.558, Train_accy 82.71, Test_accy 91.50
2023-09-11 13:53:06,463 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:53:06,958 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:53:08,582 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 13:53:08,846 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 13:54:22,832 [trainer.py] => No NME accuracy.
2023-09-11 13:54:22,832 [trainer.py] => CNN: {'total': 90.17, '00-09': 92.5, '10-19': 91.5, '20-29': 86.5, 'old': 0, 'new': 90.17}
2023-09-11 13:54:22,832 [trainer.py] => CNN top1 curve: [90.17]
2023-09-11 13:54:22,832 [trainer.py] => CNN top5 curve: [98.83]

2023-09-11 13:54:22,833 [trainer.py] => Average Accuracy (CNN): 90.17
2023-09-11 13:54:22,834 [trainer.py] => All params: 171689473
2023-09-11 13:54:22,836 [trainer.py] => Trainable params: 85890817
2023-09-11 13:54:22,838 [adam_vpt.py] => Learning on 30-60
2023-09-11 13:55:41,423 [trainer.py] => No NME accuracy.
2023-09-11 13:55:41,424 [trainer.py] => CNN: {'total': 89.07, '00-09': 87.0, '10-19': 91.5, '20-29': 83.5, '30-39': 90.5, '40-49': 88.5, '50-59': 93.47, 'old': 87.33, 'new': 90.82}
2023-09-11 13:55:41,424 [trainer.py] => CNN top1 curve: [90.17, 89.07]
2023-09-11 13:55:41,424 [trainer.py] => CNN top5 curve: [98.83, 98.33]

2023-09-11 13:55:41,424 [trainer.py] => Average Accuracy (CNN): 89.62
2023-09-11 13:55:41,425 [trainer.py] => All params: 171735553
2023-09-11 13:55:41,427 [trainer.py] => Trainable params: 85936897
2023-09-11 13:55:41,429 [adam_vpt.py] => Learning on 60-90
2023-09-11 13:57:02,897 [trainer.py] => No NME accuracy.
2023-09-11 13:57:02,897 [trainer.py] => CNN: {'total': 86.26, '00-09': 84.5, '10-19': 86.5, '20-29': 81.0, '30-39': 87.5, '40-49': 86.5, '50-59': 91.96, '60-69': 93.5, '70-79': 87.5, '80-89': 77.39, 'old': 86.32, 'new': 86.14}
2023-09-11 13:57:02,897 [trainer.py] => CNN top1 curve: [90.17, 89.07, 86.26]
2023-09-11 13:57:02,897 [trainer.py] => CNN top5 curve: [98.83, 98.33, 97.5]

2023-09-11 13:57:02,897 [trainer.py] => Average Accuracy (CNN): 88.5
2023-09-11 13:57:02,899 [trainer.py] => All params: 171781633
2023-09-11 13:57:02,900 [trainer.py] => Trainable params: 85982977
2023-09-11 13:57:02,901 [adam_vpt.py] => Learning on 90-120
2023-09-11 13:58:32,070 [trainer.py] => No NME accuracy.
2023-09-11 13:58:32,070 [trainer.py] => CNN: {'total': 82.38, '00-09': 81.5, '10-19': 83.0, '20-29': 80.5, '30-39': 86.0, '40-49': 84.5, '50-59': 86.43, '60-69': 88.0, '70-79': 85.5, '80-89': 75.38, '90-99': 79.4, '100-109': 79.9, '110-119': 78.39, 'old': 83.43, 'new': 79.23}
2023-09-11 13:58:32,071 [trainer.py] => CNN top1 curve: [90.17, 89.07, 86.26, 82.38]
2023-09-11 13:58:32,071 [trainer.py] => CNN top5 curve: [98.83, 98.33, 97.5, 96.62]

2023-09-11 13:58:32,071 [trainer.py] => Average Accuracy (CNN): 86.97
2023-09-11 13:58:32,072 [trainer.py] => All params: 171827713
2023-09-11 13:58:32,074 [trainer.py] => Trainable params: 86029057
2023-09-11 13:58:32,076 [adam_vpt.py] => Learning on 120-150
2023-09-11 14:00:05,534 [trainer.py] => No NME accuracy.
2023-09-11 14:00:05,534 [trainer.py] => CNN: {'total': 80.39, '00-09': 78.5, '10-19': 82.5, '20-29': 79.5, '30-39': 85.5, '40-49': 81.5, '50-59': 80.9, '60-69': 84.5, '70-79': 84.5, '80-89': 73.37, '90-99': 79.4, '100-109': 76.88, '110-119': 73.37, '120-129': 84.5, '130-139': 79.5, '140-149': 81.41, 'old': 80.04, 'new': 81.8}
2023-09-11 14:00:05,534 [trainer.py] => CNN top1 curve: [90.17, 89.07, 86.26, 82.38, 80.39]
2023-09-11 14:00:05,534 [trainer.py] => CNN top5 curve: [98.83, 98.33, 97.5, 96.62, 95.09]

2023-09-11 14:00:05,534 [trainer.py] => Average Accuracy (CNN): 85.654
2023-09-11 14:00:05,536 [trainer.py] => All params: 171873793
2023-09-11 14:00:05,537 [trainer.py] => Trainable params: 86075137
2023-09-11 14:00:05,540 [adam_vpt.py] => Learning on 150-180
2023-09-11 14:01:39,649 [trainer.py] => No NME accuracy.
2023-09-11 14:01:39,650 [trainer.py] => CNN: {'total': 77.53, '00-09': 75.5, '10-19': 78.5, '20-29': 76.0, '30-39': 83.5, '40-49': 78.0, '50-59': 79.4, '60-69': 84.5, '70-79': 81.5, '80-89': 70.85, '90-99': 79.4, '100-109': 74.37, '110-119': 71.86, '120-129': 84.0, '130-139': 79.0, '140-149': 79.9, '150-159': 79.9, '160-169': 73.37, '170-179': 66.0, 'old': 78.42, 'new': 73.08}
2023-09-11 14:01:39,650 [trainer.py] => CNN top1 curve: [90.17, 89.07, 86.26, 82.38, 80.39, 77.53]
2023-09-11 14:01:39,650 [trainer.py] => CNN top5 curve: [98.83, 98.33, 97.5, 96.62, 95.09, 93.9]

2023-09-11 14:01:39,650 [trainer.py] => Average Accuracy (CNN): 84.3
2023-09-11 14:01:39,651 [trainer.py] => All params: 171919873
2023-09-11 14:01:39,653 [trainer.py] => Trainable params: 86121217
2023-09-11 14:01:39,655 [adam_vpt.py] => Learning on 180-210
2023-09-11 14:03:18,469 [trainer.py] => No NME accuracy.
2023-09-11 14:03:18,470 [trainer.py] => CNN: {'total': 76.61, '00-09': 74.5, '10-19': 78.5, '20-29': 76.0, '30-39': 83.5, '40-49': 77.0, '50-59': 78.89, '60-69': 82.0, '70-79': 81.5, '80-89': 70.85, '90-99': 76.88, '100-109': 73.87, '110-119': 67.84, '120-129': 84.0, '130-139': 77.5, '140-149': 79.9, '150-159': 77.39, '160-169': 72.36, '170-179': 65.5, '180-189': 73.74, '190-199': 83.0, '200-209': 73.87, 'old': 76.56, 'new': 76.88}
2023-09-11 14:03:18,470 [trainer.py] => CNN top1 curve: [90.17, 89.07, 86.26, 82.38, 80.39, 77.53, 76.61]
2023-09-11 14:03:18,470 [trainer.py] => CNN top5 curve: [98.83, 98.33, 97.5, 96.62, 95.09, 93.9, 93.46]

2023-09-11 14:03:18,470 [trainer.py] => Average Accuracy (CNN): 83.20142857142856
2023-09-11 14:03:18,471 [trainer.py] => All params: 171965953
2023-09-11 14:03:18,472 [trainer.py] => Trainable params: 86167297
2023-09-11 14:03:18,474 [adam_vpt.py] => Learning on 210-240
2023-09-11 14:05:05,007 [trainer.py] => No NME accuracy.
2023-09-11 14:05:05,007 [trainer.py] => CNN: {'total': 75.33, '00-09': 74.0, '10-19': 76.5, '20-29': 75.5, '30-39': 83.5, '40-49': 75.5, '50-59': 78.89, '60-69': 81.5, '70-79': 81.5, '80-89': 70.85, '90-99': 76.88, '100-109': 66.83, '110-119': 67.34, '120-129': 82.0, '130-139': 77.5, '140-149': 79.4, '150-159': 76.88, '160-169': 72.36, '170-179': 63.5, '180-189': 73.74, '190-199': 82.5, '200-209': 69.85, '210-219': 53.0, '220-229': 87.44, '230-239': 81.0, 'old': 75.56, 'new': 73.79}
2023-09-11 14:05:05,007 [trainer.py] => CNN top1 curve: [90.17, 89.07, 86.26, 82.38, 80.39, 77.53, 76.61, 75.33]
2023-09-11 14:05:05,007 [trainer.py] => CNN top5 curve: [98.83, 98.33, 97.5, 96.62, 95.09, 93.9, 93.46, 92.34]

2023-09-11 14:05:05,007 [trainer.py] => Average Accuracy (CNN): 82.2175
2023-09-11 14:05:05,008 [trainer.py] => All params: 172012033
2023-09-11 14:05:05,009 [trainer.py] => Trainable params: 86213377
2023-09-11 14:05:05,011 [adam_vpt.py] => Learning on 240-270
2023-09-11 14:06:55,797 [trainer.py] => No NME accuracy.
2023-09-11 14:06:55,798 [trainer.py] => CNN: {'total': 74.44, '00-09': 74.0, '10-19': 76.0, '20-29': 75.5, '30-39': 83.5, '40-49': 75.5, '50-59': 78.39, '60-69': 80.0, '70-79': 81.0, '80-89': 70.35, '90-99': 75.38, '100-109': 65.33, '110-119': 64.32, '120-129': 77.0, '130-139': 77.0, '140-149': 74.87, '150-159': 73.37, '160-169': 72.36, '170-179': 63.5, '180-189': 73.23, '190-199': 81.0, '200-209': 67.84, '210-219': 53.0, '220-229': 87.44, '230-239': 81.0, '240-249': 69.5, '250-259': 75.88, '260-269': 83.5, 'old': 74.21, 'new': 76.29}
2023-09-11 14:06:55,798 [trainer.py] => CNN top1 curve: [90.17, 89.07, 86.26, 82.38, 80.39, 77.53, 76.61, 75.33, 74.44]
2023-09-11 14:06:55,798 [trainer.py] => CNN top5 curve: [98.83, 98.33, 97.5, 96.62, 95.09, 93.9, 93.46, 92.34, 92.33]

2023-09-11 14:06:55,798 [trainer.py] => Average Accuracy (CNN): 81.35333333333334
2023-09-11 14:06:55,800 [trainer.py] => All params: 172058113
2023-09-11 14:06:55,801 [trainer.py] => Trainable params: 86259457
2023-09-11 14:06:55,805 [adam_vpt.py] => Learning on 270-300
2023-09-11 14:08:45,618 [trainer.py] => No NME accuracy.
2023-09-11 14:08:45,618 [trainer.py] => CNN: {'total': 74.49, '00-09': 73.5, '10-19': 76.0, '20-29': 75.0, '30-39': 83.5, '40-49': 72.0, '50-59': 78.39, '60-69': 79.5, '70-79': 81.0, '80-89': 69.85, '90-99': 72.36, '100-109': 64.32, '110-119': 63.82, '120-129': 77.0, '130-139': 77.0, '140-149': 74.87, '150-159': 73.37, '160-169': 70.85, '170-179': 62.0, '180-189': 72.73, '190-199': 80.5, '200-209': 67.34, '210-219': 52.5, '220-229': 87.44, '230-239': 80.0, '240-249': 67.0, '250-259': 75.38, '260-269': 82.5, '270-279': 81.91, '280-289': 82.5, '290-299': 80.4, 'old': 73.7, 'new': 81.61}
2023-09-11 14:08:45,618 [trainer.py] => CNN top1 curve: [90.17, 89.07, 86.26, 82.38, 80.39, 77.53, 76.61, 75.33, 74.44, 74.49]
2023-09-11 14:08:45,618 [trainer.py] => CNN top5 curve: [98.83, 98.33, 97.5, 96.62, 95.09, 93.9, 93.46, 92.34, 92.33, 91.96]

2023-09-11 14:08:45,618 [trainer.py] => Average Accuracy (CNN): 80.667
