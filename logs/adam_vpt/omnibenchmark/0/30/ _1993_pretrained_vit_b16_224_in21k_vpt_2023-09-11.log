2023-09-11 07:04:27,553 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 07:04:27,553 [trainer.py] => prefix:  
2023-09-11 07:04:27,553 [trainer.py] => dataset: omnibenchmark
2023-09-11 07:04:27,553 [trainer.py] => memory_size: 0
2023-09-11 07:04:27,553 [trainer.py] => memory_per_class: 0
2023-09-11 07:04:27,553 [trainer.py] => fixed_memory: False
2023-09-11 07:04:27,553 [trainer.py] => shuffle: True
2023-09-11 07:04:27,553 [trainer.py] => init_cls: 30
2023-09-11 07:04:27,553 [trainer.py] => increment: 30
2023-09-11 07:04:27,553 [trainer.py] => model_name: adam_vpt
2023-09-11 07:04:27,553 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 07:04:27,553 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 07:04:27,553 [trainer.py] => seed: 1993
2023-09-11 07:04:27,553 [trainer.py] => tuned_epoch: 20
2023-09-11 07:04:27,553 [trainer.py] => init_lr: 0.01
2023-09-11 07:04:27,553 [trainer.py] => batch_size: 96
2023-09-11 07:04:27,553 [trainer.py] => weight_decay: 0.0005
2023-09-11 07:04:27,553 [trainer.py] => min_lr: 0
2023-09-11 07:04:27,553 [trainer.py] => optimizer: sgd
2023-09-11 07:04:27,553 [trainer.py] => vpt_type: deep
2023-09-11 07:04:27,553 [trainer.py] => prompt_token_num: 5
2023-09-11 07:04:27,744 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 07:04:29,380 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:04:29,886 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:04:30,708 [trainer.py] => All params: 85844736
2023-09-11 07:04:30,709 [trainer.py] => Trainable params: 46080
2023-09-11 07:04:30,853 [adam_vpt.py] => Learning on 0-30
2023-09-11 07:07:15,886 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 07:07:15,886 [trainer.py] => prefix:  
2023-09-11 07:07:15,886 [trainer.py] => dataset: omnibenchmark
2023-09-11 07:07:15,887 [trainer.py] => memory_size: 0
2023-09-11 07:07:15,887 [trainer.py] => memory_per_class: 0
2023-09-11 07:07:15,887 [trainer.py] => fixed_memory: False
2023-09-11 07:07:15,887 [trainer.py] => shuffle: True
2023-09-11 07:07:15,887 [trainer.py] => init_cls: 30
2023-09-11 07:07:15,887 [trainer.py] => increment: 30
2023-09-11 07:07:15,887 [trainer.py] => model_name: adam_vpt
2023-09-11 07:07:15,887 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 07:07:15,887 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 07:07:15,887 [trainer.py] => seed: 1993
2023-09-11 07:07:15,887 [trainer.py] => tuned_epoch: 20
2023-09-11 07:07:15,888 [trainer.py] => init_lr: 0.01
2023-09-11 07:07:15,888 [trainer.py] => batch_size: 96
2023-09-11 07:07:15,888 [trainer.py] => weight_decay: 0.0005
2023-09-11 07:07:15,888 [trainer.py] => min_lr: 0
2023-09-11 07:07:15,888 [trainer.py] => optimizer: sgd
2023-09-11 07:07:15,888 [trainer.py] => vpt_type: deep
2023-09-11 07:07:15,888 [trainer.py] => prompt_token_num: 5
2023-09-11 07:07:16,571 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 07:07:18,758 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:07:19,278 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:07:20,129 [trainer.py] => All params: 85844736
2023-09-11 07:07:20,139 [trainer.py] => Trainable params: 46080
2023-09-11 07:07:20,274 [adam_vpt.py] => Learning on 0-30
2023-09-11 07:07:41,841 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 07:07:41,841 [trainer.py] => prefix:  
2023-09-11 07:07:41,842 [trainer.py] => dataset: omnibenchmark
2023-09-11 07:07:41,842 [trainer.py] => memory_size: 0
2023-09-11 07:07:41,842 [trainer.py] => memory_per_class: 0
2023-09-11 07:07:41,842 [trainer.py] => fixed_memory: False
2023-09-11 07:07:41,842 [trainer.py] => shuffle: True
2023-09-11 07:07:41,842 [trainer.py] => init_cls: 30
2023-09-11 07:07:41,842 [trainer.py] => increment: 30
2023-09-11 07:07:41,842 [trainer.py] => model_name: adam_vpt
2023-09-11 07:07:41,842 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 07:07:41,842 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 07:07:41,842 [trainer.py] => seed: 1993
2023-09-11 07:07:41,843 [trainer.py] => tuned_epoch: 20
2023-09-11 07:07:41,843 [trainer.py] => init_lr: 0.01
2023-09-11 07:07:41,843 [trainer.py] => batch_size: 96
2023-09-11 07:07:41,843 [trainer.py] => weight_decay: 0.0005
2023-09-11 07:07:41,843 [trainer.py] => min_lr: 0
2023-09-11 07:07:41,843 [trainer.py] => optimizer: sgd
2023-09-11 07:07:41,843 [trainer.py] => vpt_type: deep
2023-09-11 07:07:41,843 [trainer.py] => prompt_token_num: 5
2023-09-11 07:07:42,538 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 07:07:44,766 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:07:45,279 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:07:46,206 [trainer.py] => All params: 85844736
2023-09-11 07:07:46,216 [trainer.py] => Trainable params: 46080
2023-09-11 07:07:46,351 [adam_vpt.py] => Learning on 0-30
2023-09-11 07:08:51,291 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 07:08:51,291 [trainer.py] => prefix:  
2023-09-11 07:08:51,291 [trainer.py] => dataset: omnibenchmark
2023-09-11 07:08:51,291 [trainer.py] => memory_size: 0
2023-09-11 07:08:51,292 [trainer.py] => memory_per_class: 0
2023-09-11 07:08:51,292 [trainer.py] => fixed_memory: False
2023-09-11 07:08:51,292 [trainer.py] => shuffle: True
2023-09-11 07:08:51,292 [trainer.py] => init_cls: 30
2023-09-11 07:08:51,292 [trainer.py] => increment: 30
2023-09-11 07:08:51,292 [trainer.py] => model_name: adam_vpt
2023-09-11 07:08:51,292 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 07:08:51,292 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 07:08:51,292 [trainer.py] => seed: 1993
2023-09-11 07:08:51,292 [trainer.py] => tuned_epoch: 20
2023-09-11 07:08:51,292 [trainer.py] => init_lr: 0.01
2023-09-11 07:08:51,292 [trainer.py] => batch_size: 96
2023-09-11 07:08:51,292 [trainer.py] => weight_decay: 0.0005
2023-09-11 07:08:51,292 [trainer.py] => min_lr: 0
2023-09-11 07:08:51,292 [trainer.py] => optimizer: sgd
2023-09-11 07:08:51,292 [trainer.py] => vpt_type: deep
2023-09-11 07:08:51,292 [trainer.py] => prompt_token_num: 5
2023-09-11 07:08:51,483 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 07:08:53,145 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:08:53,654 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:08:54,418 [trainer.py] => All params: 85844736
2023-09-11 07:08:54,418 [trainer.py] => Trainable params: 46080
2023-09-11 07:08:54,564 [adam_vpt.py] => Learning on 0-30
2023-09-11 07:26:32,512 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.378, Train_accy 89.15, Test_accy 93.00
2023-09-11 07:26:33,377 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:26:33,881 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 07:26:35,321 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 07:26:35,571 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:37:58,264 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 08:37:58,264 [trainer.py] => prefix:  
2023-09-11 08:37:58,265 [trainer.py] => dataset: omnibenchmark
2023-09-11 08:37:58,265 [trainer.py] => memory_size: 0
2023-09-11 08:37:58,265 [trainer.py] => memory_per_class: 0
2023-09-11 08:37:58,265 [trainer.py] => fixed_memory: False
2023-09-11 08:37:58,265 [trainer.py] => shuffle: True
2023-09-11 08:37:58,265 [trainer.py] => init_cls: 30
2023-09-11 08:37:58,265 [trainer.py] => increment: 30
2023-09-11 08:37:58,265 [trainer.py] => model_name: adam_vpt
2023-09-11 08:37:58,265 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 08:37:58,265 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 08:37:58,265 [trainer.py] => seed: 1993
2023-09-11 08:37:58,265 [trainer.py] => tuned_epoch: 20
2023-09-11 08:37:58,265 [trainer.py] => init_lr: 0.01
2023-09-11 08:37:58,265 [trainer.py] => batch_size: 96
2023-09-11 08:37:58,265 [trainer.py] => weight_decay: 0.0005
2023-09-11 08:37:58,265 [trainer.py] => min_lr: 0
2023-09-11 08:37:58,265 [trainer.py] => optimizer: sgd
2023-09-11 08:37:58,265 [trainer.py] => vpt_type: deep
2023-09-11 08:37:58,265 [trainer.py] => prompt_token_num: 5
2023-09-11 08:37:58,642 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 08:38:00,231 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 08:38:00,758 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:38:01,511 [trainer.py] => All params: 85844736
2023-09-11 08:38:01,511 [trainer.py] => Trainable params: 46080
2023-09-11 08:38:01,644 [adam_vpt.py] => Learning on 0-30
2023-09-11 08:38:59,130 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 08:38:59,130 [trainer.py] => prefix:  
2023-09-11 08:38:59,130 [trainer.py] => dataset: omnibenchmark
2023-09-11 08:38:59,130 [trainer.py] => memory_size: 0
2023-09-11 08:38:59,130 [trainer.py] => memory_per_class: 0
2023-09-11 08:38:59,130 [trainer.py] => fixed_memory: False
2023-09-11 08:38:59,130 [trainer.py] => shuffle: True
2023-09-11 08:38:59,130 [trainer.py] => init_cls: 30
2023-09-11 08:38:59,130 [trainer.py] => increment: 30
2023-09-11 08:38:59,130 [trainer.py] => model_name: adam_vpt
2023-09-11 08:38:59,130 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 08:38:59,130 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 08:38:59,130 [trainer.py] => seed: 1993
2023-09-11 08:38:59,130 [trainer.py] => tuned_epoch: 20
2023-09-11 08:38:59,130 [trainer.py] => init_lr: 0.01
2023-09-11 08:38:59,130 [trainer.py] => batch_size: 96
2023-09-11 08:38:59,130 [trainer.py] => weight_decay: 0.0005
2023-09-11 08:38:59,130 [trainer.py] => min_lr: 0
2023-09-11 08:38:59,130 [trainer.py] => optimizer: sgd
2023-09-11 08:38:59,130 [trainer.py] => vpt_type: deep
2023-09-11 08:38:59,130 [trainer.py] => prompt_token_num: 5
2023-09-11 08:38:59,326 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 08:39:00,951 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 08:39:01,452 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:39:02,268 [trainer.py] => All params: 85844736
2023-09-11 08:39:02,269 [trainer.py] => Trainable params: 46080
2023-09-11 08:39:02,408 [adam_vpt.py] => Learning on 0-30
2023-09-11 08:39:49,911 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 3.087, Train_accy 45.49
2023-09-11 08:40:44,170 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 1.425, Train_accy 77.17, Test_accy 81.83
2023-09-11 08:41:38,553 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.701, Train_accy 80.62, Test_accy 87.67
2023-09-11 08:42:32,703 [adam_vpt.py] => Task 0, Epoch 4/20 => Loss 0.585, Train_accy 83.26, Test_accy 87.83
2023-09-11 08:43:26,592 [adam_vpt.py] => Task 0, Epoch 5/20 => Loss 0.530, Train_accy 84.10, Test_accy 89.17
2023-09-11 08:44:14,527 [adam_vpt.py] => Task 0, Epoch 6/20 => Loss 0.503, Train_accy 84.90
2023-09-11 08:45:08,324 [adam_vpt.py] => Task 0, Epoch 7/20 => Loss 0.473, Train_accy 86.17, Test_accy 91.00
2023-09-11 08:46:02,358 [adam_vpt.py] => Task 0, Epoch 8/20 => Loss 0.456, Train_accy 86.15, Test_accy 90.17
2023-09-11 08:46:56,444 [adam_vpt.py] => Task 0, Epoch 9/20 => Loss 0.435, Train_accy 87.25, Test_accy 91.33
2023-09-11 08:47:50,553 [adam_vpt.py] => Task 0, Epoch 10/20 => Loss 0.417, Train_accy 87.58, Test_accy 91.17
2023-09-11 08:48:38,324 [adam_vpt.py] => Task 0, Epoch 11/20 => Loss 0.419, Train_accy 87.35
2023-09-11 08:49:32,672 [adam_vpt.py] => Task 0, Epoch 12/20 => Loss 0.401, Train_accy 88.24, Test_accy 91.00
2023-09-11 08:50:27,115 [adam_vpt.py] => Task 0, Epoch 13/20 => Loss 0.397, Train_accy 88.29, Test_accy 92.50
2023-09-11 08:51:21,256 [adam_vpt.py] => Task 0, Epoch 14/20 => Loss 0.394, Train_accy 88.33, Test_accy 92.17
2023-09-11 08:52:15,690 [adam_vpt.py] => Task 0, Epoch 15/20 => Loss 0.385, Train_accy 88.55, Test_accy 91.17
2023-09-11 08:53:03,692 [adam_vpt.py] => Task 0, Epoch 16/20 => Loss 0.388, Train_accy 88.50
2023-09-11 08:53:58,240 [adam_vpt.py] => Task 0, Epoch 17/20 => Loss 0.376, Train_accy 88.90, Test_accy 92.67
2023-09-11 08:54:52,546 [adam_vpt.py] => Task 0, Epoch 18/20 => Loss 0.374, Train_accy 88.95, Test_accy 92.33
2023-09-11 08:55:46,680 [adam_vpt.py] => Task 0, Epoch 19/20 => Loss 0.380, Train_accy 88.79, Test_accy 92.00
2023-09-11 08:56:41,228 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.378, Train_accy 89.04, Test_accy 92.17
2023-09-11 08:56:42,055 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 08:56:42,584 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:56:44,123 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 08:56:44,376 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 08:57:58,950 [trainer.py] => No NME accuracy.
2023-09-11 08:57:58,950 [trainer.py] => CNN: {'total': 90.5, '00-09': 93.0, '10-19': 91.0, '20-29': 87.5, 'old': 0, 'new': 90.5}
2023-09-11 08:57:58,950 [trainer.py] => CNN top1 curve: [90.5]
2023-09-11 08:57:58,950 [trainer.py] => CNN top5 curve: [99.0]

2023-09-11 08:57:58,950 [trainer.py] => Average Accuracy (CNN): 90.5
2023-09-11 08:57:58,952 [trainer.py] => All params: 171689473
2023-09-11 08:57:58,953 [trainer.py] => Trainable params: 85890817
2023-09-11 08:57:58,955 [adam_vpt.py] => Learning on 30-60
2023-09-11 08:59:17,822 [trainer.py] => No NME accuracy.
2023-09-11 08:59:17,823 [trainer.py] => CNN: {'total': 89.66, '00-09': 88.5, '10-19': 91.0, '20-29': 84.5, '30-39': 91.0, '40-49': 87.5, '50-59': 95.48, 'old': 88.0, 'new': 91.32}
2023-09-11 08:59:17,823 [trainer.py] => CNN top1 curve: [90.5, 89.66]
2023-09-11 08:59:17,823 [trainer.py] => CNN top5 curve: [99.0, 98.67]

2023-09-11 08:59:17,823 [trainer.py] => Average Accuracy (CNN): 90.08
2023-09-11 08:59:17,824 [trainer.py] => All params: 171735553
2023-09-11 08:59:17,825 [trainer.py] => Trainable params: 85936897
2023-09-11 08:59:17,827 [adam_vpt.py] => Learning on 60-90
2023-09-11 09:00:39,429 [trainer.py] => No NME accuracy.
2023-09-11 09:00:39,430 [trainer.py] => CNN: {'total': 86.76, '00-09': 85.5, '10-19': 85.5, '20-29': 81.5, '30-39': 88.5, '40-49': 85.5, '50-59': 94.47, '60-69': 93.5, '70-79': 88.0, '80-89': 78.39, 'old': 86.82, 'new': 86.64}
2023-09-11 09:00:39,430 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76]
2023-09-11 09:00:39,430 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16]

2023-09-11 09:00:39,430 [trainer.py] => Average Accuracy (CNN): 88.97333333333334
2023-09-11 09:00:39,431 [trainer.py] => All params: 171781633
2023-09-11 09:00:39,432 [trainer.py] => Trainable params: 85982977
2023-09-11 09:00:39,433 [adam_vpt.py] => Learning on 90-120
2023-09-11 09:02:10,587 [trainer.py] => No NME accuracy.
2023-09-11 09:02:10,587 [trainer.py] => CNN: {'total': 82.76, '00-09': 82.5, '10-19': 82.0, '20-29': 81.0, '30-39': 87.5, '40-49': 83.5, '50-59': 86.93, '60-69': 89.0, '70-79': 86.0, '80-89': 74.37, '90-99': 80.9, '100-109': 79.9, '110-119': 79.4, 'old': 83.65, 'new': 80.07}
2023-09-11 09:02:10,587 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76]
2023-09-11 09:02:10,587 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7]

2023-09-11 09:02:10,587 [trainer.py] => Average Accuracy (CNN): 87.42
2023-09-11 09:02:10,588 [trainer.py] => All params: 171827713
2023-09-11 09:02:10,589 [trainer.py] => Trainable params: 86029057
2023-09-11 09:02:10,590 [adam_vpt.py] => Learning on 120-150
2023-09-11 09:03:44,178 [trainer.py] => No NME accuracy.
2023-09-11 09:03:44,178 [trainer.py] => CNN: {'total': 80.53, '00-09': 79.5, '10-19': 82.0, '20-29': 80.5, '30-39': 87.0, '40-49': 80.5, '50-59': 79.9, '60-69': 84.0, '70-79': 85.5, '80-89': 73.37, '90-99': 80.4, '100-109': 76.88, '110-119': 72.86, '120-129': 83.5, '130-139': 81.5, '140-149': 80.4, 'old': 80.21, 'new': 81.8}
2023-09-11 09:03:44,178 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53]
2023-09-11 09:03:44,178 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46]

2023-09-11 09:03:44,178 [trainer.py] => Average Accuracy (CNN): 86.042
2023-09-11 09:03:44,180 [trainer.py] => All params: 171873793
2023-09-11 09:03:44,182 [trainer.py] => Trainable params: 86075137
2023-09-11 09:03:44,185 [adam_vpt.py] => Learning on 150-180
2023-09-11 09:05:18,442 [trainer.py] => No NME accuracy.
2023-09-11 09:05:18,442 [trainer.py] => CNN: {'total': 77.76, '00-09': 77.0, '10-19': 79.0, '20-29': 78.0, '30-39': 84.5, '40-49': 77.5, '50-59': 78.39, '60-69': 82.0, '70-79': 82.5, '80-89': 70.35, '90-99': 80.4, '100-109': 74.37, '110-119': 71.36, '120-129': 82.0, '130-139': 81.0, '140-149': 79.4, '150-159': 77.39, '160-169': 76.88, '170-179': 67.5, 'old': 78.52, 'new': 73.91}
2023-09-11 09:05:18,442 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76]
2023-09-11 09:05:18,442 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07]

2023-09-11 09:05:18,442 [trainer.py] => Average Accuracy (CNN): 84.66166666666668
2023-09-11 09:05:18,444 [trainer.py] => All params: 171919873
2023-09-11 09:05:18,445 [trainer.py] => Trainable params: 86121217
2023-09-11 09:05:18,448 [adam_vpt.py] => Learning on 180-210
2023-09-11 09:06:57,934 [trainer.py] => No NME accuracy.
2023-09-11 09:06:57,934 [trainer.py] => CNN: {'total': 76.84, '00-09': 76.0, '10-19': 79.0, '20-29': 77.5, '30-39': 84.5, '40-49': 76.5, '50-59': 77.89, '60-69': 81.0, '70-79': 82.0, '80-89': 69.35, '90-99': 77.89, '100-109': 73.87, '110-119': 68.84, '120-129': 82.0, '130-139': 80.0, '140-149': 79.4, '150-159': 75.38, '160-169': 75.88, '170-179': 67.0, '180-189': 73.23, '190-199': 84.0, '200-209': 72.36, 'old': 76.89, 'new': 76.55}
2023-09-11 09:06:57,934 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76, 76.84]
2023-09-11 09:06:57,934 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07, 93.67]

2023-09-11 09:06:57,935 [trainer.py] => Average Accuracy (CNN): 83.54428571428572
2023-09-11 09:06:57,936 [trainer.py] => All params: 171965953
2023-09-11 09:06:57,937 [trainer.py] => Trainable params: 86167297
2023-09-11 09:06:57,940 [adam_vpt.py] => Learning on 210-240
2023-09-11 09:08:45,632 [trainer.py] => No NME accuracy.
2023-09-11 09:08:45,632 [trainer.py] => CNN: {'total': 75.04, '00-09': 73.5, '10-19': 76.5, '20-29': 76.5, '30-39': 84.5, '40-49': 75.0, '50-59': 77.89, '60-69': 80.0, '70-79': 82.0, '80-89': 69.35, '90-99': 77.39, '100-109': 66.33, '110-119': 67.84, '120-129': 78.5, '130-139': 80.0, '140-149': 78.39, '150-159': 75.38, '160-169': 75.88, '170-179': 65.0, '180-189': 72.73, '190-199': 83.5, '200-209': 69.35, '210-219': 48.5, '220-229': 86.43, '230-239': 80.5, 'old': 75.51, 'new': 71.79}
2023-09-11 09:08:45,632 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76, 76.84, 75.04]
2023-09-11 09:08:45,632 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07, 93.67, 92.38]

2023-09-11 09:08:45,632 [trainer.py] => Average Accuracy (CNN): 82.48125
2023-09-11 09:08:45,634 [trainer.py] => All params: 172012033
2023-09-11 09:08:45,635 [trainer.py] => Trainable params: 86213377
2023-09-11 09:08:45,638 [adam_vpt.py] => Learning on 240-270
2023-09-11 09:10:36,951 [trainer.py] => No NME accuracy.
2023-09-11 09:10:36,951 [trainer.py] => CNN: {'total': 74.01, '00-09': 73.5, '10-19': 76.0, '20-29': 76.5, '30-39': 84.0, '40-49': 75.0, '50-59': 76.88, '60-69': 78.5, '70-79': 81.0, '80-89': 68.84, '90-99': 75.88, '100-109': 65.33, '110-119': 65.33, '120-129': 74.0, '130-139': 79.0, '140-149': 73.87, '150-159': 72.36, '160-169': 75.88, '170-179': 65.0, '180-189': 72.73, '190-199': 82.5, '200-209': 65.33, '210-219': 48.5, '220-229': 86.43, '230-239': 80.5, '240-249': 69.0, '250-259': 75.88, '260-269': 80.5, 'old': 73.87, 'new': 75.13}
2023-09-11 09:10:36,952 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76, 76.84, 75.04, 74.01]
2023-09-11 09:10:36,952 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07, 93.67, 92.38, 92.28]

2023-09-11 09:10:36,952 [trainer.py] => Average Accuracy (CNN): 81.54
2023-09-11 09:10:36,953 [trainer.py] => All params: 172058113
2023-09-11 09:10:36,955 [trainer.py] => Trainable params: 86259457
2023-09-11 09:10:36,960 [adam_vpt.py] => Learning on 270-300
2023-09-11 09:12:27,025 [trainer.py] => No NME accuracy.
2023-09-11 09:12:27,025 [trainer.py] => CNN: {'total': 74.05, '00-09': 73.5, '10-19': 76.0, '20-29': 76.0, '30-39': 83.5, '40-49': 71.0, '50-59': 76.88, '60-69': 78.5, '70-79': 80.5, '80-89': 68.84, '90-99': 72.36, '100-109': 64.82, '110-119': 64.82, '120-129': 74.0, '130-139': 78.5, '140-149': 73.37, '150-159': 72.36, '160-169': 73.37, '170-179': 64.0, '180-189': 72.22, '190-199': 82.0, '200-209': 64.82, '210-219': 48.0, '220-229': 86.43, '230-239': 79.0, '240-249': 67.0, '250-259': 75.38, '260-269': 79.5, '270-279': 82.91, '280-289': 80.5, '290-299': 81.41, 'old': 73.21, 'new': 81.61}
2023-09-11 09:12:27,025 [trainer.py] => CNN top1 curve: [90.5, 89.66, 86.76, 82.76, 80.53, 77.76, 76.84, 75.04, 74.01, 74.05]
2023-09-11 09:12:27,025 [trainer.py] => CNN top5 curve: [99.0, 98.67, 98.16, 96.7, 95.46, 94.07, 93.67, 92.38, 92.28, 92.08]

2023-09-11 09:12:27,025 [trainer.py] => Average Accuracy (CNN): 80.791
2023-09-11 09:13:42,517 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-09-11 09:13:42,517 [trainer.py] => prefix:  
2023-09-11 09:13:42,518 [trainer.py] => dataset: omnibenchmark
2023-09-11 09:13:42,518 [trainer.py] => memory_size: 0
2023-09-11 09:13:42,518 [trainer.py] => memory_per_class: 0
2023-09-11 09:13:42,518 [trainer.py] => fixed_memory: False
2023-09-11 09:13:42,518 [trainer.py] => shuffle: True
2023-09-11 09:13:42,518 [trainer.py] => init_cls: 30
2023-09-11 09:13:42,518 [trainer.py] => increment: 30
2023-09-11 09:13:42,518 [trainer.py] => model_name: adam_vpt
2023-09-11 09:13:42,518 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-09-11 09:13:42,518 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-11 09:13:42,518 [trainer.py] => seed: 1993
2023-09-11 09:13:42,518 [trainer.py] => tuned_epoch: 20
2023-09-11 09:13:42,518 [trainer.py] => init_lr: 0.02
2023-09-11 09:13:42,518 [trainer.py] => batch_size: 96
2023-09-11 09:13:42,518 [trainer.py] => weight_decay: 0.0005
2023-09-11 09:13:42,518 [trainer.py] => min_lr: 0
2023-09-11 09:13:42,518 [trainer.py] => optimizer: sgd
2023-09-11 09:13:42,518 [trainer.py] => vpt_type: deep
2023-09-11 09:13:42,518 [trainer.py] => prompt_token_num: 5
2023-09-11 09:13:42,709 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-11 09:13:44,322 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 09:13:44,831 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 09:13:45,637 [trainer.py] => All params: 85844736
2023-09-11 09:13:45,637 [trainer.py] => Trainable params: 46080
2023-09-11 09:13:45,783 [adam_vpt.py] => Learning on 0-30
2023-09-11 09:14:33,477 [adam_vpt.py] => Task 0, Epoch 1/20 => Loss 2.560, Train_accy 55.99
2023-09-11 09:15:27,719 [adam_vpt.py] => Task 0, Epoch 2/20 => Loss 0.686, Train_accy 80.63, Test_accy 86.17
2023-09-11 09:16:22,101 [adam_vpt.py] => Task 0, Epoch 3/20 => Loss 0.530, Train_accy 84.18, Test_accy 89.33
2023-09-11 09:17:16,396 [adam_vpt.py] => Task 0, Epoch 4/20 => Loss 0.474, Train_accy 85.31, Test_accy 89.33
2023-09-11 09:18:10,830 [adam_vpt.py] => Task 0, Epoch 5/20 => Loss 0.438, Train_accy 86.43, Test_accy 90.67
2023-09-11 09:18:58,713 [adam_vpt.py] => Task 0, Epoch 6/20 => Loss 0.425, Train_accy 87.02
2023-09-11 09:19:52,874 [adam_vpt.py] => Task 0, Epoch 7/20 => Loss 0.402, Train_accy 88.03, Test_accy 91.00
2023-09-11 09:20:47,089 [adam_vpt.py] => Task 0, Epoch 8/20 => Loss 0.389, Train_accy 88.19, Test_accy 91.33
2023-09-11 09:21:41,347 [adam_vpt.py] => Task 0, Epoch 9/20 => Loss 0.373, Train_accy 88.69, Test_accy 91.83
2023-09-11 09:22:35,290 [adam_vpt.py] => Task 0, Epoch 10/20 => Loss 0.356, Train_accy 89.31, Test_accy 91.50
2023-09-11 09:23:23,088 [adam_vpt.py] => Task 0, Epoch 11/20 => Loss 0.363, Train_accy 88.89
2023-09-11 09:24:17,562 [adam_vpt.py] => Task 0, Epoch 12/20 => Loss 0.343, Train_accy 89.64, Test_accy 91.83
2023-09-11 09:25:11,942 [adam_vpt.py] => Task 0, Epoch 13/20 => Loss 0.344, Train_accy 89.65, Test_accy 92.67
2023-09-11 09:26:06,338 [adam_vpt.py] => Task 0, Epoch 14/20 => Loss 0.340, Train_accy 89.57, Test_accy 92.67
2023-09-11 09:27:00,852 [adam_vpt.py] => Task 0, Epoch 15/20 => Loss 0.331, Train_accy 89.92, Test_accy 92.00
2023-09-11 09:27:48,499 [adam_vpt.py] => Task 0, Epoch 16/20 => Loss 0.334, Train_accy 89.69
2023-09-11 09:28:42,749 [adam_vpt.py] => Task 0, Epoch 17/20 => Loss 0.324, Train_accy 90.06, Test_accy 92.17
2023-09-11 09:29:36,963 [adam_vpt.py] => Task 0, Epoch 18/20 => Loss 0.321, Train_accy 90.35, Test_accy 92.33
2023-09-11 09:30:31,436 [adam_vpt.py] => Task 0, Epoch 19/20 => Loss 0.327, Train_accy 89.96, Test_accy 92.50
2023-09-11 09:31:25,971 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.322, Train_accy 90.47, Test_accy 92.67
2023-09-11 09:31:26,791 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 09:31:27,336 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 09:31:28,868 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-11 09:31:29,130 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-11 09:32:43,471 [trainer.py] => No NME accuracy.
2023-09-11 09:32:43,471 [trainer.py] => CNN: {'total': 91.5, '00-09': 93.0, '10-19': 91.5, '20-29': 90.0, 'old': 0, 'new': 91.5}
2023-09-11 09:32:43,471 [trainer.py] => CNN top1 curve: [91.5]
2023-09-11 09:32:43,471 [trainer.py] => CNN top5 curve: [99.0]

2023-09-11 09:32:43,472 [trainer.py] => Average Accuracy (CNN): 91.5
2023-09-11 09:32:43,473 [trainer.py] => All params: 171689473
2023-09-11 09:32:43,475 [trainer.py] => Trainable params: 85890817
2023-09-11 09:32:43,476 [adam_vpt.py] => Learning on 30-60
