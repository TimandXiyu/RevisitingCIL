2023-09-13 17:17:22,071 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 17:17:22,072 [trainer.py] => prefix:  
2023-09-13 17:17:22,072 [trainer.py] => dataset: omnibenchmark
2023-09-13 17:17:22,073 [trainer.py] => memory_size: 0
2023-09-13 17:17:22,073 [trainer.py] => memory_per_class: 0
2023-09-13 17:17:22,074 [trainer.py] => fixed_memory: False
2023-09-13 17:17:22,074 [trainer.py] => shuffle: True
2023-09-13 17:17:22,075 [trainer.py] => init_cls: 30
2023-09-13 17:17:22,076 [trainer.py] => increment: 30
2023-09-13 17:17:22,076 [trainer.py] => model_name: adam_lora
2023-09-13 17:17:22,076 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 17:17:22,077 [trainer.py] => device: [device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-13 17:17:22,078 [trainer.py] => visible_device: 2,3
2023-09-13 17:17:22,078 [trainer.py] => seed: 1993
2023-09-13 17:17:22,079 [trainer.py] => tuned_epoch: 20
2023-09-13 17:17:22,079 [trainer.py] => init_lr: 0.02
2023-09-13 17:17:22,080 [trainer.py] => batch_size: 96
2023-09-13 17:17:22,080 [trainer.py] => use_A: False
2023-09-13 17:17:22,081 [trainer.py] => weight_decay: 0.0005
2023-09-13 17:17:22,081 [trainer.py] => min_lr: 0
2023-09-13 17:17:22,082 [trainer.py] => ffn_num: 16
2023-09-13 17:17:22,082 [trainer.py] => optimizer: sgd
2023-09-13 17:17:22,084 [trainer.py] => vpt_type: shallow
2023-09-13 17:17:22,085 [trainer.py] => prompt_token_num: 5
2023-09-13 17:17:24,808 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 17:17:49,696 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 17:18:12,974 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 17:20:25,967 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 17:20:25,968 [trainer.py] => prefix:  
2023-09-13 17:20:25,969 [trainer.py] => dataset: omnibenchmark
2023-09-13 17:20:25,969 [trainer.py] => memory_size: 0
2023-09-13 17:20:25,970 [trainer.py] => memory_per_class: 0
2023-09-13 17:20:25,971 [trainer.py] => fixed_memory: False
2023-09-13 17:20:25,971 [trainer.py] => shuffle: True
2023-09-13 17:20:25,972 [trainer.py] => init_cls: 30
2023-09-13 17:20:25,973 [trainer.py] => increment: 30
2023-09-13 17:20:25,973 [trainer.py] => model_name: adam_lora
2023-09-13 17:20:25,974 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 17:20:25,974 [trainer.py] => device: [device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-13 17:20:25,975 [trainer.py] => visible_device: 2,3
2023-09-13 17:20:25,975 [trainer.py] => seed: 1993
2023-09-13 17:20:25,976 [trainer.py] => tuned_epoch: 20
2023-09-13 17:20:25,976 [trainer.py] => init_lr: 0.02
2023-09-13 17:20:25,977 [trainer.py] => batch_size: 96
2023-09-13 17:20:25,977 [trainer.py] => use_A: False
2023-09-13 17:20:25,978 [trainer.py] => weight_decay: 0.0005
2023-09-13 17:20:25,979 [trainer.py] => min_lr: 0
2023-09-13 17:20:25,979 [trainer.py] => ffn_num: 16
2023-09-13 17:20:25,980 [trainer.py] => optimizer: sgd
2023-09-13 17:20:25,981 [trainer.py] => vpt_type: shallow
2023-09-13 17:20:25,981 [trainer.py] => prompt_token_num: 5
2023-09-13 17:20:29,315 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 17:20:36,682 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 17:20:37,087 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 17:41:41,858 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 17:41:41,859 [trainer.py] => prefix:  
2023-09-13 17:41:41,859 [trainer.py] => dataset: omnibenchmark
2023-09-13 17:41:41,860 [trainer.py] => memory_size: 0
2023-09-13 17:41:41,860 [trainer.py] => memory_per_class: 0
2023-09-13 17:41:41,861 [trainer.py] => fixed_memory: False
2023-09-13 17:41:41,861 [trainer.py] => shuffle: True
2023-09-13 17:41:41,862 [trainer.py] => init_cls: 30
2023-09-13 17:41:41,863 [trainer.py] => increment: 30
2023-09-13 17:41:41,865 [trainer.py] => model_name: adam_lora
2023-09-13 17:41:41,865 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 17:41:41,866 [trainer.py] => device: [device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-13 17:41:41,866 [trainer.py] => visible_device: 2,3
2023-09-13 17:41:41,867 [trainer.py] => seed: 1993
2023-09-13 17:41:41,867 [trainer.py] => tuned_epoch: 20
2023-09-13 17:41:41,868 [trainer.py] => init_lr: 0.02
2023-09-13 17:41:41,868 [trainer.py] => batch_size: 96
2023-09-13 17:41:41,869 [trainer.py] => use_A: False
2023-09-13 17:41:41,869 [trainer.py] => weight_decay: 0.0005
2023-09-13 17:41:41,870 [trainer.py] => min_lr: 0
2023-09-13 17:41:41,871 [trainer.py] => ffn_num: 16
2023-09-13 17:41:41,871 [trainer.py] => optimizer: sgd
2023-09-13 17:41:41,872 [trainer.py] => vpt_type: shallow
2023-09-13 17:41:41,873 [trainer.py] => prompt_token_num: 5
2023-09-13 17:41:44,847 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 17:41:51,447 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 17:41:51,912 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 17:41:57,367 [trainer.py] => All params: 86388480
2023-09-13 17:41:57,390 [trainer.py] => Trainable params: 589824
2023-09-13 17:42:13,337 [adam_lora.py] => Learning on 0-30
2023-09-13 17:43:17,522 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 17:43:17,522 [trainer.py] => prefix:  
2023-09-13 17:43:17,525 [trainer.py] => dataset: omnibenchmark
2023-09-13 17:43:17,525 [trainer.py] => memory_size: 0
2023-09-13 17:43:17,526 [trainer.py] => memory_per_class: 0
2023-09-13 17:43:17,526 [trainer.py] => fixed_memory: False
2023-09-13 17:43:17,527 [trainer.py] => shuffle: True
2023-09-13 17:43:17,527 [trainer.py] => init_cls: 30
2023-09-13 17:43:17,528 [trainer.py] => increment: 30
2023-09-13 17:43:17,528 [trainer.py] => model_name: adam_lora
2023-09-13 17:43:17,529 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 17:43:17,529 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 17:43:17,530 [trainer.py] => visible_device: 2,3
2023-09-13 17:43:17,531 [trainer.py] => seed: 1993
2023-09-13 17:43:17,531 [trainer.py] => tuned_epoch: 20
2023-09-13 17:43:17,532 [trainer.py] => init_lr: 0.02
2023-09-13 17:43:17,532 [trainer.py] => batch_size: 96
2023-09-13 17:43:17,533 [trainer.py] => use_A: False
2023-09-13 17:43:17,533 [trainer.py] => weight_decay: 0.0005
2023-09-13 17:43:17,534 [trainer.py] => min_lr: 0
2023-09-13 17:43:17,534 [trainer.py] => ffn_num: 16
2023-09-13 17:43:17,535 [trainer.py] => optimizer: sgd
2023-09-13 17:43:17,535 [trainer.py] => vpt_type: shallow
2023-09-13 17:43:17,536 [trainer.py] => prompt_token_num: 5
2023-09-13 17:43:20,628 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 17:43:27,708 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 17:43:28,070 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 17:43:32,411 [trainer.py] => All params: 86388480
2023-09-13 17:43:32,444 [trainer.py] => Trainable params: 589824
2023-09-13 17:43:43,459 [adam_lora.py] => Learning on 0-30
2023-09-13 17:46:52,625 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 17:46:52,626 [trainer.py] => prefix:  
2023-09-13 17:46:52,626 [trainer.py] => dataset: omnibenchmark
2023-09-13 17:46:52,626 [trainer.py] => memory_size: 0
2023-09-13 17:46:52,627 [trainer.py] => memory_per_class: 0
2023-09-13 17:46:52,627 [trainer.py] => fixed_memory: False
2023-09-13 17:46:52,627 [trainer.py] => shuffle: True
2023-09-13 17:46:52,627 [trainer.py] => init_cls: 30
2023-09-13 17:46:52,628 [trainer.py] => increment: 30
2023-09-13 17:46:52,628 [trainer.py] => model_name: adam_lora
2023-09-13 17:46:52,628 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 17:46:52,628 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 17:46:52,629 [trainer.py] => visible_device: 2,3
2023-09-13 17:46:52,629 [trainer.py] => seed: 1993
2023-09-13 17:46:52,629 [trainer.py] => tuned_epoch: 20
2023-09-13 17:46:52,629 [trainer.py] => init_lr: 0.02
2023-09-13 17:46:52,630 [trainer.py] => batch_size: 96
2023-09-13 17:46:52,630 [trainer.py] => use_A: False
2023-09-13 17:46:52,630 [trainer.py] => weight_decay: 0.0005
2023-09-13 17:46:52,630 [trainer.py] => min_lr: 0
2023-09-13 17:46:52,631 [trainer.py] => ffn_num: 16
2023-09-13 17:46:52,631 [trainer.py] => optimizer: sgd
2023-09-13 17:46:52,631 [trainer.py] => vpt_type: shallow
2023-09-13 17:46:52,632 [trainer.py] => prompt_token_num: 5
2023-09-13 17:46:53,727 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 17:46:59,309 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 17:46:59,658 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 17:47:03,229 [trainer.py] => All params: 86388480
2023-09-13 17:47:03,231 [trainer.py] => Trainable params: 589824
2023-09-13 17:47:04,385 [adam_lora.py] => Learning on 0-30
2023-09-13 17:48:44,687 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 3.399, Train_accy 4.00, Test_accy 4.67
2023-09-13 17:49:59,050 [adam_lora.py] => Task 0, Epoch 2/20 => Loss 3.394, Train_accy 4.45, Test_accy 4.83
2023-09-13 17:56:29,870 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 17:56:29,870 [trainer.py] => prefix:  
2023-09-13 17:56:29,871 [trainer.py] => dataset: omnibenchmark
2023-09-13 17:56:29,871 [trainer.py] => memory_size: 0
2023-09-13 17:56:29,872 [trainer.py] => memory_per_class: 0
2023-09-13 17:56:29,872 [trainer.py] => fixed_memory: False
2023-09-13 17:56:29,872 [trainer.py] => shuffle: True
2023-09-13 17:56:29,873 [trainer.py] => init_cls: 30
2023-09-13 17:56:29,873 [trainer.py] => increment: 30
2023-09-13 17:56:29,873 [trainer.py] => model_name: adam_lora
2023-09-13 17:56:29,874 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 17:56:29,874 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 17:56:29,875 [trainer.py] => visible_device: 2,3
2023-09-13 17:56:29,875 [trainer.py] => seed: 1993
2023-09-13 17:56:29,875 [trainer.py] => tuned_epoch: 20
2023-09-13 17:56:29,876 [trainer.py] => init_lr: 0.02
2023-09-13 17:56:29,876 [trainer.py] => batch_size: 96
2023-09-13 17:56:29,877 [trainer.py] => use_A: False
2023-09-13 17:56:29,877 [trainer.py] => weight_decay: 0.0005
2023-09-13 17:56:29,877 [trainer.py] => min_lr: 0
2023-09-13 17:56:29,878 [trainer.py] => ffn_num: 16
2023-09-13 17:56:29,878 [trainer.py] => optimizer: sgd
2023-09-13 17:56:29,879 [trainer.py] => vpt_type: shallow
2023-09-13 17:56:29,879 [trainer.py] => prompt_token_num: 5
2023-09-13 17:56:32,530 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:07:57,739 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 18:07:57,739 [trainer.py] => prefix:  
2023-09-13 18:07:57,739 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:07:57,740 [trainer.py] => memory_size: 0
2023-09-13 18:07:57,740 [trainer.py] => memory_per_class: 0
2023-09-13 18:07:57,740 [trainer.py] => fixed_memory: False
2023-09-13 18:07:57,741 [trainer.py] => shuffle: True
2023-09-13 18:07:57,741 [trainer.py] => init_cls: 30
2023-09-13 18:07:57,741 [trainer.py] => increment: 30
2023-09-13 18:07:57,741 [trainer.py] => model_name: adam_lora
2023-09-13 18:07:57,742 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 18:07:57,742 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:07:57,742 [trainer.py] => visible_device: 2,3
2023-09-13 18:07:57,743 [trainer.py] => seed: 1993
2023-09-13 18:07:57,745 [trainer.py] => tuned_epoch: 20
2023-09-13 18:07:57,745 [trainer.py] => init_lr: 0.02
2023-09-13 18:07:57,745 [trainer.py] => batch_size: 96
2023-09-13 18:07:57,745 [trainer.py] => use_A: False
2023-09-13 18:07:57,746 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:07:57,746 [trainer.py] => min_lr: 0
2023-09-13 18:07:57,746 [trainer.py] => ffn_num: 16
2023-09-13 18:07:57,747 [trainer.py] => optimizer: sgd
2023-09-13 18:07:57,747 [trainer.py] => vpt_type: shallow
2023-09-13 18:07:57,747 [trainer.py] => prompt_token_num: 5
2023-09-13 18:07:58,970 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:08:05,669 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:08:06,101 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:08:11,604 [trainer.py] => All params: 86388480
2023-09-13 18:08:11,611 [trainer.py] => Trainable params: 589824
2023-09-13 18:08:12,891 [adam_lora.py] => Learning on 0-30
2023-09-13 18:09:33,143 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 3.399, Train_accy 4.01, Test_accy 4.17
2023-09-13 18:09:52,860 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 18:09:52,861 [trainer.py] => prefix:  
2023-09-13 18:09:52,861 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:09:52,862 [trainer.py] => memory_size: 0
2023-09-13 18:09:52,862 [trainer.py] => memory_per_class: 0
2023-09-13 18:09:52,862 [trainer.py] => fixed_memory: False
2023-09-13 18:09:52,865 [trainer.py] => shuffle: True
2023-09-13 18:09:52,865 [trainer.py] => init_cls: 30
2023-09-13 18:09:52,865 [trainer.py] => increment: 30
2023-09-13 18:09:52,866 [trainer.py] => model_name: adam_lora
2023-09-13 18:09:52,866 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 18:09:52,867 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:09:52,867 [trainer.py] => visible_device: 2,3
2023-09-13 18:09:52,868 [trainer.py] => seed: 1993
2023-09-13 18:09:52,868 [trainer.py] => tuned_epoch: 20
2023-09-13 18:09:52,869 [trainer.py] => init_lr: 0.02
2023-09-13 18:09:52,869 [trainer.py] => batch_size: 96
2023-09-13 18:09:52,869 [trainer.py] => use_A: False
2023-09-13 18:09:52,870 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:09:52,870 [trainer.py] => min_lr: 0
2023-09-13 18:09:52,871 [trainer.py] => ffn_num: 16
2023-09-13 18:09:52,871 [trainer.py] => optimizer: sgd
2023-09-13 18:09:52,872 [trainer.py] => vpt_type: shallow
2023-09-13 18:09:52,872 [trainer.py] => prompt_token_num: 5
2023-09-13 18:09:55,585 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:10:01,773 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:10:02,145 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:10:07,510 [trainer.py] => All params: 86388480
2023-09-13 18:10:07,541 [trainer.py] => Trainable params: 589824
2023-09-13 18:10:08,827 [adam_lora.py] => Learning on 0-30
2023-09-13 18:19:12,148 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 18:19:12,148 [trainer.py] => prefix:  
2023-09-13 18:19:12,149 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:19:12,149 [trainer.py] => memory_size: 0
2023-09-13 18:19:12,149 [trainer.py] => memory_per_class: 0
2023-09-13 18:19:12,149 [trainer.py] => fixed_memory: False
2023-09-13 18:19:12,150 [trainer.py] => shuffle: True
2023-09-13 18:19:12,150 [trainer.py] => init_cls: 30
2023-09-13 18:19:12,150 [trainer.py] => increment: 30
2023-09-13 18:19:12,150 [trainer.py] => model_name: adam_lora
2023-09-13 18:19:12,151 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 18:19:12,151 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:19:12,151 [trainer.py] => visible_device: 2,3
2023-09-13 18:19:12,152 [trainer.py] => seed: 1993
2023-09-13 18:19:12,152 [trainer.py] => tuned_epoch: 20
2023-09-13 18:19:12,152 [trainer.py] => init_lr: 0.02
2023-09-13 18:19:12,153 [trainer.py] => batch_size: 96
2023-09-13 18:19:12,153 [trainer.py] => use_A: False
2023-09-13 18:19:12,153 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:19:12,153 [trainer.py] => min_lr: 0
2023-09-13 18:19:12,154 [trainer.py] => ffn_num: 16
2023-09-13 18:19:12,154 [trainer.py] => optimizer: sgd
2023-09-13 18:19:12,154 [trainer.py] => vpt_type: shallow
2023-09-13 18:19:12,154 [trainer.py] => prompt_token_num: 5
2023-09-13 18:19:13,215 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:19:19,331 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:19:19,681 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:19:23,334 [trainer.py] => All params: 86388480
2023-09-13 18:19:23,335 [trainer.py] => Trainable params: 589824
2023-09-13 18:19:24,612 [adam_lora.py] => Learning on 0-30
2023-09-13 18:20:36,350 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 3.399, Train_accy 4.01, Test_accy 4.17
2023-09-13 18:21:20,921 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 18:21:20,921 [trainer.py] => prefix:  
2023-09-13 18:21:20,922 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:21:20,922 [trainer.py] => memory_size: 0
2023-09-13 18:21:20,922 [trainer.py] => memory_per_class: 0
2023-09-13 18:21:20,922 [trainer.py] => fixed_memory: False
2023-09-13 18:21:20,923 [trainer.py] => shuffle: True
2023-09-13 18:21:20,924 [trainer.py] => init_cls: 30
2023-09-13 18:21:20,925 [trainer.py] => increment: 30
2023-09-13 18:21:20,925 [trainer.py] => model_name: adam_lora
2023-09-13 18:21:20,925 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 18:21:20,925 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:21:20,926 [trainer.py] => visible_device: 2,3
2023-09-13 18:21:20,926 [trainer.py] => seed: 1993
2023-09-13 18:21:20,926 [trainer.py] => tuned_epoch: 20
2023-09-13 18:21:20,926 [trainer.py] => init_lr: 0.02
2023-09-13 18:21:20,926 [trainer.py] => batch_size: 96
2023-09-13 18:21:20,927 [trainer.py] => use_A: False
2023-09-13 18:21:20,927 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:21:20,927 [trainer.py] => min_lr: 0
2023-09-13 18:21:20,927 [trainer.py] => ffn_num: 16
2023-09-13 18:21:20,927 [trainer.py] => optimizer: sgd
2023-09-13 18:21:20,928 [trainer.py] => vpt_type: shallow
2023-09-13 18:21:20,928 [trainer.py] => prompt_token_num: 5
2023-09-13 18:21:22,045 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:21:27,425 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:21:27,768 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:21:30,717 [trainer.py] => All params: 85798656
2023-09-13 18:21:30,719 [trainer.py] => Trainable params: 0
2023-09-13 18:21:32,023 [adam_lora.py] => Learning on 0-30
2023-09-13 18:22:26,130 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 3.399, Train_accy 3.97, Test_accy 5.00
2023-09-13 18:25:43,640 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 18:25:43,641 [trainer.py] => prefix:  
2023-09-13 18:25:43,641 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:25:43,641 [trainer.py] => memory_size: 0
2023-09-13 18:25:43,642 [trainer.py] => memory_per_class: 0
2023-09-13 18:25:43,642 [trainer.py] => fixed_memory: False
2023-09-13 18:25:43,643 [trainer.py] => shuffle: True
2023-09-13 18:25:43,645 [trainer.py] => init_cls: 30
2023-09-13 18:25:43,645 [trainer.py] => increment: 30
2023-09-13 18:25:43,645 [trainer.py] => model_name: adam_lora
2023-09-13 18:25:43,646 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 18:25:43,646 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:25:43,646 [trainer.py] => visible_device: 2,3
2023-09-13 18:25:43,647 [trainer.py] => seed: 1993
2023-09-13 18:25:43,647 [trainer.py] => tuned_epoch: 20
2023-09-13 18:25:43,648 [trainer.py] => init_lr: 0.02
2023-09-13 18:25:43,648 [trainer.py] => batch_size: 96
2023-09-13 18:25:43,648 [trainer.py] => use_A: False
2023-09-13 18:25:43,649 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:25:43,649 [trainer.py] => min_lr: 0
2023-09-13 18:25:43,650 [trainer.py] => ffn_num: 16
2023-09-13 18:25:43,650 [trainer.py] => optimizer: sgd
2023-09-13 18:25:43,651 [trainer.py] => vpt_type: shallow
2023-09-13 18:25:43,651 [trainer.py] => prompt_token_num: 5
2023-09-13 18:25:45,938 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:26:10,102 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:26:10,476 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:56:55,375 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 18:56:55,375 [trainer.py] => prefix:  
2023-09-13 18:56:55,375 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:56:55,376 [trainer.py] => memory_size: 0
2023-09-13 18:56:55,376 [trainer.py] => memory_per_class: 0
2023-09-13 18:56:55,376 [trainer.py] => fixed_memory: False
2023-09-13 18:56:55,376 [trainer.py] => shuffle: True
2023-09-13 18:56:55,377 [trainer.py] => init_cls: 30
2023-09-13 18:56:55,377 [trainer.py] => increment: 30
2023-09-13 18:56:55,377 [trainer.py] => model_name: adam_lora
2023-09-13 18:56:55,377 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 18:56:55,378 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:56:55,378 [trainer.py] => visible_device: 2,3
2023-09-13 18:56:55,378 [trainer.py] => seed: 1993
2023-09-13 18:56:55,378 [trainer.py] => tuned_epoch: 20
2023-09-13 18:56:55,379 [trainer.py] => init_lr: 0.02
2023-09-13 18:56:55,379 [trainer.py] => batch_size: 96
2023-09-13 18:56:55,379 [trainer.py] => use_A: False
2023-09-13 18:56:55,379 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:56:55,380 [trainer.py] => min_lr: 0
2023-09-13 18:56:55,380 [trainer.py] => ffn_num: 16
2023-09-13 18:56:55,380 [trainer.py] => optimizer: sgd
2023-09-13 18:56:55,381 [trainer.py] => vpt_type: shallow
2023-09-13 18:56:55,381 [trainer.py] => prompt_token_num: 5
2023-09-13 18:56:56,568 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:57:02,262 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:57:02,785 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:57:07,309 [trainer.py] => All params: 85798656
2023-09-13 18:57:07,310 [trainer.py] => Trainable params: 0
2023-09-13 18:57:08,590 [adam_lora.py] => Learning on 0-30
2023-09-13 18:57:59,374 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 3.208, Train_accy 34.01, Test_accy 76.50
2023-09-13 18:58:15,550 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 18:58:15,550 [trainer.py] => prefix:  
2023-09-13 18:58:15,551 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:58:15,551 [trainer.py] => memory_size: 0
2023-09-13 18:58:15,551 [trainer.py] => memory_per_class: 0
2023-09-13 18:58:15,552 [trainer.py] => fixed_memory: False
2023-09-13 18:58:15,552 [trainer.py] => shuffle: True
2023-09-13 18:58:15,553 [trainer.py] => init_cls: 30
2023-09-13 18:58:15,553 [trainer.py] => increment: 30
2023-09-13 18:58:15,554 [trainer.py] => model_name: adam_lora
2023-09-13 18:58:15,554 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 18:58:15,554 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:58:15,555 [trainer.py] => visible_device: 2,3
2023-09-13 18:58:15,555 [trainer.py] => seed: 1993
2023-09-13 18:58:15,555 [trainer.py] => tuned_epoch: 20
2023-09-13 18:58:15,556 [trainer.py] => init_lr: 0.02
2023-09-13 18:58:15,556 [trainer.py] => batch_size: 96
2023-09-13 18:58:15,557 [trainer.py] => use_A: False
2023-09-13 18:58:15,557 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:58:15,557 [trainer.py] => min_lr: 0
2023-09-13 18:58:15,558 [trainer.py] => ffn_num: 16
2023-09-13 18:58:15,558 [trainer.py] => optimizer: sgd
2023-09-13 18:58:15,558 [trainer.py] => vpt_type: shallow
2023-09-13 18:58:15,559 [trainer.py] => prompt_token_num: 5
2023-09-13 18:58:16,639 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:58:22,372 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:58:22,693 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:58:27,339 [trainer.py] => All params: 86388480
2023-09-13 18:58:27,341 [trainer.py] => Trainable params: 589824
2023-09-13 18:58:28,623 [adam_lora.py] => Learning on 0-30
2023-09-13 18:59:37,620 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 3.222, Train_accy 32.61, Test_accy 77.17
2023-09-13 19:00:44,298 [adam_lora.py] => Task 0, Epoch 2/20 => Loss 2.192, Train_accy 63.41, Test_accy 81.33
2023-09-13 19:01:53,153 [adam_lora.py] => Task 0, Epoch 3/20 => Loss 1.471, Train_accy 68.35, Test_accy 88.00
2023-09-13 19:03:00,715 [adam_lora.py] => Task 0, Epoch 4/20 => Loss 1.107, Train_accy 73.09, Test_accy 86.00
2023-09-13 19:04:08,032 [adam_lora.py] => Task 0, Epoch 5/20 => Loss 0.862, Train_accy 75.42, Test_accy 88.00
2023-09-13 19:05:18,361 [adam_lora.py] => Task 0, Epoch 6/20 => Loss 0.766, Train_accy 77.75, Test_accy 88.67
2023-09-13 19:06:25,873 [adam_lora.py] => Task 0, Epoch 7/20 => Loss 0.723, Train_accy 78.22, Test_accy 90.00
2023-09-13 19:07:33,479 [adam_lora.py] => Task 0, Epoch 8/20 => Loss 0.700, Train_accy 78.88, Test_accy 90.17
2023-09-13 19:08:40,532 [adam_lora.py] => Task 0, Epoch 9/20 => Loss 0.667, Train_accy 79.99, Test_accy 89.83
2023-09-13 19:09:47,162 [adam_lora.py] => Task 0, Epoch 10/20 => Loss 0.645, Train_accy 80.23, Test_accy 89.67
2023-09-13 19:10:53,587 [adam_lora.py] => Task 0, Epoch 11/20 => Loss 0.639, Train_accy 80.09, Test_accy 90.33
2023-09-13 19:11:58,370 [adam_lora.py] => Task 0, Epoch 12/20 => Loss 0.633, Train_accy 80.89, Test_accy 92.00
2023-09-13 19:13:05,497 [adam_lora.py] => Task 0, Epoch 13/20 => Loss 0.629, Train_accy 81.13, Test_accy 91.83
2023-09-13 19:14:11,881 [adam_lora.py] => Task 0, Epoch 14/20 => Loss 0.610, Train_accy 81.88, Test_accy 91.17
2023-09-13 19:15:18,922 [adam_lora.py] => Task 0, Epoch 15/20 => Loss 0.617, Train_accy 82.01, Test_accy 90.33
2023-09-13 19:16:25,751 [adam_lora.py] => Task 0, Epoch 16/20 => Loss 0.601, Train_accy 81.99, Test_accy 91.17
2023-09-13 19:17:32,414 [adam_lora.py] => Task 0, Epoch 17/20 => Loss 0.602, Train_accy 81.82, Test_accy 91.83
2023-09-13 19:18:38,872 [adam_lora.py] => Task 0, Epoch 18/20 => Loss 0.595, Train_accy 82.62, Test_accy 91.00
2023-09-13 19:19:44,934 [adam_lora.py] => Task 0, Epoch 19/20 => Loss 0.594, Train_accy 82.42, Test_accy 91.33
2023-09-13 19:20:51,757 [adam_lora.py] => Task 0, Epoch 20/20 => Loss 0.595, Train_accy 82.51, Test_accy 91.33
2023-09-13 19:20:55,806 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 19:20:56,122 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 19:21:04,579 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 19:21:04,840 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 19:22:35,396 [trainer.py] => No NME accuracy.
2023-09-13 19:22:35,397 [trainer.py] => CNN: {'total': 87.5, '00-09': 90.5, '10-19': 88.0, '20-29': 84.0, 'old': 0, 'new': 87.5}
2023-09-13 19:22:35,397 [trainer.py] => CNN top1 curve: [87.5]
2023-09-13 19:22:35,397 [trainer.py] => CNN top5 curve: [98.83]

2023-09-13 19:22:35,398 [trainer.py] => Average Accuracy (CNN): 87.5
2023-09-13 19:22:35,402 [trainer.py] => All params: 172823041
2023-09-13 19:22:35,405 [trainer.py] => Trainable params: 1225729
2023-09-13 19:22:35,419 [adam_lora.py] => Learning on 30-60
2023-09-13 19:24:09,596 [trainer.py] => No NME accuracy.
2023-09-13 19:24:09,596 [trainer.py] => CNN: {'total': 88.07, '00-09': 86.0, '10-19': 87.5, '20-29': 79.5, '30-39': 92.0, '40-49': 88.5, '50-59': 94.97, 'old': 84.33, 'new': 91.82}
2023-09-13 19:24:09,597 [trainer.py] => CNN top1 curve: [87.5, 88.07]
2023-09-13 19:24:09,598 [trainer.py] => CNN top5 curve: [98.83, 97.91]

2023-09-13 19:24:09,598 [trainer.py] => Average Accuracy (CNN): 87.785
2023-09-13 19:24:09,602 [trainer.py] => All params: 172869121
2023-09-13 19:24:09,605 [trainer.py] => Trainable params: 1271809
2023-09-13 19:24:09,615 [adam_lora.py] => Learning on 60-90
2023-09-13 19:26:05,638 [trainer.py] => No NME accuracy.
2023-09-13 19:26:05,639 [trainer.py] => CNN: {'total': 85.54, '00-09': 82.5, '10-19': 82.0, '20-29': 76.0, '30-39': 90.0, '40-49': 86.5, '50-59': 93.47, '60-69': 92.5, '70-79': 87.5, '80-89': 79.4, 'old': 85.07, 'new': 86.48}
2023-09-13 19:26:05,639 [trainer.py] => CNN top1 curve: [87.5, 88.07, 85.54]
2023-09-13 19:26:05,640 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.11]

2023-09-13 19:26:05,640 [trainer.py] => Average Accuracy (CNN): 87.03666666666668
2023-09-13 19:26:05,642 [trainer.py] => All params: 172915201
2023-09-13 19:26:05,646 [trainer.py] => Trainable params: 1317889
2023-09-13 19:26:05,653 [adam_lora.py] => Learning on 90-120
2023-09-13 19:27:57,011 [trainer.py] => No NME accuracy.
2023-09-13 19:27:57,011 [trainer.py] => CNN: {'total': 81.63, '00-09': 80.0, '10-19': 80.0, '20-29': 75.0, '30-39': 89.5, '40-49': 84.5, '50-59': 85.93, '60-69': 87.5, '70-79': 84.0, '80-89': 76.88, '90-99': 80.4, '100-109': 77.89, '110-119': 77.89, 'old': 82.59, 'new': 78.73}
2023-09-13 19:27:57,012 [trainer.py] => CNN top1 curve: [87.5, 88.07, 85.54, 81.63]
2023-09-13 19:27:57,012 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.11, 96.12]

2023-09-13 19:27:57,012 [trainer.py] => Average Accuracy (CNN): 85.685
2023-09-13 19:27:57,015 [trainer.py] => All params: 172961281
2023-09-13 19:27:57,017 [trainer.py] => Trainable params: 1363969
2023-09-13 19:27:57,025 [adam_lora.py] => Learning on 120-150
2023-09-13 19:29:42,564 [trainer.py] => No NME accuracy.
2023-09-13 19:29:42,565 [trainer.py] => CNN: {'total': 79.83, '00-09': 77.5, '10-19': 79.5, '20-29': 74.0, '30-39': 88.5, '40-49': 82.0, '50-59': 80.4, '60-69': 84.0, '70-79': 83.0, '80-89': 74.87, '90-99': 79.9, '100-109': 74.37, '110-119': 73.37, '120-129': 85.0, '130-139': 80.0, '140-149': 80.9, 'old': 79.29, 'new': 81.97}
2023-09-13 19:29:42,565 [trainer.py] => CNN top1 curve: [87.5, 88.07, 85.54, 81.63, 79.83]
2023-09-13 19:29:42,566 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.11, 96.12, 94.96]

2023-09-13 19:29:42,566 [trainer.py] => Average Accuracy (CNN): 84.514
2023-09-13 19:29:42,570 [trainer.py] => All params: 173007361
2023-09-13 19:29:42,572 [trainer.py] => Trainable params: 1410049
2023-09-13 19:29:42,582 [adam_lora.py] => Learning on 150-180
2023-09-13 19:31:28,614 [trainer.py] => No NME accuracy.
2023-09-13 19:31:28,615 [trainer.py] => CNN: {'total': 77.12, '00-09': 75.0, '10-19': 75.5, '20-29': 72.5, '30-39': 85.0, '40-49': 78.5, '50-59': 78.89, '60-69': 83.5, '70-79': 80.0, '80-89': 71.86, '90-99': 79.4, '100-109': 71.86, '110-119': 72.36, '120-129': 84.5, '130-139': 79.0, '140-149': 78.39, '150-159': 79.9, '160-169': 74.37, '170-179': 67.5, 'old': 77.76, 'new': 73.91}
2023-09-13 19:31:28,615 [trainer.py] => CNN top1 curve: [87.5, 88.07, 85.54, 81.63, 79.83, 77.12]
2023-09-13 19:31:28,615 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.11, 96.12, 94.96, 93.57]

2023-09-13 19:31:28,617 [trainer.py] => Average Accuracy (CNN): 83.28166666666667
2023-09-13 19:31:28,620 [trainer.py] => All params: 173053441
2023-09-13 19:31:28,623 [trainer.py] => Trainable params: 1456129
2023-09-13 19:31:28,635 [adam_lora.py] => Learning on 180-210
2023-09-13 19:33:39,989 [trainer.py] => No NME accuracy.
2023-09-13 19:33:39,989 [trainer.py] => CNN: {'total': 76.15, '00-09': 75.0, '10-19': 75.5, '20-29': 71.5, '30-39': 85.0, '40-49': 77.5, '50-59': 77.89, '60-69': 80.5, '70-79': 80.0, '80-89': 71.86, '90-99': 77.39, '100-109': 71.36, '110-119': 68.34, '120-129': 84.5, '130-139': 78.0, '140-149': 77.89, '150-159': 77.89, '160-169': 73.37, '170-179': 67.0, '180-189': 73.74, '190-199': 83.0, '200-209': 71.86, 'old': 76.14, 'new': 76.21}
2023-09-13 19:33:39,990 [trainer.py] => CNN top1 curve: [87.5, 88.07, 85.54, 81.63, 79.83, 77.12, 76.15]
2023-09-13 19:33:39,990 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.11, 96.12, 94.96, 93.57, 93.01]

2023-09-13 19:33:39,990 [trainer.py] => Average Accuracy (CNN): 82.26285714285714
2023-09-13 19:33:39,993 [trainer.py] => All params: 173099521
2023-09-13 19:33:39,994 [trainer.py] => Trainable params: 1502209
2023-09-13 19:33:40,004 [adam_lora.py] => Learning on 210-240
2023-09-13 19:35:39,470 [trainer.py] => No NME accuracy.
2023-09-13 19:35:39,471 [trainer.py] => CNN: {'total': 74.44, '00-09': 74.5, '10-19': 73.5, '20-29': 69.0, '30-39': 85.0, '40-49': 76.0, '50-59': 77.89, '60-69': 80.0, '70-79': 80.0, '80-89': 71.36, '90-99': 77.39, '100-109': 64.32, '110-119': 67.34, '120-129': 82.0, '130-139': 77.5, '140-149': 76.88, '150-159': 77.39, '160-169': 72.36, '170-179': 64.5, '180-189': 73.23, '190-199': 82.5, '200-209': 67.34, '210-219': 50.0, '220-229': 86.43, '230-239': 80.0, 'old': 74.77, 'new': 72.12}
2023-09-13 19:35:39,471 [trainer.py] => CNN top1 curve: [87.5, 88.07, 85.54, 81.63, 79.83, 77.12, 76.15, 74.44]
2023-09-13 19:35:39,471 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.11, 96.12, 94.96, 93.57, 93.01, 91.62]

2023-09-13 19:35:39,472 [trainer.py] => Average Accuracy (CNN): 81.285
2023-09-13 19:35:39,475 [trainer.py] => All params: 173145601
2023-09-13 19:35:39,477 [trainer.py] => Trainable params: 1548289
2023-09-13 19:35:39,485 [adam_lora.py] => Learning on 240-270
2023-09-13 19:37:48,587 [trainer.py] => No NME accuracy.
2023-09-13 19:37:48,588 [trainer.py] => CNN: {'total': 73.58, '00-09': 74.5, '10-19': 73.0, '20-29': 69.0, '30-39': 84.5, '40-49': 76.0, '50-59': 76.88, '60-69': 78.0, '70-79': 79.5, '80-89': 70.35, '90-99': 75.88, '100-109': 63.82, '110-119': 63.82, '120-129': 76.5, '130-139': 77.0, '140-149': 72.86, '150-159': 73.87, '160-169': 72.36, '170-179': 64.5, '180-189': 73.23, '190-199': 81.0, '200-209': 65.33, '210-219': 50.0, '220-229': 86.43, '230-239': 80.0, '240-249': 70.0, '250-259': 76.38, '260-269': 82.0, 'old': 73.27, 'new': 76.13}
2023-09-13 19:37:48,588 [trainer.py] => CNN top1 curve: [87.5, 88.07, 85.54, 81.63, 79.83, 77.12, 76.15, 74.44, 73.58]
2023-09-13 19:37:48,588 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.11, 96.12, 94.96, 93.57, 93.01, 91.62, 91.78]

2023-09-13 19:37:48,589 [trainer.py] => Average Accuracy (CNN): 80.42888888888889
2023-09-13 19:37:48,591 [trainer.py] => All params: 173191681
2023-09-13 19:37:48,594 [trainer.py] => Trainable params: 1594369
2023-09-13 19:37:48,610 [adam_lora.py] => Learning on 270-300
2023-09-13 19:39:54,811 [trainer.py] => No NME accuracy.
2023-09-13 19:39:54,811 [trainer.py] => CNN: {'total': 73.67, '00-09': 74.0, '10-19': 73.0, '20-29': 68.0, '30-39': 84.5, '40-49': 73.0, '50-59': 76.88, '60-69': 77.5, '70-79': 79.5, '80-89': 69.85, '90-99': 72.36, '100-109': 62.81, '110-119': 62.81, '120-129': 76.5, '130-139': 76.5, '140-149': 72.86, '150-159': 73.87, '160-169': 70.85, '170-179': 64.0, '180-189': 72.22, '190-199': 80.5, '200-209': 65.33, '210-219': 49.0, '220-229': 86.43, '230-239': 80.0, '240-249': 67.5, '250-259': 75.88, '260-269': 81.0, '270-279': 79.4, '280-289': 83.0, '290-299': 80.9, 'old': 72.84, 'new': 81.1}
2023-09-13 19:39:54,811 [trainer.py] => CNN top1 curve: [87.5, 88.07, 85.54, 81.63, 79.83, 77.12, 76.15, 74.44, 73.58, 73.67]
2023-09-13 19:39:54,812 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.11, 96.12, 94.96, 93.57, 93.01, 91.62, 91.78, 91.76]

2023-09-13 19:39:54,812 [trainer.py] => Average Accuracy (CNN): 79.753
2023-09-13 20:43:28,441 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 20:43:28,441 [trainer.py] => prefix:  
2023-09-13 20:43:28,442 [trainer.py] => dataset: omnibenchmark
2023-09-13 20:43:28,442 [trainer.py] => memory_size: 0
2023-09-13 20:43:28,445 [trainer.py] => memory_per_class: 0
2023-09-13 20:43:28,445 [trainer.py] => fixed_memory: False
2023-09-13 20:43:28,445 [trainer.py] => shuffle: True
2023-09-13 20:43:28,445 [trainer.py] => init_cls: 30
2023-09-13 20:43:28,446 [trainer.py] => increment: 30
2023-09-13 20:43:28,446 [trainer.py] => model_name: adam_lora
2023-09-13 20:43:28,446 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 20:43:28,446 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 20:43:28,447 [trainer.py] => visible_device: 2,3
2023-09-13 20:43:28,447 [trainer.py] => seed: 1993
2023-09-13 20:43:28,447 [trainer.py] => tuned_epoch: 20
2023-09-13 20:43:28,447 [trainer.py] => init_lr: 0.05
2023-09-13 20:43:28,448 [trainer.py] => batch_size: 96
2023-09-13 20:43:28,448 [trainer.py] => use_A: False
2023-09-13 20:43:28,448 [trainer.py] => weight_decay: 0.0005
2023-09-13 20:43:28,448 [trainer.py] => min_lr: 0
2023-09-13 20:43:28,449 [trainer.py] => ffn_num: 16
2023-09-13 20:43:28,449 [trainer.py] => optimizer: sgd
2023-09-13 20:43:28,449 [trainer.py] => vpt_type: shallow
2023-09-13 20:43:28,449 [trainer.py] => prompt_token_num: 5
2023-09-13 20:43:29,700 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 20:43:34,625 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 20:43:35,047 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 20:43:53,669 [trainer.py] => All params: 86388480
2023-09-13 20:43:53,672 [trainer.py] => Trainable params: 589824
2023-09-13 20:43:55,237 [adam_lora.py] => Learning on 0-30
2023-09-13 20:45:18,394 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 2.678, Train_accy 47.31, Test_accy 82.33
2023-09-13 20:46:10,329 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 20:46:10,330 [trainer.py] => prefix:  
2023-09-13 20:46:10,330 [trainer.py] => dataset: omnibenchmark
2023-09-13 20:46:10,330 [trainer.py] => memory_size: 0
2023-09-13 20:46:10,331 [trainer.py] => memory_per_class: 0
2023-09-13 20:46:10,331 [trainer.py] => fixed_memory: False
2023-09-13 20:46:10,331 [trainer.py] => shuffle: True
2023-09-13 20:46:10,332 [trainer.py] => init_cls: 30
2023-09-13 20:46:10,332 [trainer.py] => increment: 30
2023-09-13 20:46:10,332 [trainer.py] => model_name: adam_lora
2023-09-13 20:46:10,332 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 20:46:10,333 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 20:46:10,333 [trainer.py] => visible_device: 2,3
2023-09-13 20:46:10,333 [trainer.py] => seed: 1993
2023-09-13 20:46:10,333 [trainer.py] => tuned_epoch: 20
2023-09-13 20:46:10,334 [trainer.py] => init_lr: 0.05
2023-09-13 20:46:10,334 [trainer.py] => batch_size: 96
2023-09-13 20:46:10,334 [trainer.py] => use_A: False
2023-09-13 20:46:10,335 [trainer.py] => weight_decay: 0.0005
2023-09-13 20:46:10,335 [trainer.py] => min_lr: 0
2023-09-13 20:46:10,335 [trainer.py] => ffn_num: 16
2023-09-13 20:46:10,335 [trainer.py] => optimizer: sgd
2023-09-13 20:46:10,336 [trainer.py] => vpt_type: shallow
2023-09-13 20:46:10,336 [trainer.py] => prompt_token_num: 5
2023-09-13 20:46:11,450 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 20:46:16,482 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 20:46:16,846 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 20:46:22,835 [trainer.py] => All params: 85946112
2023-09-13 20:46:22,836 [trainer.py] => Trainable params: 147456
2023-09-13 20:46:24,132 [adam_lora.py] => Learning on 0-30
2023-09-13 20:47:34,996 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 2.625, Train_accy 48.76, Test_accy 84.67
2023-09-13 20:48:42,678 [adam_lora.py] => Task 0, Epoch 2/20 => Loss 0.829, Train_accy 75.27, Test_accy 84.33
2023-09-13 20:49:51,894 [adam_lora.py] => Task 0, Epoch 3/20 => Loss 0.676, Train_accy 78.55, Test_accy 90.00
2023-09-13 20:50:59,367 [adam_lora.py] => Task 0, Epoch 4/20 => Loss 0.616, Train_accy 79.99, Test_accy 89.83
2023-09-13 20:52:06,509 [adam_lora.py] => Task 0, Epoch 5/20 => Loss 0.589, Train_accy 81.30, Test_accy 90.83
2023-09-13 20:53:14,029 [adam_lora.py] => Task 0, Epoch 6/20 => Loss 0.572, Train_accy 82.04, Test_accy 92.67
2023-09-13 20:54:22,003 [adam_lora.py] => Task 0, Epoch 7/20 => Loss 0.559, Train_accy 82.23, Test_accy 92.83
2023-09-13 20:55:28,982 [adam_lora.py] => Task 0, Epoch 8/20 => Loss 0.545, Train_accy 82.45, Test_accy 92.00
2023-09-13 20:56:36,391 [adam_lora.py] => Task 0, Epoch 9/20 => Loss 0.514, Train_accy 83.66, Test_accy 93.17
2023-09-13 20:57:43,590 [adam_lora.py] => Task 0, Epoch 10/20 => Loss 0.517, Train_accy 83.59, Test_accy 93.00
2023-09-13 20:58:51,841 [adam_lora.py] => Task 0, Epoch 11/20 => Loss 0.496, Train_accy 84.27, Test_accy 92.67
2023-09-13 20:59:59,252 [adam_lora.py] => Task 0, Epoch 12/20 => Loss 0.506, Train_accy 83.87, Test_accy 92.50
2023-09-13 21:01:08,262 [adam_lora.py] => Task 0, Epoch 13/20 => Loss 0.490, Train_accy 84.46, Test_accy 93.67
2023-09-13 21:02:15,793 [adam_lora.py] => Task 0, Epoch 14/20 => Loss 0.488, Train_accy 84.70, Test_accy 93.00
2023-09-13 21:03:23,304 [adam_lora.py] => Task 0, Epoch 15/20 => Loss 0.476, Train_accy 84.53, Test_accy 93.50
2023-09-13 21:04:31,108 [adam_lora.py] => Task 0, Epoch 16/20 => Loss 0.466, Train_accy 85.07, Test_accy 93.33
2023-09-13 21:05:36,910 [adam_lora.py] => Task 0, Epoch 17/20 => Loss 0.461, Train_accy 85.57, Test_accy 93.00
2023-09-13 21:06:44,154 [adam_lora.py] => Task 0, Epoch 18/20 => Loss 0.474, Train_accy 84.97, Test_accy 93.67
2023-09-13 21:07:51,545 [adam_lora.py] => Task 0, Epoch 19/20 => Loss 0.467, Train_accy 85.64, Test_accy 93.67
2023-09-13 21:08:59,378 [adam_lora.py] => Task 0, Epoch 20/20 => Loss 0.475, Train_accy 84.64, Test_accy 93.50
2023-09-13 21:09:02,588 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 21:09:02,928 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 21:09:29,749 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 21:09:30,029 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 21:11:01,845 [trainer.py] => No NME accuracy.
2023-09-13 21:11:01,846 [trainer.py] => CNN: {'total': 89.67, '00-09': 90.5, '10-19': 91.0, '20-29': 87.5, 'old': 0, 'new': 89.67}
2023-09-13 21:11:01,849 [trainer.py] => CNN top1 curve: [89.67]
2023-09-13 21:11:01,849 [trainer.py] => CNN top5 curve: [99.33]

2023-09-13 21:11:01,849 [trainer.py] => Average Accuracy (CNN): 89.67
2023-09-13 21:11:01,853 [trainer.py] => All params: 171938305
2023-09-13 21:11:01,856 [trainer.py] => Trainable params: 340993
2023-09-13 21:11:01,860 [adam_lora.py] => Learning on 30-60
2023-09-13 21:12:37,628 [trainer.py] => No NME accuracy.
2023-09-13 21:12:37,629 [trainer.py] => CNN: {'total': 88.91, '00-09': 85.0, '10-19': 90.0, '20-29': 84.5, '30-39': 91.5, '40-49': 88.5, '50-59': 93.97, 'old': 86.5, 'new': 91.32}
2023-09-13 21:12:37,629 [trainer.py] => CNN top1 curve: [89.67, 88.91]
2023-09-13 21:12:37,630 [trainer.py] => CNN top5 curve: [99.33, 99.08]

2023-09-13 21:12:37,630 [trainer.py] => Average Accuracy (CNN): 89.28999999999999
2023-09-13 21:12:37,634 [trainer.py] => All params: 171984385
2023-09-13 21:12:37,637 [trainer.py] => Trainable params: 387073
2023-09-13 21:12:37,646 [adam_lora.py] => Learning on 60-90
2023-09-13 21:14:07,140 [trainer.py] => No NME accuracy.
2023-09-13 21:14:07,141 [trainer.py] => CNN: {'total': 86.82, '00-09': 83.0, '10-19': 84.5, '20-29': 83.0, '30-39': 89.5, '40-49': 86.5, '50-59': 92.46, '60-69': 92.0, '70-79': 88.0, '80-89': 82.41, 'old': 86.49, 'new': 87.48}
2023-09-13 21:14:07,141 [trainer.py] => CNN top1 curve: [89.67, 88.91, 86.82]
2023-09-13 21:14:07,142 [trainer.py] => CNN top5 curve: [99.33, 99.08, 97.89]

2023-09-13 21:14:07,143 [trainer.py] => Average Accuracy (CNN): 88.46666666666665
2023-09-13 21:14:07,149 [trainer.py] => All params: 172030465
2023-09-13 21:14:07,153 [trainer.py] => Trainable params: 433153
2023-09-13 21:14:07,164 [adam_lora.py] => Learning on 90-120
2023-09-13 21:15:48,407 [trainer.py] => No NME accuracy.
2023-09-13 21:15:48,410 [trainer.py] => CNN: {'total': 82.96, '00-09': 81.5, '10-19': 81.5, '20-29': 82.5, '30-39': 88.5, '40-49': 84.0, '50-59': 85.43, '60-69': 87.5, '70-79': 85.5, '80-89': 79.4, '90-99': 82.41, '100-109': 78.89, '110-119': 78.39, 'old': 83.98, 'new': 79.9}
2023-09-13 21:15:48,410 [trainer.py] => CNN top1 curve: [89.67, 88.91, 86.82, 82.96]
2023-09-13 21:15:48,410 [trainer.py] => CNN top5 curve: [99.33, 99.08, 97.89, 96.83]

2023-09-13 21:15:48,410 [trainer.py] => Average Accuracy (CNN): 87.08999999999999
2023-09-13 21:15:48,413 [trainer.py] => All params: 172076545
2023-09-13 21:15:48,415 [trainer.py] => Trainable params: 479233
2023-09-13 21:15:48,422 [adam_lora.py] => Learning on 120-150
2023-09-13 21:17:28,601 [trainer.py] => No NME accuracy.
2023-09-13 21:17:28,601 [trainer.py] => CNN: {'total': 81.0, '00-09': 79.0, '10-19': 81.5, '20-29': 81.5, '30-39': 88.5, '40-49': 82.0, '50-59': 80.4, '60-69': 83.5, '70-79': 84.5, '80-89': 77.89, '90-99': 81.41, '100-109': 75.88, '110-119': 73.37, '120-129': 84.5, '130-139': 80.5, '140-149': 80.4, 'old': 80.79, 'new': 81.8}
2023-09-13 21:17:28,602 [trainer.py] => CNN top1 curve: [89.67, 88.91, 86.82, 82.96, 81.0]
2023-09-13 21:17:28,603 [trainer.py] => CNN top5 curve: [99.33, 99.08, 97.89, 96.83, 95.62]

2023-09-13 21:17:28,603 [trainer.py] => Average Accuracy (CNN): 85.87199999999999
2023-09-13 21:17:28,611 [trainer.py] => All params: 172122625
2023-09-13 21:17:28,613 [trainer.py] => Trainable params: 525313
2023-09-13 21:17:28,619 [adam_lora.py] => Learning on 150-180
2023-09-13 21:19:17,679 [trainer.py] => No NME accuracy.
2023-09-13 21:19:17,680 [trainer.py] => CNN: {'total': 78.15, '00-09': 76.5, '10-19': 79.0, '20-29': 77.5, '30-39': 86.0, '40-49': 78.5, '50-59': 78.89, '60-69': 82.5, '70-79': 81.5, '80-89': 75.38, '90-99': 80.9, '100-109': 72.86, '110-119': 70.85, '120-129': 83.5, '130-139': 80.0, '140-149': 79.4, '150-159': 78.89, '160-169': 73.37, '170-179': 71.0, 'old': 78.89, 'new': 74.41}
2023-09-13 21:19:17,680 [trainer.py] => CNN top1 curve: [89.67, 88.91, 86.82, 82.96, 81.0, 78.15]
2023-09-13 21:19:17,680 [trainer.py] => CNN top5 curve: [99.33, 99.08, 97.89, 96.83, 95.62, 94.32]

2023-09-13 21:19:17,681 [trainer.py] => Average Accuracy (CNN): 84.585
2023-09-13 21:19:17,684 [trainer.py] => All params: 172168705
2023-09-13 21:19:17,687 [trainer.py] => Trainable params: 571393
2023-09-13 21:19:17,705 [adam_lora.py] => Learning on 180-210
2023-09-13 21:21:11,529 [trainer.py] => No NME accuracy.
2023-09-13 21:21:11,530 [trainer.py] => CNN: {'total': 77.25, '00-09': 76.0, '10-19': 78.5, '20-29': 77.0, '30-39': 86.0, '40-49': 77.5, '50-59': 78.39, '60-69': 81.0, '70-79': 81.5, '80-89': 74.37, '90-99': 79.4, '100-109': 71.86, '110-119': 67.84, '120-129': 83.0, '130-139': 78.5, '140-149': 78.89, '150-159': 75.88, '160-169': 71.86, '170-179': 70.5, '180-189': 75.25, '190-199': 84.5, '200-209': 74.37, 'old': 77.12, 'new': 78.06}
2023-09-13 21:21:11,530 [trainer.py] => CNN top1 curve: [89.67, 88.91, 86.82, 82.96, 81.0, 78.15, 77.25]
2023-09-13 21:21:11,531 [trainer.py] => CNN top5 curve: [99.33, 99.08, 97.89, 96.83, 95.62, 94.32, 94.03]

2023-09-13 21:21:11,531 [trainer.py] => Average Accuracy (CNN): 83.53714285714285
2023-09-13 21:21:11,534 [trainer.py] => All params: 172214785
2023-09-13 21:21:11,536 [trainer.py] => Trainable params: 617473
2023-09-13 21:21:11,544 [adam_lora.py] => Learning on 210-240
2023-09-13 21:23:22,376 [trainer.py] => No NME accuracy.
2023-09-13 21:23:22,377 [trainer.py] => CNN: {'total': 75.65, '00-09': 74.5, '10-19': 76.5, '20-29': 76.5, '30-39': 86.0, '40-49': 76.0, '50-59': 78.39, '60-69': 80.5, '70-79': 81.5, '80-89': 73.37, '90-99': 78.89, '100-109': 64.32, '110-119': 67.84, '120-129': 80.0, '130-139': 78.5, '140-149': 77.89, '150-159': 75.38, '160-169': 71.86, '170-179': 68.5, '180-189': 75.25, '190-199': 84.0, '200-209': 70.35, '210-219': 49.5, '220-229': 89.45, '230-239': 80.5, 'old': 76.01, 'new': 73.12}
2023-09-13 21:23:22,378 [trainer.py] => CNN top1 curve: [89.67, 88.91, 86.82, 82.96, 81.0, 78.15, 77.25, 75.65]
2023-09-13 21:23:22,378 [trainer.py] => CNN top5 curve: [99.33, 99.08, 97.89, 96.83, 95.62, 94.32, 94.03, 92.84]

2023-09-13 21:23:22,378 [trainer.py] => Average Accuracy (CNN): 82.55125
2023-09-13 21:23:22,380 [trainer.py] => All params: 172260865
2023-09-13 21:23:22,382 [trainer.py] => Trainable params: 663553
2023-09-13 21:23:22,393 [adam_lora.py] => Learning on 240-270
2023-09-13 21:25:27,510 [trainer.py] => No NME accuracy.
2023-09-13 21:25:27,511 [trainer.py] => CNN: {'total': 74.64, '00-09': 74.5, '10-19': 76.0, '20-29': 76.5, '30-39': 85.0, '40-49': 76.0, '50-59': 77.89, '60-69': 78.5, '70-79': 81.0, '80-89': 72.36, '90-99': 77.39, '100-109': 62.81, '110-119': 64.82, '120-129': 74.5, '130-139': 77.5, '140-149': 73.87, '150-159': 72.86, '160-169': 71.86, '170-179': 68.0, '180-189': 74.75, '190-199': 82.0, '200-209': 67.84, '210-219': 49.5, '220-229': 89.45, '230-239': 80.5, '240-249': 69.5, '250-259': 77.39, '260-269': 83.0, 'old': 74.39, 'new': 76.63}
2023-09-13 21:25:27,511 [trainer.py] => CNN top1 curve: [89.67, 88.91, 86.82, 82.96, 81.0, 78.15, 77.25, 75.65, 74.64]
2023-09-13 21:25:27,512 [trainer.py] => CNN top5 curve: [99.33, 99.08, 97.89, 96.83, 95.62, 94.32, 94.03, 92.84, 92.91]

2023-09-13 21:25:27,512 [trainer.py] => Average Accuracy (CNN): 81.67222222222222
2023-09-13 21:25:27,515 [trainer.py] => All params: 172306945
2023-09-13 21:25:27,519 [trainer.py] => Trainable params: 709633
2023-09-13 21:25:27,532 [adam_lora.py] => Learning on 270-300
2023-09-13 21:27:30,524 [trainer.py] => No NME accuracy.
2023-09-13 21:27:30,525 [trainer.py] => CNN: {'total': 74.5, '00-09': 74.0, '10-19': 76.0, '20-29': 76.0, '30-39': 85.0, '40-49': 73.0, '50-59': 77.89, '60-69': 77.5, '70-79': 81.0, '80-89': 72.36, '90-99': 73.87, '100-109': 62.31, '110-119': 64.32, '120-129': 74.5, '130-139': 77.0, '140-149': 73.87, '150-159': 72.86, '160-169': 70.35, '170-179': 66.5, '180-189': 74.24, '190-199': 81.5, '200-209': 66.83, '210-219': 48.0, '220-229': 89.45, '230-239': 79.5, '240-249': 68.0, '250-259': 76.88, '260-269': 82.0, '270-279': 78.39, '280-289': 82.5, '290-299': 79.4, 'old': 73.88, 'new': 80.1}
2023-09-13 21:27:30,526 [trainer.py] => CNN top1 curve: [89.67, 88.91, 86.82, 82.96, 81.0, 78.15, 77.25, 75.65, 74.64, 74.5]
2023-09-13 21:27:30,526 [trainer.py] => CNN top5 curve: [99.33, 99.08, 97.89, 96.83, 95.62, 94.32, 94.03, 92.84, 92.91, 92.75]

2023-09-13 21:27:30,526 [trainer.py] => Average Accuracy (CNN): 80.955
2023-09-13 21:31:57,113 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 21:31:57,113 [trainer.py] => prefix:  
2023-09-13 21:31:57,113 [trainer.py] => dataset: omnibenchmark
2023-09-13 21:31:57,114 [trainer.py] => memory_size: 0
2023-09-13 21:31:57,114 [trainer.py] => memory_per_class: 0
2023-09-13 21:31:57,114 [trainer.py] => fixed_memory: False
2023-09-13 21:31:57,115 [trainer.py] => shuffle: True
2023-09-13 21:31:57,115 [trainer.py] => init_cls: 30
2023-09-13 21:31:57,116 [trainer.py] => increment: 30
2023-09-13 21:31:57,116 [trainer.py] => model_name: adam_lora
2023-09-13 21:31:57,116 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 21:31:57,117 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 21:31:57,117 [trainer.py] => visible_device: 2,3
2023-09-13 21:31:57,117 [trainer.py] => seed: 1993
2023-09-13 21:31:57,117 [trainer.py] => tuned_epoch: 20
2023-09-13 21:31:57,118 [trainer.py] => init_lr: 0.05
2023-09-13 21:31:57,118 [trainer.py] => batch_size: 96
2023-09-13 21:31:57,118 [trainer.py] => use_A: False
2023-09-13 21:31:57,118 [trainer.py] => weight_decay: 0.0005
2023-09-13 21:31:57,119 [trainer.py] => min_lr: 0
2023-09-13 21:31:57,119 [trainer.py] => ffn_num: 16
2023-09-13 21:31:57,119 [trainer.py] => optimizer: adam
2023-09-13 21:31:57,119 [trainer.py] => vpt_type: shallow
2023-09-13 21:31:57,120 [trainer.py] => prompt_token_num: 5
2023-09-13 21:31:58,284 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 21:32:04,876 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 21:32:05,220 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 21:32:15,965 [trainer.py] => All params: 85946112
2023-09-13 21:32:15,968 [trainer.py] => Trainable params: 147456
2023-09-13 21:32:17,296 [adam_lora.py] => Learning on 0-30
2023-09-13 21:33:27,508 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 3.393, Train_accy 5.16, Test_accy 3.33
2023-09-13 21:34:33,607 [adam_lora.py] => Task 0, Epoch 2/20 => Loss 3.401, Train_accy 3.59, Test_accy 3.33
2023-09-13 21:34:47,895 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 21:34:47,896 [trainer.py] => prefix:  
2023-09-13 21:34:47,896 [trainer.py] => dataset: omnibenchmark
2023-09-13 21:34:47,896 [trainer.py] => memory_size: 0
2023-09-13 21:34:47,897 [trainer.py] => memory_per_class: 0
2023-09-13 21:34:47,897 [trainer.py] => fixed_memory: False
2023-09-13 21:34:47,897 [trainer.py] => shuffle: True
2023-09-13 21:34:47,898 [trainer.py] => init_cls: 30
2023-09-13 21:34:47,898 [trainer.py] => increment: 30
2023-09-13 21:34:47,898 [trainer.py] => model_name: adam_lora
2023-09-13 21:34:47,898 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 21:34:47,899 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 21:34:47,899 [trainer.py] => visible_device: 2,3
2023-09-13 21:34:47,899 [trainer.py] => seed: 1993
2023-09-13 21:34:47,899 [trainer.py] => tuned_epoch: 20
2023-09-13 21:34:47,900 [trainer.py] => init_lr: 0.05
2023-09-13 21:34:47,900 [trainer.py] => batch_size: 96
2023-09-13 21:34:47,900 [trainer.py] => use_A: False
2023-09-13 21:34:47,900 [trainer.py] => weight_decay: 0.0005
2023-09-13 21:34:47,901 [trainer.py] => min_lr: 0
2023-09-13 21:34:47,901 [trainer.py] => ffn_num: 16
2023-09-13 21:34:47,901 [trainer.py] => optimizer: sgd
2023-09-13 21:34:47,901 [trainer.py] => vpt_type: shallow
2023-09-13 21:34:47,901 [trainer.py] => prompt_token_num: 5
2023-09-13 21:34:49,059 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 21:34:55,154 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 21:34:55,497 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 21:35:00,622 [trainer.py] => All params: 85946112
2023-09-13 21:35:00,623 [trainer.py] => Trainable params: 147456
2023-09-13 21:35:01,881 [adam_lora.py] => Learning on 0-30
2023-09-13 21:36:11,190 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 2.637, Train_accy 47.01, Test_accy 84.50
2023-09-13 21:36:30,936 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 21:36:30,936 [trainer.py] => prefix:  
2023-09-13 21:36:30,937 [trainer.py] => dataset: omnibenchmark
2023-09-13 21:36:30,937 [trainer.py] => memory_size: 0
2023-09-13 21:36:30,937 [trainer.py] => memory_per_class: 0
2023-09-13 21:36:30,937 [trainer.py] => fixed_memory: False
2023-09-13 21:36:30,938 [trainer.py] => shuffle: True
2023-09-13 21:36:30,938 [trainer.py] => init_cls: 30
2023-09-13 21:36:30,938 [trainer.py] => increment: 30
2023-09-13 21:36:30,938 [trainer.py] => model_name: adam_lora
2023-09-13 21:36:30,939 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 21:36:30,939 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 21:36:30,939 [trainer.py] => visible_device: 2,3
2023-09-13 21:36:30,939 [trainer.py] => seed: 1993
2023-09-13 21:36:30,940 [trainer.py] => tuned_epoch: 20
2023-09-13 21:36:30,940 [trainer.py] => init_lr: 0.05
2023-09-13 21:36:30,940 [trainer.py] => batch_size: 96
2023-09-13 21:36:30,940 [trainer.py] => use_A: False
2023-09-13 21:36:30,941 [trainer.py] => weight_decay: 0.0005
2023-09-13 21:36:30,941 [trainer.py] => min_lr: 0
2023-09-13 21:36:30,941 [trainer.py] => ffn_num: 16
2023-09-13 21:36:30,941 [trainer.py] => optimizer: sgd
2023-09-13 21:36:30,942 [trainer.py] => vpt_type: shallow
2023-09-13 21:36:30,942 [trainer.py] => prompt_token_num: 5
2023-09-13 21:36:32,092 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 21:36:37,968 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 21:36:38,313 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 21:36:43,511 [trainer.py] => All params: 85946112
2023-09-13 21:36:43,514 [trainer.py] => Trainable params: 147456
2023-09-13 21:36:44,934 [adam_lora.py] => Learning on 0-30
2023-09-13 21:37:55,245 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 2.658, Train_accy 45.61, Test_accy 84.17
2023-09-13 21:39:01,455 [adam_lora.py] => Task 0, Epoch 2/20 => Loss 0.848, Train_accy 75.28, Test_accy 85.33
2023-09-13 21:40:08,028 [adam_lora.py] => Task 0, Epoch 3/20 => Loss 0.672, Train_accy 78.82, Test_accy 89.00
2023-09-13 21:41:14,423 [adam_lora.py] => Task 0, Epoch 4/20 => Loss 0.633, Train_accy 79.62, Test_accy 89.50
2023-09-13 21:42:21,286 [adam_lora.py] => Task 0, Epoch 5/20 => Loss 0.600, Train_accy 80.81, Test_accy 91.50
2023-09-13 21:43:27,540 [adam_lora.py] => Task 0, Epoch 6/20 => Loss 0.583, Train_accy 81.09, Test_accy 92.00
2023-09-13 21:44:34,888 [adam_lora.py] => Task 0, Epoch 7/20 => Loss 0.564, Train_accy 81.86, Test_accy 92.50
2023-09-13 21:45:41,979 [adam_lora.py] => Task 0, Epoch 8/20 => Loss 0.550, Train_accy 82.16, Test_accy 92.67
2023-09-13 21:46:48,351 [adam_lora.py] => Task 0, Epoch 9/20 => Loss 0.532, Train_accy 83.12, Test_accy 92.50
2023-09-13 21:47:55,204 [adam_lora.py] => Task 0, Epoch 10/20 => Loss 0.520, Train_accy 83.39, Test_accy 93.00
2023-09-13 21:49:01,847 [adam_lora.py] => Task 0, Epoch 11/20 => Loss 0.501, Train_accy 84.05, Test_accy 92.67
2023-09-13 21:50:08,223 [adam_lora.py] => Task 0, Epoch 12/20 => Loss 0.499, Train_accy 84.20, Test_accy 93.00
2023-09-13 21:51:15,051 [adam_lora.py] => Task 0, Epoch 13/20 => Loss 0.491, Train_accy 84.22, Test_accy 93.83
2023-09-13 21:52:21,728 [adam_lora.py] => Task 0, Epoch 14/20 => Loss 0.492, Train_accy 84.23, Test_accy 93.67
2023-09-13 21:53:28,173 [adam_lora.py] => Task 0, Epoch 15/20 => Loss 0.486, Train_accy 84.25, Test_accy 93.33
2023-09-13 21:54:09,456 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 21:54:09,456 [trainer.py] => prefix:  
2023-09-13 21:54:09,457 [trainer.py] => dataset: omnibenchmark
2023-09-13 21:54:09,457 [trainer.py] => memory_size: 0
2023-09-13 21:54:09,457 [trainer.py] => memory_per_class: 0
2023-09-13 21:54:09,457 [trainer.py] => fixed_memory: False
2023-09-13 21:54:09,458 [trainer.py] => shuffle: True
2023-09-13 21:54:09,458 [trainer.py] => init_cls: 30
2023-09-13 21:54:09,458 [trainer.py] => increment: 30
2023-09-13 21:54:09,459 [trainer.py] => model_name: adam_lora
2023-09-13 21:54:09,459 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 21:54:09,459 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 21:54:09,459 [trainer.py] => visible_device: 2,3
2023-09-13 21:54:09,460 [trainer.py] => seed: 1993
2023-09-13 21:54:09,460 [trainer.py] => tuned_epoch: 20
2023-09-13 21:54:09,460 [trainer.py] => init_lr: 0.05
2023-09-13 21:54:09,460 [trainer.py] => batch_size: 96
2023-09-13 21:54:09,461 [trainer.py] => use_A: False
2023-09-13 21:54:09,461 [trainer.py] => weight_decay: 0.0005
2023-09-13 21:54:09,461 [trainer.py] => min_lr: 0
2023-09-13 21:54:09,462 [trainer.py] => ffn_num: 16
2023-09-13 21:54:09,462 [trainer.py] => optimizer: sgd
2023-09-13 21:54:09,462 [trainer.py] => vpt_type: shallow
2023-09-13 21:54:09,464 [trainer.py] => prompt_token_num: 5
2023-09-13 21:54:10,765 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 21:54:17,156 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 21:54:17,528 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 21:54:22,439 [trainer.py] => All params: 85946112
2023-09-13 21:54:22,441 [trainer.py] => Trainable params: 147456
2023-09-13 21:54:24,384 [adam_lora.py] => Learning on 0-30
2023-09-13 21:54:37,065 [adam_lora.py] => Task 0, Epoch 16/20 => Loss 0.473, Train_accy 85.22, Test_accy 93.00
2023-09-13 21:55:43,187 [adam_lora.py] => Task 0, Epoch 17/20 => Loss 0.476, Train_accy 84.89, Test_accy 93.33
2023-09-13 21:56:48,984 [adam_lora.py] => Task 0, Epoch 18/20 => Loss 0.477, Train_accy 85.11, Test_accy 93.17
2023-09-13 21:57:55,868 [adam_lora.py] => Task 0, Epoch 19/20 => Loss 0.467, Train_accy 85.54, Test_accy 93.17
2023-09-13 21:59:02,985 [adam_lora.py] => Task 0, Epoch 20/20 => Loss 0.480, Train_accy 85.11, Test_accy 93.17
2023-09-13 21:59:07,957 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 21:59:08,314 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 21:59:17,467 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 21:59:17,784 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 22:00:48,587 [trainer.py] => No NME accuracy.
2023-09-13 22:00:48,588 [trainer.py] => CNN: {'total': 89.83, '00-09': 91.0, '10-19': 90.0, '20-29': 88.5, 'old': 0, 'new': 89.83}
2023-09-13 22:00:48,588 [trainer.py] => CNN top1 curve: [89.83]
2023-09-13 22:00:48,590 [trainer.py] => CNN top5 curve: [99.17]

2023-09-13 22:00:48,590 [trainer.py] => Average Accuracy (CNN): 89.83
2023-09-13 22:00:48,594 [trainer.py] => All params: 171938305
2023-09-13 22:00:48,598 [trainer.py] => Trainable params: 340993
2023-09-13 22:00:48,607 [adam_lora.py] => Learning on 30-60
2023-09-13 22:02:11,266 [trainer.py] => No NME accuracy.
2023-09-13 22:02:11,267 [trainer.py] => CNN: {'total': 88.74, '00-09': 85.0, '10-19': 89.5, '20-29': 84.5, '30-39': 91.0, '40-49': 88.5, '50-59': 93.97, 'old': 86.33, 'new': 91.15}
2023-09-13 22:02:11,267 [trainer.py] => CNN top1 curve: [89.83, 88.74]
2023-09-13 22:02:11,267 [trainer.py] => CNN top5 curve: [99.17, 98.92]

2023-09-13 22:02:11,268 [trainer.py] => Average Accuracy (CNN): 89.285
2023-09-13 22:02:11,273 [trainer.py] => All params: 171984385
2023-09-13 22:02:11,276 [trainer.py] => Trainable params: 387073
2023-09-13 22:02:11,283 [adam_lora.py] => Learning on 60-90
2023-09-13 22:03:36,358 [trainer.py] => No NME accuracy.
2023-09-13 22:03:36,359 [trainer.py] => CNN: {'total': 86.82, '00-09': 83.0, '10-19': 85.0, '20-29': 82.0, '30-39': 89.0, '40-49': 86.5, '50-59': 92.96, '60-69': 93.0, '70-79': 88.0, '80-89': 81.91, 'old': 86.41, 'new': 87.65}
2023-09-13 22:03:36,359 [trainer.py] => CNN top1 curve: [89.83, 88.74, 86.82]
2023-09-13 22:03:36,359 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.72]

2023-09-13 22:03:36,360 [trainer.py] => Average Accuracy (CNN): 88.46333333333332
2023-09-13 22:03:36,362 [trainer.py] => All params: 172030465
2023-09-13 22:03:36,366 [trainer.py] => Trainable params: 433153
2023-09-13 22:03:36,374 [adam_lora.py] => Learning on 90-120
2023-09-13 22:05:10,787 [trainer.py] => No NME accuracy.
2023-09-13 22:05:10,788 [trainer.py] => CNN: {'total': 82.88, '00-09': 81.5, '10-19': 82.0, '20-29': 81.0, '30-39': 88.0, '40-49': 84.0, '50-59': 85.43, '60-69': 88.5, '70-79': 85.5, '80-89': 78.89, '90-99': 82.41, '100-109': 79.4, '110-119': 77.89, 'old': 83.87, 'new': 79.9}
2023-09-13 22:05:10,788 [trainer.py] => CNN top1 curve: [89.83, 88.74, 86.82, 82.88]
2023-09-13 22:05:10,789 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.72, 96.58]

2023-09-13 22:05:10,789 [trainer.py] => Average Accuracy (CNN): 87.0675
2023-09-13 22:05:10,792 [trainer.py] => All params: 172076545
2023-09-13 22:05:10,793 [trainer.py] => Trainable params: 479233
2023-09-13 22:05:10,798 [adam_lora.py] => Learning on 120-150
2023-09-13 22:06:48,907 [trainer.py] => No NME accuracy.
2023-09-13 22:06:48,909 [trainer.py] => CNN: {'total': 80.83, '00-09': 79.0, '10-19': 82.0, '20-29': 80.5, '30-39': 88.0, '40-49': 81.5, '50-59': 79.4, '60-69': 84.5, '70-79': 84.0, '80-89': 77.39, '90-99': 81.41, '100-109': 76.38, '110-119': 71.86, '120-129': 85.0, '130-139': 80.0, '140-149': 81.41, 'old': 80.5, 'new': 82.14}
2023-09-13 22:06:48,910 [trainer.py] => CNN top1 curve: [89.83, 88.74, 86.82, 82.88, 80.83]
2023-09-13 22:06:48,910 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.72, 96.58, 95.39]

2023-09-13 22:06:48,910 [trainer.py] => Average Accuracy (CNN): 85.82
2023-09-13 22:06:48,915 [trainer.py] => All params: 172122625
2023-09-13 22:06:48,918 [trainer.py] => Trainable params: 525313
2023-09-13 22:06:48,927 [adam_lora.py] => Learning on 150-180
2023-09-13 22:10:27,027 [trainer.py] => config: ./exps/adam_lora.json
2023-09-13 22:10:27,027 [trainer.py] => prefix:  
2023-09-13 22:10:27,028 [trainer.py] => dataset: omnibenchmark
2023-09-13 22:10:27,028 [trainer.py] => memory_size: 0
2023-09-13 22:10:27,028 [trainer.py] => memory_per_class: 0
2023-09-13 22:10:27,028 [trainer.py] => fixed_memory: False
2023-09-13 22:10:27,029 [trainer.py] => shuffle: True
2023-09-13 22:10:27,029 [trainer.py] => init_cls: 30
2023-09-13 22:10:27,029 [trainer.py] => increment: 30
2023-09-13 22:10:27,029 [trainer.py] => model_name: adam_lora
2023-09-13 22:10:27,030 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_lora
2023-09-13 22:10:27,030 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 22:10:27,030 [trainer.py] => visible_device: 2,3
2023-09-13 22:10:27,030 [trainer.py] => seed: 1993
2023-09-13 22:10:27,031 [trainer.py] => tuned_epoch: 20
2023-09-13 22:10:27,031 [trainer.py] => init_lr: 0.05
2023-09-13 22:10:27,031 [trainer.py] => batch_size: 96
2023-09-13 22:10:27,031 [trainer.py] => use_A: False
2023-09-13 22:10:27,032 [trainer.py] => weight_decay: 0.0005
2023-09-13 22:10:27,032 [trainer.py] => min_lr: 0
2023-09-13 22:10:27,032 [trainer.py] => ffn_num: 16
2023-09-13 22:10:27,032 [trainer.py] => optimizer: sgd
2023-09-13 22:10:27,033 [trainer.py] => vpt_type: shallow
2023-09-13 22:10:27,033 [trainer.py] => prompt_token_num: 5
2023-09-13 22:10:28,112 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 22:10:32,913 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 22:10:33,266 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 22:10:37,750 [trainer.py] => All params: 85946112
2023-09-13 22:10:37,754 [trainer.py] => Trainable params: 147456
2023-09-13 22:10:39,174 [adam_lora.py] => Learning on 0-30
2023-09-13 22:11:49,095 [adam_lora.py] => Task 0, Epoch 1/20 => Loss 2.585, Train_accy 48.00, Test_accy 84.50
2023-09-13 22:12:56,461 [adam_lora.py] => Task 0, Epoch 2/20 => Loss 0.817, Train_accy 75.69, Test_accy 85.17
2023-09-13 22:14:03,807 [adam_lora.py] => Task 0, Epoch 3/20 => Loss 0.660, Train_accy 79.10, Test_accy 90.33
2023-09-13 22:15:11,838 [adam_lora.py] => Task 0, Epoch 4/20 => Loss 0.612, Train_accy 80.45, Test_accy 91.33
2023-09-13 22:16:19,179 [adam_lora.py] => Task 0, Epoch 5/20 => Loss 0.583, Train_accy 81.65, Test_accy 92.00
2023-09-13 22:17:26,949 [adam_lora.py] => Task 0, Epoch 6/20 => Loss 0.567, Train_accy 81.66, Test_accy 93.00
2023-09-13 22:18:33,263 [adam_lora.py] => Task 0, Epoch 7/20 => Loss 0.536, Train_accy 82.33, Test_accy 92.67
2023-09-13 22:19:41,066 [adam_lora.py] => Task 0, Epoch 8/20 => Loss 0.522, Train_accy 83.29, Test_accy 91.33
2023-09-13 22:20:48,555 [adam_lora.py] => Task 0, Epoch 9/20 => Loss 0.511, Train_accy 83.39, Test_accy 92.83
2023-09-13 22:21:55,820 [adam_lora.py] => Task 0, Epoch 10/20 => Loss 0.506, Train_accy 83.95, Test_accy 93.17
2023-09-13 22:23:03,914 [adam_lora.py] => Task 0, Epoch 11/20 => Loss 0.486, Train_accy 84.65, Test_accy 92.83
2023-09-13 22:24:12,549 [adam_lora.py] => Task 0, Epoch 12/20 => Loss 0.481, Train_accy 84.78, Test_accy 92.50
2023-09-13 22:25:20,403 [adam_lora.py] => Task 0, Epoch 13/20 => Loss 0.457, Train_accy 85.62, Test_accy 93.67
2023-09-13 22:26:28,055 [adam_lora.py] => Task 0, Epoch 14/20 => Loss 0.474, Train_accy 85.12, Test_accy 93.33
2023-09-13 22:27:35,085 [adam_lora.py] => Task 0, Epoch 15/20 => Loss 0.464, Train_accy 85.05, Test_accy 93.33
2023-09-13 22:28:42,591 [adam_lora.py] => Task 0, Epoch 16/20 => Loss 0.446, Train_accy 85.65, Test_accy 93.00
2023-09-13 22:29:50,824 [adam_lora.py] => Task 0, Epoch 17/20 => Loss 0.462, Train_accy 85.60, Test_accy 93.17
2023-09-13 22:30:58,399 [adam_lora.py] => Task 0, Epoch 18/20 => Loss 0.459, Train_accy 85.65, Test_accy 93.50
2023-09-13 22:32:05,584 [adam_lora.py] => Task 0, Epoch 19/20 => Loss 0.454, Train_accy 85.75, Test_accy 93.33
2023-09-13 22:33:12,687 [adam_lora.py] => Task 0, Epoch 20/20 => Loss 0.465, Train_accy 85.17, Test_accy 93.33
2023-09-13 22:33:15,777 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 22:33:16,118 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 22:33:26,488 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 22:33:26,760 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 22:34:57,152 [trainer.py] => No NME accuracy.
2023-09-13 22:34:57,163 [trainer.py] => CNN: {'total': 90.0, '00-09': 91.0, '10-19': 91.0, '20-29': 88.0, 'old': 0, 'new': 90.0}
2023-09-13 22:34:57,171 [trainer.py] => CNN top1 curve: [90.0]
2023-09-13 22:34:57,176 [trainer.py] => CNN top5 curve: [99.17]

2023-09-13 22:34:57,177 [trainer.py] => Average Accuracy (CNN): 90.0
2023-09-13 22:34:57,180 [trainer.py] => All params: 171938305
2023-09-13 22:34:57,183 [trainer.py] => Trainable params: 340993
2023-09-13 22:34:57,193 [adam_lora.py] => Learning on 30-60
2023-09-13 22:36:19,388 [trainer.py] => No NME accuracy.
2023-09-13 22:36:19,389 [trainer.py] => CNN: {'total': 88.66, '00-09': 85.5, '10-19': 89.5, '20-29': 84.0, '30-39': 91.0, '40-49': 88.0, '50-59': 93.97, 'old': 86.33, 'new': 90.98}
2023-09-13 22:36:19,389 [trainer.py] => CNN top1 curve: [90.0, 88.66]
2023-09-13 22:36:19,389 [trainer.py] => CNN top5 curve: [99.17, 98.92]

2023-09-13 22:36:19,390 [trainer.py] => Average Accuracy (CNN): 89.33
2023-09-13 22:36:19,394 [trainer.py] => All params: 171984385
2023-09-13 22:36:19,397 [trainer.py] => Trainable params: 387073
2023-09-13 22:36:19,401 [adam_lora.py] => Learning on 60-90
2023-09-13 22:37:42,467 [trainer.py] => No NME accuracy.
2023-09-13 22:37:42,474 [trainer.py] => CNN: {'total': 86.87, '00-09': 83.5, '10-19': 85.0, '20-29': 82.0, '30-39': 89.0, '40-49': 86.0, '50-59': 92.46, '60-69': 93.5, '70-79': 88.5, '80-89': 81.91, 'old': 86.32, 'new': 87.98}
2023-09-13 22:37:42,474 [trainer.py] => CNN top1 curve: [90.0, 88.66, 86.87]
2023-09-13 22:37:42,474 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.94]

2023-09-13 22:37:42,475 [trainer.py] => Average Accuracy (CNN): 88.50999999999999
2023-09-13 22:37:42,479 [trainer.py] => All params: 172030465
2023-09-13 22:37:42,482 [trainer.py] => Trainable params: 433153
2023-09-13 22:37:42,489 [adam_lora.py] => Learning on 90-120
2023-09-13 22:39:15,812 [trainer.py] => No NME accuracy.
2023-09-13 22:39:15,812 [trainer.py] => CNN: {'total': 83.01, '00-09': 81.5, '10-19': 82.5, '20-29': 81.0, '30-39': 88.5, '40-49': 83.5, '50-59': 85.43, '60-69': 88.5, '70-79': 85.5, '80-89': 78.89, '90-99': 81.91, '100-109': 79.4, '110-119': 79.4, 'old': 83.93, 'new': 80.23}
2023-09-13 22:39:15,813 [trainer.py] => CNN top1 curve: [90.0, 88.66, 86.87, 83.01]
2023-09-13 22:39:15,813 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.94, 96.74]

2023-09-13 22:39:15,813 [trainer.py] => Average Accuracy (CNN): 87.13499999999999
2023-09-13 22:39:15,816 [trainer.py] => All params: 172076545
2023-09-13 22:39:15,819 [trainer.py] => Trainable params: 479233
2023-09-13 22:39:15,830 [adam_lora.py] => Learning on 120-150
2023-09-13 22:40:56,232 [trainer.py] => No NME accuracy.
2023-09-13 22:40:56,233 [trainer.py] => CNN: {'total': 81.0, '00-09': 79.0, '10-19': 82.0, '20-29': 80.0, '30-39': 88.5, '40-49': 81.0, '50-59': 79.4, '60-69': 85.5, '70-79': 84.0, '80-89': 77.39, '90-99': 81.41, '100-109': 76.38, '110-119': 73.87, '120-129': 85.0, '130-139': 80.5, '140-149': 80.9, 'old': 80.71, 'new': 82.14}
2023-09-13 22:40:56,233 [trainer.py] => CNN top1 curve: [90.0, 88.66, 86.87, 83.01, 81.0]
2023-09-13 22:40:56,234 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.94, 96.74, 95.66]

2023-09-13 22:40:56,234 [trainer.py] => Average Accuracy (CNN): 85.90799999999999
2023-09-13 22:40:56,238 [trainer.py] => All params: 172122625
2023-09-13 22:40:56,241 [trainer.py] => Trainable params: 525313
2023-09-13 22:40:56,253 [adam_lora.py] => Learning on 150-180
2023-09-13 22:42:39,738 [trainer.py] => No NME accuracy.
2023-09-13 22:42:39,740 [trainer.py] => CNN: {'total': 78.15, '00-09': 76.5, '10-19': 79.5, '20-29': 76.5, '30-39': 86.0, '40-49': 78.0, '50-59': 77.89, '60-69': 84.0, '70-79': 81.0, '80-89': 74.87, '90-99': 80.9, '100-109': 73.37, '110-119': 71.86, '120-129': 83.5, '130-139': 80.0, '140-149': 79.9, '150-159': 78.89, '160-169': 73.87, '170-179': 70.0, 'old': 78.92, 'new': 74.25}
2023-09-13 22:42:39,740 [trainer.py] => CNN top1 curve: [90.0, 88.66, 86.87, 83.01, 81.0, 78.15]
2023-09-13 22:42:39,740 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.94, 96.74, 95.66, 94.32]

2023-09-13 22:42:39,741 [trainer.py] => Average Accuracy (CNN): 84.615
2023-09-13 22:42:39,744 [trainer.py] => All params: 172168705
2023-09-13 22:42:39,748 [trainer.py] => Trainable params: 571393
2023-09-13 22:42:39,765 [adam_lora.py] => Learning on 180-210
2023-09-13 22:44:34,309 [trainer.py] => No NME accuracy.
2023-09-13 22:44:34,311 [trainer.py] => CNN: {'total': 77.3, '00-09': 76.0, '10-19': 79.5, '20-29': 76.0, '30-39': 86.0, '40-49': 76.5, '50-59': 77.39, '60-69': 82.5, '70-79': 80.5, '80-89': 73.87, '90-99': 79.4, '100-109': 72.86, '110-119': 68.84, '120-129': 83.0, '130-139': 79.0, '140-149': 79.4, '150-159': 75.88, '160-169': 72.36, '170-179': 69.5, '180-189': 75.25, '190-199': 84.5, '200-209': 74.87, 'old': 77.14, 'new': 78.22}
2023-09-13 22:44:34,311 [trainer.py] => CNN top1 curve: [90.0, 88.66, 86.87, 83.01, 81.0, 78.15, 77.3]
2023-09-13 22:44:34,311 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.94, 96.74, 95.66, 94.32, 94.03]

2023-09-13 22:44:34,312 [trainer.py] => Average Accuracy (CNN): 83.56999999999998
2023-09-13 22:44:34,314 [trainer.py] => All params: 172214785
2023-09-13 22:44:34,316 [trainer.py] => Trainable params: 617473
2023-09-13 22:44:34,324 [adam_lora.py] => Learning on 210-240
2023-09-13 22:46:29,548 [trainer.py] => No NME accuracy.
2023-09-13 22:46:29,549 [trainer.py] => CNN: {'total': 75.73, '00-09': 74.5, '10-19': 77.5, '20-29': 75.5, '30-39': 86.0, '40-49': 75.5, '50-59': 77.39, '60-69': 82.0, '70-79': 80.5, '80-89': 72.86, '90-99': 78.89, '100-109': 65.33, '110-119': 68.84, '120-129': 79.5, '130-139': 79.0, '140-149': 78.39, '150-159': 75.38, '160-169': 72.36, '170-179': 67.5, '180-189': 75.25, '190-199': 84.0, '200-209': 71.36, '210-219': 49.5, '220-229': 89.95, '230-239': 80.5, 'old': 76.08, 'new': 73.29}
2023-09-13 22:46:29,549 [trainer.py] => CNN top1 curve: [90.0, 88.66, 86.87, 83.01, 81.0, 78.15, 77.3, 75.73]
2023-09-13 22:46:29,549 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.94, 96.74, 95.66, 94.32, 94.03, 92.73]

2023-09-13 22:46:29,550 [trainer.py] => Average Accuracy (CNN): 82.58999999999999
2023-09-13 22:46:29,553 [trainer.py] => All params: 172260865
2023-09-13 22:46:29,556 [trainer.py] => Trainable params: 663553
2023-09-13 22:46:29,568 [adam_lora.py] => Learning on 240-270
2023-09-13 22:48:33,005 [trainer.py] => No NME accuracy.
2023-09-13 22:48:33,009 [trainer.py] => CNN: {'total': 74.74, '00-09': 74.5, '10-19': 77.0, '20-29': 75.5, '30-39': 85.0, '40-49': 75.5, '50-59': 76.88, '60-69': 80.5, '70-79': 80.0, '80-89': 71.86, '90-99': 77.39, '100-109': 63.82, '110-119': 65.83, '120-129': 74.0, '130-139': 78.0, '140-149': 74.37, '150-159': 72.86, '160-169': 72.36, '170-179': 67.5, '180-189': 75.25, '190-199': 82.0, '200-209': 67.84, '210-219': 49.5, '220-229': 89.95, '230-239': 80.0, '240-249': 70.0, '250-259': 77.89, '260-269': 82.5, 'old': 74.48, 'new': 76.79}
2023-09-13 22:48:33,010 [trainer.py] => CNN top1 curve: [90.0, 88.66, 86.87, 83.01, 81.0, 78.15, 77.3, 75.73, 74.74]
2023-09-13 22:48:33,010 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.94, 96.74, 95.66, 94.32, 94.03, 92.73, 92.85]

2023-09-13 22:48:33,010 [trainer.py] => Average Accuracy (CNN): 81.71777777777777
2023-09-13 22:48:33,013 [trainer.py] => All params: 172306945
2023-09-13 22:48:33,014 [trainer.py] => Trainable params: 709633
2023-09-13 22:48:33,024 [adam_lora.py] => Learning on 270-300
2023-09-13 22:50:37,320 [trainer.py] => No NME accuracy.
2023-09-13 22:50:37,321 [trainer.py] => CNN: {'total': 74.55, '00-09': 74.0, '10-19': 77.0, '20-29': 75.0, '30-39': 85.0, '40-49': 72.5, '50-59': 76.88, '60-69': 79.5, '70-79': 80.0, '80-89': 71.86, '90-99': 73.87, '100-109': 63.32, '110-119': 64.82, '120-129': 74.0, '130-139': 77.5, '140-149': 74.37, '150-159': 72.86, '160-169': 70.85, '170-179': 66.0, '180-189': 74.75, '190-199': 81.5, '200-209': 66.83, '210-219': 48.0, '220-229': 89.95, '230-239': 78.5, '240-249': 68.0, '250-259': 77.39, '260-269': 81.5, '270-279': 79.4, '280-289': 82.0, '290-299': 79.4, 'old': 73.92, 'new': 80.27}
2023-09-13 22:50:37,321 [trainer.py] => CNN top1 curve: [90.0, 88.66, 86.87, 83.01, 81.0, 78.15, 77.3, 75.73, 74.74, 74.55]
2023-09-13 22:50:37,322 [trainer.py] => CNN top5 curve: [99.17, 98.92, 97.94, 96.74, 95.66, 94.32, 94.03, 92.73, 92.85, 92.75]

2023-09-13 22:50:37,322 [trainer.py] => Average Accuracy (CNN): 81.00099999999999
