2023-09-13 18:28:47,390 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:28:47,391 [trainer.py] => prefix:  
2023-09-13 18:28:47,391 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:28:47,391 [trainer.py] => memory_size: 0
2023-09-13 18:28:47,391 [trainer.py] => memory_per_class: 0
2023-09-13 18:28:47,391 [trainer.py] => fixed_memory: False
2023-09-13 18:28:47,392 [trainer.py] => shuffle: True
2023-09-13 18:28:47,392 [trainer.py] => init_cls: 30
2023-09-13 18:28:47,392 [trainer.py] => increment: 30
2023-09-13 18:28:47,393 [trainer.py] => model_name: adam_adapter
2023-09-13 18:28:47,393 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:28:47,393 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:28:47,393 [trainer.py] => visible_device: 2,3
2023-09-13 18:28:47,393 [trainer.py] => seed: 1993
2023-09-13 18:28:47,394 [trainer.py] => tuned_epoch: 20
2023-09-13 18:28:47,394 [trainer.py] => init_lr: 0.02
2023-09-13 18:28:47,394 [trainer.py] => batch_size: 96
2023-09-13 18:28:47,396 [trainer.py] => use_A: False
2023-09-13 18:28:47,396 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:28:47,396 [trainer.py] => min_lr: 0
2023-09-13 18:28:47,396 [trainer.py] => ffn_num: 16
2023-09-13 18:28:47,397 [trainer.py] => optimizer: sgd
2023-09-13 18:28:47,397 [trainer.py] => vpt_type: shallow
2023-09-13 18:28:47,397 [trainer.py] => prompt_token_num: 5
2023-09-13 18:28:48,576 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:28:52,374 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:28:52,713 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:28:57,422 [trainer.py] => All params: 86102976
2023-09-13 18:28:57,426 [trainer.py] => Trainable params: 304320
2023-09-13 18:28:58,692 [adam_adapter.py] => Learning on 0-30
2023-09-13 18:30:09,791 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.227, Train_accy 33.55, Test_accy 75.50
2023-09-13 18:32:28,856 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:32:28,857 [trainer.py] => prefix:  
2023-09-13 18:32:28,857 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:32:28,857 [trainer.py] => memory_size: 0
2023-09-13 18:32:28,858 [trainer.py] => memory_per_class: 0
2023-09-13 18:32:28,858 [trainer.py] => fixed_memory: False
2023-09-13 18:32:28,858 [trainer.py] => shuffle: True
2023-09-13 18:32:28,858 [trainer.py] => init_cls: 30
2023-09-13 18:32:28,859 [trainer.py] => increment: 30
2023-09-13 18:32:28,859 [trainer.py] => model_name: adam_adapter
2023-09-13 18:32:28,859 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:32:28,860 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:32:28,860 [trainer.py] => visible_device: 2,3
2023-09-13 18:32:28,860 [trainer.py] => seed: 1993
2023-09-13 18:32:28,860 [trainer.py] => tuned_epoch: 20
2023-09-13 18:32:28,861 [trainer.py] => init_lr: 0.02
2023-09-13 18:32:28,861 [trainer.py] => batch_size: 96
2023-09-13 18:32:28,861 [trainer.py] => use_A: False
2023-09-13 18:32:28,861 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:32:28,862 [trainer.py] => min_lr: 0
2023-09-13 18:32:28,862 [trainer.py] => ffn_num: 16
2023-09-13 18:32:28,862 [trainer.py] => optimizer: sgd
2023-09-13 18:32:28,864 [trainer.py] => vpt_type: shallow
2023-09-13 18:32:28,865 [trainer.py] => prompt_token_num: 5
2023-09-13 18:32:30,043 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:33:41,490 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:33:41,490 [trainer.py] => prefix:  
2023-09-13 18:33:41,491 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:33:41,491 [trainer.py] => memory_size: 0
2023-09-13 18:33:41,491 [trainer.py] => memory_per_class: 0
2023-09-13 18:33:41,492 [trainer.py] => fixed_memory: False
2023-09-13 18:33:41,492 [trainer.py] => shuffle: True
2023-09-13 18:33:41,492 [trainer.py] => init_cls: 30
2023-09-13 18:33:41,492 [trainer.py] => increment: 30
2023-09-13 18:33:41,493 [trainer.py] => model_name: adam_adapter
2023-09-13 18:33:41,494 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:33:41,494 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:33:41,494 [trainer.py] => visible_device: 2,3
2023-09-13 18:33:41,495 [trainer.py] => seed: 1993
2023-09-13 18:33:41,495 [trainer.py] => tuned_epoch: 20
2023-09-13 18:33:41,495 [trainer.py] => init_lr: 0.02
2023-09-13 18:33:41,495 [trainer.py] => batch_size: 96
2023-09-13 18:33:41,496 [trainer.py] => use_A: False
2023-09-13 18:33:41,496 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:33:41,496 [trainer.py] => min_lr: 0
2023-09-13 18:33:41,497 [trainer.py] => ffn_num: 16
2023-09-13 18:33:41,497 [trainer.py] => optimizer: sgd
2023-09-13 18:33:41,497 [trainer.py] => vpt_type: shallow
2023-09-13 18:33:41,498 [trainer.py] => prompt_token_num: 5
2023-09-13 18:33:42,799 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:33:59,892 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:33:59,892 [trainer.py] => prefix:  
2023-09-13 18:33:59,893 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:33:59,893 [trainer.py] => memory_size: 0
2023-09-13 18:33:59,893 [trainer.py] => memory_per_class: 0
2023-09-13 18:33:59,894 [trainer.py] => fixed_memory: False
2023-09-13 18:33:59,894 [trainer.py] => shuffle: True
2023-09-13 18:33:59,894 [trainer.py] => init_cls: 30
2023-09-13 18:33:59,895 [trainer.py] => increment: 30
2023-09-13 18:33:59,895 [trainer.py] => model_name: adam_adapter
2023-09-13 18:33:59,895 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:33:59,895 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:33:59,896 [trainer.py] => visible_device: 2,3
2023-09-13 18:33:59,896 [trainer.py] => seed: 1993
2023-09-13 18:33:59,896 [trainer.py] => tuned_epoch: 20
2023-09-13 18:33:59,896 [trainer.py] => init_lr: 0.02
2023-09-13 18:33:59,897 [trainer.py] => batch_size: 96
2023-09-13 18:33:59,897 [trainer.py] => use_A: False
2023-09-13 18:33:59,897 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:33:59,897 [trainer.py] => min_lr: 0
2023-09-13 18:33:59,897 [trainer.py] => ffn_num: 16
2023-09-13 18:33:59,898 [trainer.py] => optimizer: sgd
2023-09-13 18:33:59,899 [trainer.py] => vpt_type: shallow
2023-09-13 18:33:59,899 [trainer.py] => prompt_token_num: 5
2023-09-13 18:34:01,206 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:34:07,166 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:34:07,633 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:34:11,641 [trainer.py] => All params: 86388480
2023-09-13 18:34:11,643 [trainer.py] => Trainable params: 589824
2023-09-13 18:34:12,894 [adam_adapter.py] => Learning on 0-30
2023-09-13 18:36:27,858 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:36:27,859 [trainer.py] => prefix:  
2023-09-13 18:36:27,859 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:36:27,859 [trainer.py] => memory_size: 0
2023-09-13 18:36:27,859 [trainer.py] => memory_per_class: 0
2023-09-13 18:36:27,860 [trainer.py] => fixed_memory: False
2023-09-13 18:36:27,860 [trainer.py] => shuffle: True
2023-09-13 18:36:27,860 [trainer.py] => init_cls: 30
2023-09-13 18:36:27,860 [trainer.py] => increment: 30
2023-09-13 18:36:27,861 [trainer.py] => model_name: adam_adapter
2023-09-13 18:36:27,862 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:36:27,862 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:36:27,862 [trainer.py] => visible_device: 2,3
2023-09-13 18:36:27,865 [trainer.py] => seed: 1993
2023-09-13 18:36:27,865 [trainer.py] => tuned_epoch: 20
2023-09-13 18:36:27,865 [trainer.py] => init_lr: 0.02
2023-09-13 18:36:27,865 [trainer.py] => batch_size: 96
2023-09-13 18:36:27,866 [trainer.py] => use_A: False
2023-09-13 18:36:27,866 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:36:27,866 [trainer.py] => min_lr: 0
2023-09-13 18:36:27,866 [trainer.py] => ffn_num: 16
2023-09-13 18:36:27,867 [trainer.py] => optimizer: sgd
2023-09-13 18:36:27,867 [trainer.py] => vpt_type: shallow
2023-09-13 18:36:27,867 [trainer.py] => prompt_token_num: 5
2023-09-13 18:36:29,084 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:36:34,785 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:36:35,112 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:36:39,593 [trainer.py] => All params: 86388480
2023-09-13 18:36:39,603 [trainer.py] => Trainable params: 589824
2023-09-13 18:36:41,039 [adam_adapter.py] => Learning on 0-30
2023-09-13 18:37:51,876 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.399, Train_accy 4.06, Test_accy 3.83
2023-09-13 18:38:57,770 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 3.393, Train_accy 4.47, Test_accy 4.83
2023-09-13 18:40:05,164 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 3.392, Train_accy 4.54, Test_accy 5.17
2023-09-13 18:41:11,778 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 3.392, Train_accy 4.62, Test_accy 5.33
2023-09-13 18:43:24,859 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:43:24,859 [trainer.py] => prefix:  
2023-09-13 18:43:24,859 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:43:24,859 [trainer.py] => memory_size: 0
2023-09-13 18:43:24,860 [trainer.py] => memory_per_class: 0
2023-09-13 18:43:24,860 [trainer.py] => fixed_memory: False
2023-09-13 18:43:24,860 [trainer.py] => shuffle: True
2023-09-13 18:43:24,860 [trainer.py] => init_cls: 30
2023-09-13 18:43:24,861 [trainer.py] => increment: 30
2023-09-13 18:43:24,861 [trainer.py] => model_name: adam_adapter
2023-09-13 18:43:24,861 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:43:24,861 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:43:24,862 [trainer.py] => visible_device: 2,3
2023-09-13 18:43:24,862 [trainer.py] => seed: 1993
2023-09-13 18:43:24,862 [trainer.py] => tuned_epoch: 20
2023-09-13 18:43:24,862 [trainer.py] => init_lr: 0.02
2023-09-13 18:43:24,862 [trainer.py] => batch_size: 96
2023-09-13 18:43:24,865 [trainer.py] => use_A: False
2023-09-13 18:43:24,865 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:43:24,865 [trainer.py] => min_lr: 0
2023-09-13 18:43:24,865 [trainer.py] => ffn_num: 16
2023-09-13 18:43:24,866 [trainer.py] => optimizer: sgd
2023-09-13 18:43:24,866 [trainer.py] => vpt_type: shallow
2023-09-13 18:43:24,866 [trainer.py] => prompt_token_num: 5
2023-09-13 18:43:26,029 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:43:42,957 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:43:42,958 [trainer.py] => prefix:  
2023-09-13 18:43:42,958 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:43:42,958 [trainer.py] => memory_size: 0
2023-09-13 18:43:42,959 [trainer.py] => memory_per_class: 0
2023-09-13 18:43:42,959 [trainer.py] => fixed_memory: False
2023-09-13 18:43:42,959 [trainer.py] => shuffle: True
2023-09-13 18:43:42,960 [trainer.py] => init_cls: 30
2023-09-13 18:43:42,960 [trainer.py] => increment: 30
2023-09-13 18:43:42,960 [trainer.py] => model_name: adam_adapter
2023-09-13 18:43:42,960 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:43:42,962 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:43:42,962 [trainer.py] => visible_device: 2,3
2023-09-13 18:43:42,963 [trainer.py] => seed: 1993
2023-09-13 18:43:42,965 [trainer.py] => tuned_epoch: 20
2023-09-13 18:43:42,965 [trainer.py] => init_lr: 0.02
2023-09-13 18:43:42,965 [trainer.py] => batch_size: 96
2023-09-13 18:43:42,966 [trainer.py] => use_A: False
2023-09-13 18:43:42,966 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:43:42,966 [trainer.py] => min_lr: 0
2023-09-13 18:43:42,967 [trainer.py] => ffn_num: 16
2023-09-13 18:43:42,967 [trainer.py] => optimizer: sgd
2023-09-13 18:43:42,967 [trainer.py] => vpt_type: shallow
2023-09-13 18:43:42,967 [trainer.py] => prompt_token_num: 5
2023-09-13 18:43:44,386 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:43:58,848 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:43:58,848 [trainer.py] => prefix:  
2023-09-13 18:43:58,848 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:43:58,849 [trainer.py] => memory_size: 0
2023-09-13 18:43:58,849 [trainer.py] => memory_per_class: 0
2023-09-13 18:43:58,849 [trainer.py] => fixed_memory: False
2023-09-13 18:43:58,850 [trainer.py] => shuffle: True
2023-09-13 18:43:58,850 [trainer.py] => init_cls: 30
2023-09-13 18:43:58,850 [trainer.py] => increment: 30
2023-09-13 18:43:58,850 [trainer.py] => model_name: adam_adapter
2023-09-13 18:43:58,851 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:43:58,851 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:43:58,851 [trainer.py] => visible_device: 2,3
2023-09-13 18:43:58,852 [trainer.py] => seed: 1993
2023-09-13 18:43:58,852 [trainer.py] => tuned_epoch: 20
2023-09-13 18:43:58,852 [trainer.py] => init_lr: 0.02
2023-09-13 18:43:58,852 [trainer.py] => batch_size: 96
2023-09-13 18:43:58,853 [trainer.py] => use_A: False
2023-09-13 18:43:58,853 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:43:58,853 [trainer.py] => min_lr: 0
2023-09-13 18:43:58,854 [trainer.py] => ffn_num: 16
2023-09-13 18:43:58,854 [trainer.py] => optimizer: sgd
2023-09-13 18:43:58,854 [trainer.py] => vpt_type: shallow
2023-09-13 18:43:58,855 [trainer.py] => prompt_token_num: 5
2023-09-13 18:44:00,040 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:44:05,378 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:44:05,816 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:44:10,584 [trainer.py] => All params: 86102976
2023-09-13 18:44:10,586 [trainer.py] => Trainable params: 304320
2023-09-13 18:44:11,849 [adam_adapter.py] => Learning on 0-30
2023-09-13 18:45:19,908 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.218, Train_accy 34.44, Test_accy 76.50
2023-09-13 18:46:25,973 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:46:25,973 [trainer.py] => prefix:  
2023-09-13 18:46:25,973 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:46:25,973 [trainer.py] => memory_size: 0
2023-09-13 18:46:25,974 [trainer.py] => memory_per_class: 0
2023-09-13 18:46:25,974 [trainer.py] => fixed_memory: False
2023-09-13 18:46:25,975 [trainer.py] => shuffle: True
2023-09-13 18:46:25,975 [trainer.py] => init_cls: 30
2023-09-13 18:46:25,975 [trainer.py] => increment: 30
2023-09-13 18:46:25,976 [trainer.py] => model_name: adam_adapter
2023-09-13 18:46:25,976 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:46:25,976 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:46:25,977 [trainer.py] => visible_device: 2,3
2023-09-13 18:46:25,977 [trainer.py] => seed: 1993
2023-09-13 18:46:25,977 [trainer.py] => tuned_epoch: 20
2023-09-13 18:46:25,977 [trainer.py] => init_lr: 0.02
2023-09-13 18:46:25,978 [trainer.py] => batch_size: 96
2023-09-13 18:46:25,978 [trainer.py] => use_A: False
2023-09-13 18:46:25,978 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:46:25,978 [trainer.py] => min_lr: 0
2023-09-13 18:46:25,979 [trainer.py] => ffn_num: 16
2023-09-13 18:46:25,979 [trainer.py] => optimizer: sgd
2023-09-13 18:46:25,979 [trainer.py] => vpt_type: shallow
2023-09-13 18:46:25,980 [trainer.py] => prompt_token_num: 5
2023-09-13 18:46:27,748 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:46:33,476 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:46:34,055 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:46:38,240 [trainer.py] => All params: 85798656
2023-09-13 18:46:38,241 [trainer.py] => Trainable params: 0
2023-09-13 18:46:39,543 [adam_adapter.py] => Learning on 0-30
2023-09-13 18:47:30,362 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.399, Train_accy 3.92, Test_accy 4.17
2023-09-13 18:48:16,915 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 3.394, Train_accy 4.34, Test_accy 5.33
2023-09-13 18:49:02,247 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 3.393, Train_accy 4.28, Test_accy 5.50
2023-09-13 18:49:47,325 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 3.391, Train_accy 4.29, Test_accy 5.33
2023-09-13 18:50:35,020 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 3.390, Train_accy 4.44, Test_accy 5.00
2023-09-13 18:51:21,473 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 3.389, Train_accy 4.44, Test_accy 5.00
2023-09-13 18:52:07,402 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 3.388, Train_accy 4.54, Test_accy 5.17
2023-09-13 18:52:53,836 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 3.385, Train_accy 5.00, Test_accy 5.17
2023-09-13 18:54:04,551 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-13 18:54:04,551 [trainer.py] => prefix:  
2023-09-13 18:54:04,552 [trainer.py] => dataset: omnibenchmark
2023-09-13 18:54:04,552 [trainer.py] => memory_size: 0
2023-09-13 18:54:04,552 [trainer.py] => memory_per_class: 0
2023-09-13 18:54:04,552 [trainer.py] => fixed_memory: False
2023-09-13 18:54:04,553 [trainer.py] => shuffle: True
2023-09-13 18:54:04,553 [trainer.py] => init_cls: 30
2023-09-13 18:54:04,553 [trainer.py] => increment: 30
2023-09-13 18:54:04,554 [trainer.py] => model_name: adam_adapter
2023-09-13 18:54:04,554 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-13 18:54:04,554 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-13 18:54:04,555 [trainer.py] => visible_device: 2,3
2023-09-13 18:54:04,556 [trainer.py] => seed: 1993
2023-09-13 18:54:04,556 [trainer.py] => tuned_epoch: 20
2023-09-13 18:54:04,556 [trainer.py] => init_lr: 0.02
2023-09-13 18:54:04,557 [trainer.py] => batch_size: 96
2023-09-13 18:54:04,557 [trainer.py] => use_A: False
2023-09-13 18:54:04,557 [trainer.py] => weight_decay: 0.0005
2023-09-13 18:54:04,558 [trainer.py] => min_lr: 0
2023-09-13 18:54:04,558 [trainer.py] => ffn_num: 16
2023-09-13 18:54:04,558 [trainer.py] => optimizer: sgd
2023-09-13 18:54:04,558 [trainer.py] => vpt_type: shallow
2023-09-13 18:54:04,559 [trainer.py] => prompt_token_num: 5
2023-09-13 18:54:05,823 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-13 18:54:10,446 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-13 18:54:10,785 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-13 18:54:14,110 [trainer.py] => All params: 85798656
2023-09-13 18:54:14,112 [trainer.py] => Trainable params: 0
2023-09-13 18:54:15,361 [adam_adapter.py] => Learning on 0-30
2023-09-13 18:55:05,174 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.202, Train_accy 33.94, Test_accy 76.00
