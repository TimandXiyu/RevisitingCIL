2023-09-07 19:39:54,877 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-07 19:39:54,877 [trainer.py] => prefix:  
2023-09-07 19:39:54,877 [trainer.py] => dataset: omnibenchmark
2023-09-07 19:39:54,877 [trainer.py] => memory_size: 0
2023-09-07 19:39:54,877 [trainer.py] => memory_per_class: 0
2023-09-07 19:39:54,877 [trainer.py] => fixed_memory: False
2023-09-07 19:39:54,877 [trainer.py] => shuffle: True
2023-09-07 19:39:54,877 [trainer.py] => init_cls: 30
2023-09-07 19:39:54,877 [trainer.py] => increment: 30
2023-09-07 19:39:54,877 [trainer.py] => model_name: adam_adapter
2023-09-07 19:39:54,877 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-07 19:39:54,878 [trainer.py] => device: [device(type='cuda', index=0)]
2023-09-07 19:39:54,878 [trainer.py] => seed: 1993
2023-09-07 19:39:54,878 [trainer.py] => tuned_epoch: 20
2023-09-07 19:39:54,878 [trainer.py] => init_lr: 0.01
2023-09-07 19:39:54,878 [trainer.py] => batch_size: 48
2023-09-07 19:39:54,878 [trainer.py] => weight_decay: 0.0005
2023-09-07 19:39:54,878 [trainer.py] => min_lr: 0
2023-09-07 19:39:54,878 [trainer.py] => ffn_num: 64
2023-09-07 19:39:54,878 [trainer.py] => optimizer: sgd
2023-09-07 19:39:54,878 [trainer.py] => vpt_type: shallow
2023-09-07 19:39:54,878 [trainer.py] => prompt_token_num: 5
2023-09-07 19:39:55,225 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-07 19:40:19,684 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-07 19:40:19,684 [trainer.py] => prefix:  
2023-09-07 19:40:19,684 [trainer.py] => dataset: omnibenchmark
2023-09-07 19:40:19,684 [trainer.py] => memory_size: 0
2023-09-07 19:40:19,684 [trainer.py] => memory_per_class: 0
2023-09-07 19:40:19,684 [trainer.py] => fixed_memory: False
2023-09-07 19:40:19,685 [trainer.py] => shuffle: True
2023-09-07 19:40:19,685 [trainer.py] => init_cls: 30
2023-09-07 19:40:19,685 [trainer.py] => increment: 30
2023-09-07 19:40:19,685 [trainer.py] => model_name: adam_adapter
2023-09-07 19:40:19,685 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-07 19:40:19,685 [trainer.py] => device: [device(type='cuda', index=0)]
2023-09-07 19:40:19,685 [trainer.py] => seed: 1993
2023-09-07 19:40:19,685 [trainer.py] => tuned_epoch: 20
2023-09-07 19:40:19,685 [trainer.py] => init_lr: 0.01
2023-09-07 19:40:19,685 [trainer.py] => batch_size: 48
2023-09-07 19:40:19,685 [trainer.py] => weight_decay: 0.0005
2023-09-07 19:40:19,685 [trainer.py] => min_lr: 0
2023-09-07 19:40:19,685 [trainer.py] => ffn_num: 64
2023-09-07 19:40:19,685 [trainer.py] => optimizer: sgd
2023-09-07 19:40:19,685 [trainer.py] => vpt_type: shallow
2023-09-07 19:40:19,685 [trainer.py] => prompt_token_num: 5
2023-09-07 19:40:20,048 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-07 19:40:22,672 [trainer.py] => All params: 86988288
2023-09-07 19:40:22,673 [trainer.py] => Trainable params: 1189632
2023-09-07 19:40:24,384 [adam_adapter.py] => Learning on 0-30
2023-09-07 19:41:17,940 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-07 19:41:17,941 [trainer.py] => prefix:  
2023-09-07 19:41:17,941 [trainer.py] => dataset: omnibenchmark
2023-09-07 19:41:17,941 [trainer.py] => memory_size: 0
2023-09-07 19:41:17,941 [trainer.py] => memory_per_class: 0
2023-09-07 19:41:17,941 [trainer.py] => fixed_memory: False
2023-09-07 19:41:17,941 [trainer.py] => shuffle: True
2023-09-07 19:41:17,941 [trainer.py] => init_cls: 30
2023-09-07 19:41:17,941 [trainer.py] => increment: 30
2023-09-07 19:41:17,941 [trainer.py] => model_name: adam_adapter
2023-09-07 19:41:17,941 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-07 19:41:17,941 [trainer.py] => device: [device(type='cuda', index=0)]
2023-09-07 19:41:17,941 [trainer.py] => seed: 1993
2023-09-07 19:41:17,941 [trainer.py] => tuned_epoch: 20
2023-09-07 19:41:17,941 [trainer.py] => init_lr: 0.01
2023-09-07 19:41:17,942 [trainer.py] => batch_size: 48
2023-09-07 19:41:17,942 [trainer.py] => weight_decay: 0.0005
2023-09-07 19:41:17,942 [trainer.py] => min_lr: 0
2023-09-07 19:41:17,942 [trainer.py] => ffn_num: 64
2023-09-07 19:41:17,942 [trainer.py] => optimizer: sgd
2023-09-07 19:41:17,942 [trainer.py] => vpt_type: shallow
2023-09-07 19:41:17,942 [trainer.py] => prompt_token_num: 5
2023-09-07 19:41:18,756 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-07 19:41:21,619 [trainer.py] => All params: 86988288
2023-09-07 19:41:21,622 [trainer.py] => Trainable params: 1189632
2023-09-07 19:41:27,390 [adam_adapter.py] => Learning on 0-30
2023-09-07 19:42:38,048 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-07 19:42:38,048 [trainer.py] => prefix:  
2023-09-07 19:42:38,048 [trainer.py] => dataset: omnibenchmark
2023-09-07 19:42:38,048 [trainer.py] => memory_size: 0
2023-09-07 19:42:38,048 [trainer.py] => memory_per_class: 0
2023-09-07 19:42:38,048 [trainer.py] => fixed_memory: False
2023-09-07 19:42:38,048 [trainer.py] => shuffle: True
2023-09-07 19:42:38,048 [trainer.py] => init_cls: 30
2023-09-07 19:42:38,048 [trainer.py] => increment: 30
2023-09-07 19:42:38,048 [trainer.py] => model_name: adam_adapter
2023-09-07 19:42:38,048 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-07 19:42:38,048 [trainer.py] => device: [device(type='cuda', index=0)]
2023-09-07 19:42:38,048 [trainer.py] => seed: 1993
2023-09-07 19:42:38,048 [trainer.py] => tuned_epoch: 20
2023-09-07 19:42:38,048 [trainer.py] => init_lr: 0.01
2023-09-07 19:42:38,048 [trainer.py] => batch_size: 48
2023-09-07 19:42:38,049 [trainer.py] => weight_decay: 0.0005
2023-09-07 19:42:38,049 [trainer.py] => min_lr: 0
2023-09-07 19:42:38,049 [trainer.py] => ffn_num: 64
2023-09-07 19:42:38,049 [trainer.py] => optimizer: sgd
2023-09-07 19:42:38,049 [trainer.py] => vpt_type: shallow
2023-09-07 19:42:38,049 [trainer.py] => prompt_token_num: 5
2023-09-07 19:42:38,423 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-07 19:42:40,910 [trainer.py] => All params: 86988288
2023-09-07 19:42:40,911 [trainer.py] => Trainable params: 1189632
2023-09-07 19:42:42,671 [adam_adapter.py] => Learning on 0-30
2023-09-09 06:31:00,879 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 06:31:00,880 [trainer.py] => prefix:  
2023-09-09 06:31:00,880 [trainer.py] => dataset: omnibenchmark
2023-09-09 06:31:00,880 [trainer.py] => memory_size: 0
2023-09-09 06:31:00,880 [trainer.py] => memory_per_class: 0
2023-09-09 06:31:00,880 [trainer.py] => fixed_memory: False
2023-09-09 06:31:00,880 [trainer.py] => shuffle: True
2023-09-09 06:31:00,880 [trainer.py] => init_cls: 30
2023-09-09 06:31:00,880 [trainer.py] => increment: 30
2023-09-09 06:31:00,880 [trainer.py] => model_name: adam_adapter
2023-09-09 06:31:00,880 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 06:31:00,880 [trainer.py] => device: [device(type='cuda', index=0)]
2023-09-09 06:31:00,880 [trainer.py] => seed: 1993
2023-09-09 06:31:00,880 [trainer.py] => tuned_epoch: 20
2023-09-09 06:31:00,880 [trainer.py] => init_lr: 0.01
2023-09-09 06:31:00,880 [trainer.py] => batch_size: 48
2023-09-09 06:31:00,880 [trainer.py] => weight_decay: 0.0005
2023-09-09 06:31:00,880 [trainer.py] => min_lr: 0
2023-09-09 06:31:00,880 [trainer.py] => ffn_num: 64
2023-09-09 06:31:00,880 [trainer.py] => optimizer: sgd
2023-09-09 06:31:00,881 [trainer.py] => vpt_type: shallow
2023-09-09 06:31:00,881 [trainer.py] => prompt_token_num: 5
2023-09-09 06:31:01,103 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 06:33:13,297 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 06:33:13,297 [trainer.py] => prefix:  
2023-09-09 06:33:13,297 [trainer.py] => dataset: omnibenchmark
2023-09-09 06:33:13,297 [trainer.py] => memory_size: 0
2023-09-09 06:33:13,297 [trainer.py] => memory_per_class: 0
2023-09-09 06:33:13,297 [trainer.py] => fixed_memory: False
2023-09-09 06:33:13,297 [trainer.py] => shuffle: True
2023-09-09 06:33:13,297 [trainer.py] => init_cls: 30
2023-09-09 06:33:13,297 [trainer.py] => increment: 30
2023-09-09 06:33:13,297 [trainer.py] => model_name: adam_adapter
2023-09-09 06:33:13,297 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 06:33:13,297 [trainer.py] => device: [device(type='cuda', index=0)]
2023-09-09 06:33:13,297 [trainer.py] => seed: 1993
2023-09-09 06:33:13,297 [trainer.py] => tuned_epoch: 20
2023-09-09 06:33:13,297 [trainer.py] => init_lr: 0.01
2023-09-09 06:33:13,297 [trainer.py] => batch_size: 48
2023-09-09 06:33:13,297 [trainer.py] => weight_decay: 0.0005
2023-09-09 06:33:13,297 [trainer.py] => min_lr: 0
2023-09-09 06:33:13,297 [trainer.py] => ffn_num: 64
2023-09-09 06:33:13,297 [trainer.py] => optimizer: sgd
2023-09-09 06:33:13,297 [trainer.py] => vpt_type: shallow
2023-09-09 06:33:13,297 [trainer.py] => prompt_token_num: 5
2023-09-09 06:33:13,491 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 06:33:50,435 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 06:33:50,435 [trainer.py] => prefix:  
2023-09-09 06:33:50,435 [trainer.py] => dataset: omnibenchmark
2023-09-09 06:33:50,435 [trainer.py] => memory_size: 0
2023-09-09 06:33:50,435 [trainer.py] => memory_per_class: 0
2023-09-09 06:33:50,435 [trainer.py] => fixed_memory: False
2023-09-09 06:33:50,436 [trainer.py] => shuffle: True
2023-09-09 06:33:50,436 [trainer.py] => init_cls: 30
2023-09-09 06:33:50,436 [trainer.py] => increment: 30
2023-09-09 06:33:50,436 [trainer.py] => model_name: adam_adapter
2023-09-09 06:33:50,436 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 06:33:50,436 [trainer.py] => device: [device(type='cuda', index=0)]
2023-09-09 06:33:50,436 [trainer.py] => seed: 1993
2023-09-09 06:33:50,436 [trainer.py] => tuned_epoch: 20
2023-09-09 06:33:50,436 [trainer.py] => init_lr: 0.01
2023-09-09 06:33:50,436 [trainer.py] => batch_size: 48
2023-09-09 06:33:50,436 [trainer.py] => weight_decay: 0.0005
2023-09-09 06:33:50,436 [trainer.py] => min_lr: 0
2023-09-09 06:33:50,436 [trainer.py] => ffn_num: 64
2023-09-09 06:33:50,436 [trainer.py] => optimizer: sgd
2023-09-09 06:33:50,436 [trainer.py] => vpt_type: shallow
2023-09-09 06:33:50,436 [trainer.py] => prompt_token_num: 5
2023-09-09 06:33:50,626 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 06:33:52,988 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 06:34:36,809 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 06:34:36,912 [trainer.py] => All params: 86988288
2023-09-09 06:34:36,912 [trainer.py] => Trainable params: 1189632
2023-09-09 06:34:37,181 [adam_adapter.py] => Learning on 0-30
2023-09-09 06:35:35,351 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 06:35:35,351 [trainer.py] => prefix:  
2023-09-09 06:35:35,351 [trainer.py] => dataset: omnibenchmark
2023-09-09 06:35:35,351 [trainer.py] => memory_size: 0
2023-09-09 06:35:35,351 [trainer.py] => memory_per_class: 0
2023-09-09 06:35:35,351 [trainer.py] => fixed_memory: False
2023-09-09 06:35:35,351 [trainer.py] => shuffle: True
2023-09-09 06:35:35,351 [trainer.py] => init_cls: 30
2023-09-09 06:35:35,351 [trainer.py] => increment: 30
2023-09-09 06:35:35,351 [trainer.py] => model_name: adam_adapter
2023-09-09 06:35:35,351 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 06:35:35,351 [trainer.py] => device: [device(type='cuda', index=0)]
2023-09-09 06:35:35,352 [trainer.py] => seed: 1993
2023-09-09 06:35:35,352 [trainer.py] => tuned_epoch: 20
2023-09-09 06:35:35,352 [trainer.py] => init_lr: 0.01
2023-09-09 06:35:35,352 [trainer.py] => batch_size: 48
2023-09-09 06:35:35,352 [trainer.py] => weight_decay: 0.0005
2023-09-09 06:35:35,352 [trainer.py] => min_lr: 0
2023-09-09 06:35:35,352 [trainer.py] => ffn_num: 64
2023-09-09 06:35:35,352 [trainer.py] => optimizer: sgd
2023-09-09 06:35:35,352 [trainer.py] => vpt_type: shallow
2023-09-09 06:35:35,352 [trainer.py] => prompt_token_num: 5
2023-09-09 06:35:35,542 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 06:35:37,670 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 06:35:38,178 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 06:35:38,248 [trainer.py] => All params: 86988288
2023-09-09 06:35:38,249 [trainer.py] => Trainable params: 1189632
2023-09-09 06:35:38,381 [adam_adapter.py] => Learning on 0-30
2023-09-09 06:36:53,883 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 06:36:53,883 [trainer.py] => prefix:  
2023-09-09 06:36:53,883 [trainer.py] => dataset: omnibenchmark
2023-09-09 06:36:53,883 [trainer.py] => memory_size: 0
2023-09-09 06:36:53,883 [trainer.py] => memory_per_class: 0
2023-09-09 06:36:53,883 [trainer.py] => fixed_memory: False
2023-09-09 06:36:53,883 [trainer.py] => shuffle: True
2023-09-09 06:36:53,883 [trainer.py] => init_cls: 30
2023-09-09 06:36:53,883 [trainer.py] => increment: 30
2023-09-09 06:36:53,883 [trainer.py] => model_name: adam_adapter
2023-09-09 06:36:53,883 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 06:36:53,883 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 06:36:53,883 [trainer.py] => seed: 1993
2023-09-09 06:36:53,883 [trainer.py] => tuned_epoch: 20
2023-09-09 06:36:53,883 [trainer.py] => init_lr: 0.02
2023-09-09 06:36:53,883 [trainer.py] => batch_size: 96
2023-09-09 06:36:53,883 [trainer.py] => weight_decay: 0.0005
2023-09-09 06:36:53,883 [trainer.py] => min_lr: 0
2023-09-09 06:36:53,883 [trainer.py] => ffn_num: 64
2023-09-09 06:36:53,883 [trainer.py] => optimizer: sgd
2023-09-09 06:36:53,884 [trainer.py] => vpt_type: shallow
2023-09-09 06:36:53,884 [trainer.py] => prompt_token_num: 5
2023-09-09 06:36:54,074 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 06:36:56,134 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 06:36:56,842 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 06:36:56,908 [trainer.py] => All params: 86988288
2023-09-09 06:36:56,909 [trainer.py] => Trainable params: 1189632
2023-09-09 06:36:57,042 [adam_adapter.py] => Learning on 0-30
2023-09-09 06:38:36,386 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 06:38:36,386 [trainer.py] => prefix:  
2023-09-09 06:38:36,386 [trainer.py] => dataset: omnibenchmark
2023-09-09 06:38:36,386 [trainer.py] => memory_size: 0
2023-09-09 06:38:36,386 [trainer.py] => memory_per_class: 0
2023-09-09 06:38:36,387 [trainer.py] => fixed_memory: False
2023-09-09 06:38:36,387 [trainer.py] => shuffle: True
2023-09-09 06:38:36,387 [trainer.py] => init_cls: 30
2023-09-09 06:38:36,387 [trainer.py] => increment: 30
2023-09-09 06:38:36,387 [trainer.py] => model_name: adam_adapter
2023-09-09 06:38:36,387 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 06:38:36,387 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 06:38:36,387 [trainer.py] => seed: 1993
2023-09-09 06:38:36,387 [trainer.py] => tuned_epoch: 20
2023-09-09 06:38:36,387 [trainer.py] => init_lr: 0.02
2023-09-09 06:38:36,387 [trainer.py] => batch_size: 96
2023-09-09 06:38:36,387 [trainer.py] => weight_decay: 0.0005
2023-09-09 06:38:36,387 [trainer.py] => min_lr: 0
2023-09-09 06:38:36,387 [trainer.py] => ffn_num: 64
2023-09-09 06:38:36,387 [trainer.py] => optimizer: sgd
2023-09-09 06:38:36,387 [trainer.py] => vpt_type: shallow
2023-09-09 06:38:36,387 [trainer.py] => prompt_token_num: 5
2023-09-09 06:38:36,582 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 06:38:38,711 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 06:38:39,220 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 06:38:39,288 [trainer.py] => All params: 86988288
2023-09-09 06:38:39,288 [trainer.py] => Trainable params: 1189632
2023-09-09 06:38:39,427 [adam_adapter.py] => Learning on 0-30
2023-09-09 06:56:37,649 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.339, Train_accy 90.35, Test_accy 91.83
2023-09-09 06:56:38,743 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 06:56:39,251 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 06:56:39,959 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 06:56:40,380 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:01:18,872 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:01:18,872 [trainer.py] => prefix:  
2023-09-09 07:01:18,872 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:01:18,872 [trainer.py] => memory_size: 0
2023-09-09 07:01:18,872 [trainer.py] => memory_per_class: 0
2023-09-09 07:01:18,872 [trainer.py] => fixed_memory: False
2023-09-09 07:01:18,872 [trainer.py] => shuffle: True
2023-09-09 07:01:18,872 [trainer.py] => init_cls: 30
2023-09-09 07:01:18,872 [trainer.py] => increment: 30
2023-09-09 07:01:18,872 [trainer.py] => model_name: adam_adapter
2023-09-09 07:01:18,872 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:01:18,872 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:01:18,872 [trainer.py] => seed: 1993
2023-09-09 07:01:18,873 [trainer.py] => tuned_epoch: 1
2023-09-09 07:01:18,873 [trainer.py] => init_lr: 0.04
2023-09-09 07:01:18,873 [trainer.py] => batch_size: 192
2023-09-09 07:01:18,873 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:01:18,873 [trainer.py] => min_lr: 0
2023-09-09 07:01:18,873 [trainer.py] => ffn_num: 64
2023-09-09 07:01:18,873 [trainer.py] => optimizer: sgd
2023-09-09 07:01:18,873 [trainer.py] => vpt_type: shallow
2023-09-09 07:01:18,873 [trainer.py] => prompt_token_num: 5
2023-09-09 07:01:19,063 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:01:21,108 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:01:21,604 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:01:21,670 [trainer.py] => All params: 86988288
2023-09-09 07:01:21,671 [trainer.py] => Trainable params: 1189632
2023-09-09 07:01:21,819 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:02:15,437 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.060, Train_accy 54.44, Test_accy 79.83
2023-09-09 07:02:16,524 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:02:16,773 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:02:17,470 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:02:17,732 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:04:19,027 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:04:19,027 [trainer.py] => prefix:  
2023-09-09 07:04:19,027 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:04:19,027 [trainer.py] => memory_size: 0
2023-09-09 07:04:19,027 [trainer.py] => memory_per_class: 0
2023-09-09 07:04:19,027 [trainer.py] => fixed_memory: False
2023-09-09 07:04:19,027 [trainer.py] => shuffle: True
2023-09-09 07:04:19,027 [trainer.py] => init_cls: 30
2023-09-09 07:04:19,027 [trainer.py] => increment: 30
2023-09-09 07:04:19,027 [trainer.py] => model_name: adam_adapter
2023-09-09 07:04:19,027 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:04:19,027 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:04:19,027 [trainer.py] => seed: 1993
2023-09-09 07:04:19,027 [trainer.py] => tuned_epoch: 1
2023-09-09 07:04:19,027 [trainer.py] => init_lr: 0.04
2023-09-09 07:04:19,027 [trainer.py] => batch_size: 192
2023-09-09 07:04:19,027 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:04:19,027 [trainer.py] => min_lr: 0
2023-09-09 07:04:19,027 [trainer.py] => ffn_num: 64
2023-09-09 07:04:19,027 [trainer.py] => optimizer: sgd
2023-09-09 07:04:19,027 [trainer.py] => vpt_type: shallow
2023-09-09 07:04:19,027 [trainer.py] => prompt_token_num: 5
2023-09-09 07:04:19,219 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:04:21,454 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:04:21,955 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:04:22,016 [trainer.py] => All params: 86988288
2023-09-09 07:04:22,017 [trainer.py] => Trainable params: 1189632
2023-09-09 07:04:22,155 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:04:43,821 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:04:43,821 [trainer.py] => prefix:  
2023-09-09 07:04:43,821 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:04:43,821 [trainer.py] => memory_size: 0
2023-09-09 07:04:43,821 [trainer.py] => memory_per_class: 0
2023-09-09 07:04:43,821 [trainer.py] => fixed_memory: False
2023-09-09 07:04:43,821 [trainer.py] => shuffle: True
2023-09-09 07:04:43,821 [trainer.py] => init_cls: 30
2023-09-09 07:04:43,821 [trainer.py] => increment: 30
2023-09-09 07:04:43,821 [trainer.py] => model_name: adam_adapter
2023-09-09 07:04:43,821 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:04:43,821 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:04:43,821 [trainer.py] => seed: 1993
2023-09-09 07:04:43,821 [trainer.py] => tuned_epoch: 1
2023-09-09 07:04:43,821 [trainer.py] => init_lr: 0.04
2023-09-09 07:04:43,821 [trainer.py] => batch_size: 192
2023-09-09 07:04:43,821 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:04:43,821 [trainer.py] => min_lr: 0
2023-09-09 07:04:43,821 [trainer.py] => ffn_num: 64
2023-09-09 07:04:43,821 [trainer.py] => optimizer: sgd
2023-09-09 07:04:43,821 [trainer.py] => vpt_type: shallow
2023-09-09 07:04:43,821 [trainer.py] => prompt_token_num: 5
2023-09-09 07:04:44,013 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:04:46,003 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:04:46,501 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:04:46,573 [trainer.py] => All params: 86988288
2023-09-09 07:04:46,573 [trainer.py] => Trainable params: 1189632
2023-09-09 07:04:46,729 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:05:40,648 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.060, Train_accy 54.44, Test_accy 79.83
2023-09-09 07:05:41,925 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:05:42,190 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:05:42,890 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:05:43,148 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:08:31,331 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:08:31,331 [trainer.py] => prefix:  
2023-09-09 07:08:31,331 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:08:31,331 [trainer.py] => memory_size: 0
2023-09-09 07:08:31,331 [trainer.py] => memory_per_class: 0
2023-09-09 07:08:31,331 [trainer.py] => fixed_memory: False
2023-09-09 07:08:31,331 [trainer.py] => shuffle: True
2023-09-09 07:08:31,331 [trainer.py] => init_cls: 30
2023-09-09 07:08:31,332 [trainer.py] => increment: 30
2023-09-09 07:08:31,332 [trainer.py] => model_name: adam_adapter
2023-09-09 07:08:31,332 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:08:31,332 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:08:31,332 [trainer.py] => seed: 1993
2023-09-09 07:08:31,332 [trainer.py] => tuned_epoch: 1
2023-09-09 07:08:31,332 [trainer.py] => init_lr: 0.04
2023-09-09 07:08:31,332 [trainer.py] => batch_size: 192
2023-09-09 07:08:31,332 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:08:31,332 [trainer.py] => min_lr: 0
2023-09-09 07:08:31,332 [trainer.py] => ffn_num: 64
2023-09-09 07:08:31,333 [trainer.py] => optimizer: sgd
2023-09-09 07:08:31,333 [trainer.py] => vpt_type: shallow
2023-09-09 07:08:31,333 [trainer.py] => prompt_token_num: 5
2023-09-09 07:08:32,072 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:08:34,885 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:08:35,396 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:08:35,506 [trainer.py] => All params: 86988288
2023-09-09 07:08:35,517 [trainer.py] => Trainable params: 1189632
2023-09-09 07:08:35,667 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:10:50,808 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.060, Train_accy 54.44, Test_accy 79.83
2023-09-09 07:10:51,924 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:10:52,198 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:10:52,935 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:10:53,195 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:12:42,456 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:12:42,456 [trainer.py] => prefix:  
2023-09-09 07:12:42,456 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:12:42,456 [trainer.py] => memory_size: 0
2023-09-09 07:12:42,456 [trainer.py] => memory_per_class: 0
2023-09-09 07:12:42,456 [trainer.py] => fixed_memory: False
2023-09-09 07:12:42,456 [trainer.py] => shuffle: True
2023-09-09 07:12:42,457 [trainer.py] => init_cls: 30
2023-09-09 07:12:42,457 [trainer.py] => increment: 30
2023-09-09 07:12:42,457 [trainer.py] => model_name: adam_adapter
2023-09-09 07:12:42,457 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:12:42,457 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:12:42,457 [trainer.py] => seed: 1993
2023-09-09 07:12:42,457 [trainer.py] => tuned_epoch: 1
2023-09-09 07:12:42,457 [trainer.py] => init_lr: 0.04
2023-09-09 07:12:42,457 [trainer.py] => batch_size: 192
2023-09-09 07:12:42,458 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:12:42,458 [trainer.py] => min_lr: 0
2023-09-09 07:12:42,458 [trainer.py] => ffn_num: 64
2023-09-09 07:12:42,458 [trainer.py] => optimizer: sgd
2023-09-09 07:12:42,458 [trainer.py] => vpt_type: shallow
2023-09-09 07:12:42,458 [trainer.py] => prompt_token_num: 5
2023-09-09 07:12:43,210 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:12:45,889 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:12:46,459 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:12:46,553 [trainer.py] => All params: 86988288
2023-09-09 07:12:46,565 [trainer.py] => Trainable params: 1189632
2023-09-09 07:12:46,704 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:13:50,460 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.060, Train_accy 54.44, Test_accy 79.83
2023-09-09 07:13:51,598 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:13:51,869 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:13:52,598 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:13:52,869 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:15:41,714 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:15:41,714 [trainer.py] => prefix:  
2023-09-09 07:15:41,714 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:15:41,714 [trainer.py] => memory_size: 0
2023-09-09 07:15:41,714 [trainer.py] => memory_per_class: 0
2023-09-09 07:15:41,714 [trainer.py] => fixed_memory: False
2023-09-09 07:15:41,714 [trainer.py] => shuffle: True
2023-09-09 07:15:41,714 [trainer.py] => init_cls: 30
2023-09-09 07:15:41,714 [trainer.py] => increment: 30
2023-09-09 07:15:41,714 [trainer.py] => model_name: adam_adapter
2023-09-09 07:15:41,714 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:15:41,715 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:15:41,715 [trainer.py] => seed: 1993
2023-09-09 07:15:41,715 [trainer.py] => tuned_epoch: 1
2023-09-09 07:15:41,715 [trainer.py] => init_lr: 0.04
2023-09-09 07:15:41,715 [trainer.py] => batch_size: 192
2023-09-09 07:15:41,715 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:15:41,715 [trainer.py] => min_lr: 0
2023-09-09 07:15:41,715 [trainer.py] => ffn_num: 64
2023-09-09 07:15:41,715 [trainer.py] => optimizer: sgd
2023-09-09 07:15:41,715 [trainer.py] => vpt_type: shallow
2023-09-09 07:15:41,715 [trainer.py] => prompt_token_num: 5
2023-09-09 07:15:41,905 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:15:43,908 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:15:44,411 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:15:44,479 [trainer.py] => All params: 86988288
2023-09-09 07:15:44,479 [trainer.py] => Trainable params: 1189632
2023-09-09 07:15:44,634 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:16:38,364 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.060, Train_accy 54.44, Test_accy 79.83
2023-09-09 07:16:39,450 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:16:39,719 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:16:40,410 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:16:40,689 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:20:04,302 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:20:04,302 [trainer.py] => prefix:  
2023-09-09 07:20:04,302 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:20:04,302 [trainer.py] => memory_size: 0
2023-09-09 07:20:04,302 [trainer.py] => memory_per_class: 0
2023-09-09 07:20:04,302 [trainer.py] => fixed_memory: False
2023-09-09 07:20:04,302 [trainer.py] => shuffle: True
2023-09-09 07:20:04,302 [trainer.py] => init_cls: 30
2023-09-09 07:20:04,302 [trainer.py] => increment: 30
2023-09-09 07:20:04,302 [trainer.py] => model_name: adam_adapter
2023-09-09 07:20:04,302 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:20:04,302 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:20:04,302 [trainer.py] => seed: 1993
2023-09-09 07:20:04,302 [trainer.py] => tuned_epoch: 1
2023-09-09 07:20:04,302 [trainer.py] => init_lr: 0.04
2023-09-09 07:20:04,303 [trainer.py] => batch_size: 192
2023-09-09 07:20:04,303 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:20:04,303 [trainer.py] => min_lr: 0
2023-09-09 07:20:04,303 [trainer.py] => ffn_num: 64
2023-09-09 07:20:04,303 [trainer.py] => optimizer: sgd
2023-09-09 07:20:04,303 [trainer.py] => vpt_type: shallow
2023-09-09 07:20:04,303 [trainer.py] => prompt_token_num: 5
2023-09-09 07:20:04,493 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:20:06,506 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:20:07,015 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:20:07,087 [trainer.py] => All params: 86988288
2023-09-09 07:20:07,088 [trainer.py] => Trainable params: 1189632
2023-09-09 07:20:07,231 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:20:40,078 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:20:40,078 [trainer.py] => prefix:  
2023-09-09 07:20:40,078 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:20:40,078 [trainer.py] => memory_size: 0
2023-09-09 07:20:40,078 [trainer.py] => memory_per_class: 0
2023-09-09 07:20:40,078 [trainer.py] => fixed_memory: False
2023-09-09 07:20:40,078 [trainer.py] => shuffle: True
2023-09-09 07:20:40,078 [trainer.py] => init_cls: 30
2023-09-09 07:20:40,078 [trainer.py] => increment: 30
2023-09-09 07:20:40,079 [trainer.py] => model_name: adam_adapter
2023-09-09 07:20:40,079 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:20:40,079 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:20:40,079 [trainer.py] => seed: 1993
2023-09-09 07:20:40,079 [trainer.py] => tuned_epoch: 1
2023-09-09 07:20:40,079 [trainer.py] => init_lr: 0.04
2023-09-09 07:20:40,079 [trainer.py] => batch_size: 192
2023-09-09 07:20:40,079 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:20:40,079 [trainer.py] => min_lr: 0
2023-09-09 07:20:40,079 [trainer.py] => ffn_num: 64
2023-09-09 07:20:40,079 [trainer.py] => optimizer: sgd
2023-09-09 07:20:40,079 [trainer.py] => vpt_type: shallow
2023-09-09 07:20:40,079 [trainer.py] => prompt_token_num: 5
2023-09-09 07:20:40,269 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:20:42,245 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:20:42,741 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:20:42,810 [trainer.py] => All params: 86988288
2023-09-09 07:20:42,811 [trainer.py] => Trainable params: 1189632
2023-09-09 07:20:42,955 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:20:58,269 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:20:58,269 [trainer.py] => prefix:  
2023-09-09 07:20:58,269 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:20:58,269 [trainer.py] => memory_size: 0
2023-09-09 07:20:58,269 [trainer.py] => memory_per_class: 0
2023-09-09 07:20:58,269 [trainer.py] => fixed_memory: False
2023-09-09 07:20:58,269 [trainer.py] => shuffle: True
2023-09-09 07:20:58,269 [trainer.py] => init_cls: 30
2023-09-09 07:20:58,269 [trainer.py] => increment: 30
2023-09-09 07:20:58,269 [trainer.py] => model_name: adam_adapter
2023-09-09 07:20:58,269 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:20:58,269 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:20:58,269 [trainer.py] => seed: 1993
2023-09-09 07:20:58,269 [trainer.py] => tuned_epoch: 1
2023-09-09 07:20:58,269 [trainer.py] => init_lr: 0.04
2023-09-09 07:20:58,269 [trainer.py] => batch_size: 192
2023-09-09 07:20:58,269 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:20:58,269 [trainer.py] => min_lr: 0
2023-09-09 07:20:58,269 [trainer.py] => ffn_num: 64
2023-09-09 07:20:58,269 [trainer.py] => optimizer: sgd
2023-09-09 07:20:58,269 [trainer.py] => vpt_type: shallow
2023-09-09 07:20:58,269 [trainer.py] => prompt_token_num: 5
2023-09-09 07:20:58,462 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:21:00,555 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:21:01,044 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:21:01,103 [trainer.py] => All params: 86988288
2023-09-09 07:21:01,103 [trainer.py] => Trainable params: 1189632
2023-09-09 07:21:01,235 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:21:55,117 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.060, Train_accy 54.44, Test_accy 79.83
2023-09-09 07:21:56,318 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:21:56,570 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:21:57,298 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:21:57,549 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:23:31,055 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:23:31,055 [trainer.py] => prefix:  
2023-09-09 07:23:31,055 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:23:31,055 [trainer.py] => memory_size: 0
2023-09-09 07:23:31,055 [trainer.py] => memory_per_class: 0
2023-09-09 07:23:31,055 [trainer.py] => fixed_memory: False
2023-09-09 07:23:31,056 [trainer.py] => shuffle: True
2023-09-09 07:23:31,056 [trainer.py] => init_cls: 30
2023-09-09 07:23:31,056 [trainer.py] => increment: 30
2023-09-09 07:23:31,056 [trainer.py] => model_name: adam_adapter
2023-09-09 07:23:31,056 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:23:31,056 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:23:31,056 [trainer.py] => seed: 1993
2023-09-09 07:23:31,056 [trainer.py] => tuned_epoch: 1
2023-09-09 07:23:31,056 [trainer.py] => init_lr: 0.04
2023-09-09 07:23:31,056 [trainer.py] => batch_size: 192
2023-09-09 07:23:31,056 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:23:31,057 [trainer.py] => min_lr: 0
2023-09-09 07:23:31,057 [trainer.py] => ffn_num: 64
2023-09-09 07:23:31,057 [trainer.py] => optimizer: sgd
2023-09-09 07:23:31,057 [trainer.py] => vpt_type: shallow
2023-09-09 07:23:31,057 [trainer.py] => prompt_token_num: 5
2023-09-09 07:23:31,803 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:23:34,491 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:23:35,001 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:23:35,097 [trainer.py] => All params: 86988288
2023-09-09 07:23:35,108 [trainer.py] => Trainable params: 1189632
2023-09-09 07:23:38,277 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:26:02,076 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.058, Train_accy 54.66, Test_accy 79.67
2023-09-09 07:26:03,191 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:26:03,470 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:26:04,199 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:26:04,477 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:29:34,635 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:29:34,635 [trainer.py] => prefix:  
2023-09-09 07:29:34,635 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:29:34,635 [trainer.py] => memory_size: 0
2023-09-09 07:29:34,635 [trainer.py] => memory_per_class: 0
2023-09-09 07:29:34,635 [trainer.py] => fixed_memory: False
2023-09-09 07:29:34,635 [trainer.py] => shuffle: True
2023-09-09 07:29:34,635 [trainer.py] => init_cls: 30
2023-09-09 07:29:34,635 [trainer.py] => increment: 30
2023-09-09 07:29:34,635 [trainer.py] => model_name: adam_adapter
2023-09-09 07:29:34,636 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:29:34,636 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:29:34,636 [trainer.py] => seed: 1993
2023-09-09 07:29:34,636 [trainer.py] => tuned_epoch: 1
2023-09-09 07:29:34,636 [trainer.py] => init_lr: 0.04
2023-09-09 07:29:34,636 [trainer.py] => batch_size: 192
2023-09-09 07:29:34,636 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:29:34,636 [trainer.py] => min_lr: 0
2023-09-09 07:29:34,636 [trainer.py] => ffn_num: 64
2023-09-09 07:29:34,636 [trainer.py] => optimizer: sgd
2023-09-09 07:29:34,636 [trainer.py] => vpt_type: shallow
2023-09-09 07:29:34,636 [trainer.py] => prompt_token_num: 5
2023-09-09 07:29:34,827 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:29:36,860 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:29:37,359 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:29:37,426 [trainer.py] => All params: 86988288
2023-09-09 07:29:37,427 [trainer.py] => Trainable params: 1189632
2023-09-09 07:29:37,610 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:30:37,136 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.058, Train_accy 54.66, Test_accy 79.67
2023-09-09 07:30:38,330 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:30:38,984 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:30:39,686 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:30:39,950 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:32:08,337 [trainer.py] => No NME accuracy.
2023-09-09 07:32:08,337 [trainer.py] => CNN: {'total': 87.17, '00-09': 90.5, '10-19': 88.0, '20-29': 83.0, 'old': 0, 'new': 87.17}
2023-09-09 07:32:08,337 [trainer.py] => CNN top1 curve: [87.17]
2023-09-09 07:32:08,337 [trainer.py] => CNN top5 curve: [98.83]

2023-09-09 07:32:08,337 [trainer.py] => Average Accuracy (CNN): 87.17
2023-09-09 07:32:08,339 [trainer.py] => All params: 172833025
2023-09-09 07:32:08,339 [trainer.py] => Trainable params: 87034369
2023-09-09 07:32:08,353 [adam_adapter.py] => Learning on 30-60
2023-09-09 07:41:13,915 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:41:13,915 [trainer.py] => prefix:  
2023-09-09 07:41:13,916 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:41:13,916 [trainer.py] => memory_size: 0
2023-09-09 07:41:13,916 [trainer.py] => memory_per_class: 0
2023-09-09 07:41:13,916 [trainer.py] => fixed_memory: False
2023-09-09 07:41:13,916 [trainer.py] => shuffle: True
2023-09-09 07:41:13,916 [trainer.py] => init_cls: 30
2023-09-09 07:41:13,916 [trainer.py] => increment: 30
2023-09-09 07:41:13,916 [trainer.py] => model_name: adam_adapter
2023-09-09 07:41:13,916 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:41:13,916 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:41:13,916 [trainer.py] => seed: 1993
2023-09-09 07:41:13,917 [trainer.py] => tuned_epoch: 1
2023-09-09 07:41:13,917 [trainer.py] => init_lr: 0.04
2023-09-09 07:41:13,917 [trainer.py] => batch_size: 192
2023-09-09 07:41:13,917 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:41:13,917 [trainer.py] => min_lr: 0
2023-09-09 07:41:13,917 [trainer.py] => ffn_num: 64
2023-09-09 07:41:13,917 [trainer.py] => optimizer: sgd
2023-09-09 07:41:13,917 [trainer.py] => vpt_type: shallow
2023-09-09 07:41:13,917 [trainer.py] => prompt_token_num: 5
2023-09-09 07:41:14,644 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:41:17,693 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:41:18,210 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:41:18,322 [trainer.py] => All params: 86988288
2023-09-09 07:41:18,335 [trainer.py] => Trainable params: 1189632
2023-09-09 07:41:25,508 [adam_adapter.py] => Learning on 0-30
2023-09-09 07:42:30,394 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.058, Train_accy 54.66, Test_accy 79.67
2023-09-09 07:42:31,511 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:42:31,776 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:43:44,721 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:43:44,992 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:54:54,704 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 07:54:54,704 [trainer.py] => prefix:  
2023-09-09 07:54:54,704 [trainer.py] => dataset: omnibenchmark
2023-09-09 07:54:54,704 [trainer.py] => memory_size: 0
2023-09-09 07:54:54,704 [trainer.py] => memory_per_class: 0
2023-09-09 07:54:54,704 [trainer.py] => fixed_memory: False
2023-09-09 07:54:54,704 [trainer.py] => shuffle: True
2023-09-09 07:54:54,704 [trainer.py] => init_cls: 30
2023-09-09 07:54:54,704 [trainer.py] => increment: 30
2023-09-09 07:54:54,704 [trainer.py] => model_name: adam_adapter
2023-09-09 07:54:54,704 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 07:54:54,704 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 07:54:54,704 [trainer.py] => seed: 1993
2023-09-09 07:54:54,704 [trainer.py] => tuned_epoch: 20
2023-09-09 07:54:54,704 [trainer.py] => init_lr: 0.04
2023-09-09 07:54:54,704 [trainer.py] => batch_size: 192
2023-09-09 07:54:54,704 [trainer.py] => weight_decay: 0.0005
2023-09-09 07:54:54,704 [trainer.py] => min_lr: 0
2023-09-09 07:54:54,705 [trainer.py] => ffn_num: 64
2023-09-09 07:54:54,705 [trainer.py] => optimizer: sgd
2023-09-09 07:54:54,705 [trainer.py] => vpt_type: shallow
2023-09-09 07:54:54,705 [trainer.py] => prompt_token_num: 5
2023-09-09 07:54:54,895 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 07:54:56,868 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 07:54:57,362 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 07:54:57,429 [trainer.py] => All params: 86988288
2023-09-09 07:54:57,429 [trainer.py] => Trainable params: 1189632
2023-09-09 07:54:57,585 [adam_adapter.py] => Learning on 0-30
2023-09-09 08:14:52,964 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.341, Train_accy 90.22, Test_accy 92.17
2023-09-09 08:14:54,073 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:14:54,572 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:14:55,257 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:14:55,505 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:16:25,576 [trainer.py] => No NME accuracy.
2023-09-09 08:16:25,576 [trainer.py] => CNN: {'total': 89.33, '00-09': 89.0, '10-19': 90.5, '20-29': 88.5, 'old': 0, 'new': 89.33}
2023-09-09 08:16:25,576 [trainer.py] => CNN top1 curve: [89.33]
2023-09-09 08:16:25,577 [trainer.py] => CNN top5 curve: [99.0]

2023-09-09 08:16:25,577 [trainer.py] => Average Accuracy (CNN): 89.33
2023-09-09 08:16:25,578 [trainer.py] => All params: 172833025
2023-09-09 08:16:25,580 [trainer.py] => Trainable params: 87034369
2023-09-09 08:16:25,582 [adam_adapter.py] => Learning on 30-60
2023-09-09 08:17:17,743 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 08:17:17,743 [trainer.py] => prefix:  
2023-09-09 08:17:17,743 [trainer.py] => dataset: omnibenchmark
2023-09-09 08:17:17,743 [trainer.py] => memory_size: 0
2023-09-09 08:17:17,743 [trainer.py] => memory_per_class: 0
2023-09-09 08:17:17,743 [trainer.py] => fixed_memory: False
2023-09-09 08:17:17,743 [trainer.py] => shuffle: True
2023-09-09 08:17:17,743 [trainer.py] => init_cls: 30
2023-09-09 08:17:17,743 [trainer.py] => increment: 30
2023-09-09 08:17:17,743 [trainer.py] => model_name: adam_adapter
2023-09-09 08:17:17,743 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 08:17:17,743 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 08:17:17,743 [trainer.py] => seed: 1993
2023-09-09 08:17:17,743 [trainer.py] => tuned_epoch: 20
2023-09-09 08:17:17,743 [trainer.py] => init_lr: 0.04
2023-09-09 08:17:17,743 [trainer.py] => batch_size: 192
2023-09-09 08:17:17,743 [trainer.py] => weight_decay: 0.0005
2023-09-09 08:17:17,743 [trainer.py] => min_lr: 0
2023-09-09 08:17:17,743 [trainer.py] => ffn_num: 64
2023-09-09 08:17:17,743 [trainer.py] => optimizer: sgd
2023-09-09 08:17:17,743 [trainer.py] => vpt_type: shallow
2023-09-09 08:17:17,743 [trainer.py] => prompt_token_num: 5
2023-09-09 08:17:17,940 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 08:17:19,984 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:17:20,493 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:17:20,560 [trainer.py] => All params: 86988288
2023-09-09 08:17:20,560 [trainer.py] => Trainable params: 1189632
2023-09-09 08:17:20,648 [adam_adapter.py] => Learning on 0-30
2023-09-09 08:26:20,743 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 08:26:20,743 [trainer.py] => prefix:  
2023-09-09 08:26:20,743 [trainer.py] => dataset: omnibenchmark
2023-09-09 08:26:20,743 [trainer.py] => memory_size: 0
2023-09-09 08:26:20,743 [trainer.py] => memory_per_class: 0
2023-09-09 08:26:20,743 [trainer.py] => fixed_memory: False
2023-09-09 08:26:20,744 [trainer.py] => shuffle: True
2023-09-09 08:26:20,744 [trainer.py] => init_cls: 30
2023-09-09 08:26:20,744 [trainer.py] => increment: 30
2023-09-09 08:26:20,744 [trainer.py] => model_name: adam_adapter
2023-09-09 08:26:20,744 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 08:26:20,744 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 08:26:20,744 [trainer.py] => seed: 1993
2023-09-09 08:26:20,744 [trainer.py] => tuned_epoch: 1
2023-09-09 08:26:20,744 [trainer.py] => init_lr: 0.04
2023-09-09 08:26:20,744 [trainer.py] => batch_size: 192
2023-09-09 08:26:20,745 [trainer.py] => weight_decay: 0.0005
2023-09-09 08:26:20,745 [trainer.py] => min_lr: 0
2023-09-09 08:26:20,745 [trainer.py] => ffn_num: 64
2023-09-09 08:26:20,745 [trainer.py] => optimizer: sgd
2023-09-09 08:26:20,745 [trainer.py] => vpt_type: shallow
2023-09-09 08:26:20,745 [trainer.py] => prompt_token_num: 5
2023-09-09 08:26:21,474 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 08:26:24,493 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:26:25,012 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:26:25,120 [trainer.py] => All params: 86988288
2023-09-09 08:26:25,131 [trainer.py] => Trainable params: 1189632
2023-09-09 08:26:25,281 [adam_adapter.py] => Learning on 0-30
2023-09-09 08:27:31,090 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.058, Train_accy 54.66, Test_accy 79.67
2023-09-09 08:27:32,208 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:27:32,466 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:27:55,017 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:27:55,296 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:29:10,563 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 08:29:10,563 [trainer.py] => prefix:  
2023-09-09 08:29:10,563 [trainer.py] => dataset: omnibenchmark
2023-09-09 08:29:10,563 [trainer.py] => memory_size: 0
2023-09-09 08:29:10,563 [trainer.py] => memory_per_class: 0
2023-09-09 08:29:10,563 [trainer.py] => fixed_memory: False
2023-09-09 08:29:10,563 [trainer.py] => shuffle: True
2023-09-09 08:29:10,563 [trainer.py] => init_cls: 30
2023-09-09 08:29:10,563 [trainer.py] => increment: 30
2023-09-09 08:29:10,563 [trainer.py] => model_name: adam_adapter
2023-09-09 08:29:10,563 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 08:29:10,563 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 08:29:10,563 [trainer.py] => seed: 1993
2023-09-09 08:29:10,563 [trainer.py] => tuned_epoch: 1
2023-09-09 08:29:10,563 [trainer.py] => init_lr: 0.04
2023-09-09 08:29:10,563 [trainer.py] => batch_size: 192
2023-09-09 08:29:10,563 [trainer.py] => weight_decay: 0.0005
2023-09-09 08:29:10,563 [trainer.py] => min_lr: 0
2023-09-09 08:29:10,563 [trainer.py] => ffn_num: 64
2023-09-09 08:29:10,564 [trainer.py] => optimizer: sgd
2023-09-09 08:29:10,564 [trainer.py] => vpt_type: shallow
2023-09-09 08:29:10,564 [trainer.py] => prompt_token_num: 5
2023-09-09 08:29:10,754 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 08:29:13,134 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:29:13,636 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:29:13,704 [trainer.py] => All params: 86988288
2023-09-09 08:29:13,704 [trainer.py] => Trainable params: 1189632
2023-09-09 08:29:13,857 [adam_adapter.py] => Learning on 0-30
2023-09-09 08:30:13,314 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.058, Train_accy 54.66, Test_accy 79.67
2023-09-09 08:30:14,460 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:30:14,711 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:30:15,553 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:30:15,858 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:31:45,400 [trainer.py] => No NME accuracy.
2023-09-09 08:31:45,400 [trainer.py] => CNN: {'total': 87.17, '00-09': 90.5, '10-19': 88.0, '20-29': 83.0, 'old': 0, 'new': 87.17}
2023-09-09 08:31:45,400 [trainer.py] => CNN top1 curve: [87.17]
2023-09-09 08:31:45,400 [trainer.py] => CNN top5 curve: [98.83]

2023-09-09 08:31:45,400 [trainer.py] => Average Accuracy (CNN): 87.17
2023-09-09 08:31:45,402 [trainer.py] => All params: 172833025
2023-09-09 08:31:45,404 [trainer.py] => Trainable params: 87034369
2023-09-09 08:31:45,405 [adam_adapter.py] => Learning on 30-60
2023-09-09 08:33:13,679 [trainer.py] => No NME accuracy.
2023-09-09 08:33:13,679 [trainer.py] => CNN: {'total': 87.74, '00-09': 86.0, '10-19': 87.5, '20-29': 79.0, '30-39': 91.0, '40-49': 88.0, '50-59': 94.97, 'old': 84.17, 'new': 91.32}
2023-09-09 08:33:13,679 [trainer.py] => CNN top1 curve: [87.17, 87.74]
2023-09-09 08:33:13,679 [trainer.py] => CNN top5 curve: [98.83, 97.91]

2023-09-09 08:33:13,680 [trainer.py] => Average Accuracy (CNN): 87.455
2023-09-09 08:33:13,681 [trainer.py] => All params: 172879105
2023-09-09 08:33:13,682 [trainer.py] => Trainable params: 87080449
2023-09-09 08:33:13,684 [adam_adapter.py] => Learning on 60-90
2023-09-09 08:34:49,759 [trainer.py] => No NME accuracy.
2023-09-09 08:34:49,759 [trainer.py] => CNN: {'total': 85.09, '00-09': 82.0, '10-19': 82.0, '20-29': 75.5, '30-39': 88.5, '40-49': 86.0, '50-59': 93.97, '60-69': 91.5, '70-79': 87.5, '80-89': 78.89, 'old': 84.65, 'new': 85.98}
2023-09-09 08:34:49,759 [trainer.py] => CNN top1 curve: [87.17, 87.74, 85.09]
2023-09-09 08:34:49,759 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.05]

2023-09-09 08:34:49,759 [trainer.py] => Average Accuracy (CNN): 86.66666666666667
2023-09-09 08:34:49,761 [trainer.py] => All params: 172925185
2023-09-09 08:34:49,762 [trainer.py] => Trainable params: 87126529
2023-09-09 08:34:49,764 [adam_adapter.py] => Learning on 90-120
2023-09-09 08:36:35,989 [trainer.py] => No NME accuracy.
2023-09-09 08:36:35,990 [trainer.py] => CNN: {'total': 81.34, '00-09': 80.5, '10-19': 79.0, '20-29': 74.0, '30-39': 88.0, '40-49': 84.5, '50-59': 85.93, '60-69': 87.0, '70-79': 84.0, '80-89': 74.87, '90-99': 79.9, '100-109': 79.9, '110-119': 78.39, 'old': 81.98, 'new': 79.4}
2023-09-09 08:36:35,990 [trainer.py] => CNN top1 curve: [87.17, 87.74, 85.09, 81.34]
2023-09-09 08:36:35,990 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.05, 95.78]

2023-09-09 08:36:35,990 [trainer.py] => Average Accuracy (CNN): 85.33500000000001
2023-09-09 08:36:35,992 [trainer.py] => All params: 172971265
2023-09-09 08:36:35,993 [trainer.py] => Trainable params: 87172609
2023-09-09 08:36:35,995 [adam_adapter.py] => Learning on 120-150
2023-09-09 08:38:28,801 [trainer.py] => No NME accuracy.
2023-09-09 08:38:28,801 [trainer.py] => CNN: {'total': 79.53, '00-09': 77.5, '10-19': 78.0, '20-29': 73.0, '30-39': 87.5, '40-49': 82.0, '50-59': 80.4, '60-69': 82.5, '70-79': 83.0, '80-89': 73.37, '90-99': 79.4, '100-109': 77.39, '110-119': 72.36, '120-129': 85.0, '130-139': 80.5, '140-149': 80.9, 'old': 78.87, 'new': 82.14}
2023-09-09 08:38:28,801 [trainer.py] => CNN top1 curve: [87.17, 87.74, 85.09, 81.34, 79.53]
2023-09-09 08:38:28,801 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.05, 95.78, 94.66]

2023-09-09 08:38:28,801 [trainer.py] => Average Accuracy (CNN): 84.174
2023-09-09 08:38:28,803 [trainer.py] => All params: 173017345
2023-09-09 08:38:28,803 [trainer.py] => Trainable params: 87218689
2023-09-09 08:38:28,805 [adam_adapter.py] => Learning on 150-180
2023-09-09 08:40:24,093 [trainer.py] => No NME accuracy.
2023-09-09 08:40:24,093 [trainer.py] => CNN: {'total': 76.78, '00-09': 74.5, '10-19': 74.5, '20-29': 71.5, '30-39': 84.0, '40-49': 78.5, '50-59': 78.89, '60-69': 82.0, '70-79': 80.0, '80-89': 70.85, '90-99': 78.89, '100-109': 74.87, '110-119': 70.35, '120-129': 84.5, '130-139': 80.0, '140-149': 78.89, '150-159': 80.4, '160-169': 73.87, '170-179': 65.5, 'old': 77.49, 'new': 73.24}
2023-09-09 08:40:24,093 [trainer.py] => CNN top1 curve: [87.17, 87.74, 85.09, 81.34, 79.53, 76.78]
2023-09-09 08:40:24,093 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.05, 95.78, 94.66, 93.35]

2023-09-09 08:40:24,094 [trainer.py] => Average Accuracy (CNN): 82.94166666666666
2023-09-09 08:40:24,095 [trainer.py] => All params: 173063425
2023-09-09 08:40:24,095 [trainer.py] => Trainable params: 87264769
2023-09-09 08:40:24,097 [adam_adapter.py] => Learning on 180-210
2023-09-09 08:42:23,453 [trainer.py] => No NME accuracy.
2023-09-09 08:42:23,453 [trainer.py] => CNN: {'total': 75.77, '00-09': 74.0, '10-19': 74.5, '20-29': 71.0, '30-39': 84.0, '40-49': 77.0, '50-59': 77.89, '60-69': 78.5, '70-79': 79.5, '80-89': 70.85, '90-99': 77.39, '100-109': 74.37, '110-119': 67.34, '120-129': 84.5, '130-139': 79.0, '140-149': 78.39, '150-159': 77.89, '160-169': 72.86, '170-179': 65.0, '180-189': 73.23, '190-199': 83.5, '200-209': 70.35, 'old': 75.78, 'new': 75.71}
2023-09-09 08:42:23,453 [trainer.py] => CNN top1 curve: [87.17, 87.74, 85.09, 81.34, 79.53, 76.78, 75.77]
2023-09-09 08:42:23,453 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.05, 95.78, 94.66, 93.35, 92.84]

2023-09-09 08:42:23,453 [trainer.py] => Average Accuracy (CNN): 81.91714285714285
2023-09-09 08:42:23,455 [trainer.py] => All params: 173109505
2023-09-09 08:42:23,456 [trainer.py] => Trainable params: 87310849
2023-09-09 08:42:23,458 [adam_adapter.py] => Learning on 210-240
2023-09-09 08:44:31,098 [trainer.py] => No NME accuracy.
2023-09-09 08:44:31,098 [trainer.py] => CNN: {'total': 74.08, '00-09': 73.0, '10-19': 72.0, '20-29': 69.0, '30-39': 84.0, '40-49': 75.0, '50-59': 77.89, '60-69': 77.5, '70-79': 79.5, '80-89': 70.85, '90-99': 77.39, '100-109': 66.83, '110-119': 66.33, '120-129': 82.0, '130-139': 78.5, '140-149': 77.39, '150-159': 77.39, '160-169': 72.36, '170-179': 62.5, '180-189': 72.73, '190-199': 83.0, '200-209': 66.33, '210-219': 51.0, '220-229': 84.92, '230-239': 80.5, 'old': 74.36, 'new': 72.12}
2023-09-09 08:44:31,098 [trainer.py] => CNN top1 curve: [87.17, 87.74, 85.09, 81.34, 79.53, 76.78, 75.77, 74.08]
2023-09-09 08:44:31,098 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.05, 95.78, 94.66, 93.35, 92.84, 91.48]

2023-09-09 08:44:31,099 [trainer.py] => Average Accuracy (CNN): 80.9375
2023-09-09 08:44:31,100 [trainer.py] => All params: 173155585
2023-09-09 08:44:31,100 [trainer.py] => Trainable params: 87356929
2023-09-09 08:44:31,103 [adam_adapter.py] => Learning on 240-270
2023-09-09 08:46:41,659 [trainer.py] => No NME accuracy.
2023-09-09 08:46:41,659 [trainer.py] => CNN: {'total': 73.1, '00-09': 73.0, '10-19': 71.5, '20-29': 69.0, '30-39': 83.5, '40-49': 75.0, '50-59': 77.39, '60-69': 75.5, '70-79': 79.0, '80-89': 69.85, '90-99': 75.88, '100-109': 65.83, '110-119': 63.82, '120-129': 76.5, '130-139': 77.0, '140-149': 73.37, '150-159': 73.37, '160-169': 72.36, '170-179': 62.5, '180-189': 72.73, '190-199': 81.5, '200-209': 63.82, '210-219': 51.0, '220-229': 84.92, '230-239': 80.5, '240-249': 68.5, '250-259': 75.38, '260-269': 81.0, 'old': 72.87, 'new': 74.96}
2023-09-09 08:46:41,659 [trainer.py] => CNN top1 curve: [87.17, 87.74, 85.09, 81.34, 79.53, 76.78, 75.77, 74.08, 73.1]
2023-09-09 08:46:41,659 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.05, 95.78, 94.66, 93.35, 92.84, 91.48, 91.61]

2023-09-09 08:46:41,659 [trainer.py] => Average Accuracy (CNN): 80.06666666666666
2023-09-09 08:46:41,661 [trainer.py] => All params: 173201665
2023-09-09 08:46:41,662 [trainer.py] => Trainable params: 87403009
2023-09-09 08:46:41,667 [adam_adapter.py] => Learning on 270-300
2023-09-09 08:48:51,128 [trainer.py] => No NME accuracy.
2023-09-09 08:48:51,128 [trainer.py] => CNN: {'total': 73.15, '00-09': 72.5, '10-19': 71.5, '20-29': 68.0, '30-39': 83.5, '40-49': 72.5, '50-59': 76.88, '60-69': 74.5, '70-79': 79.0, '80-89': 69.85, '90-99': 71.86, '100-109': 64.32, '110-119': 63.32, '120-129': 76.5, '130-139': 76.5, '140-149': 73.37, '150-159': 73.37, '160-169': 70.85, '170-179': 62.0, '180-189': 71.72, '190-199': 81.0, '200-209': 63.82, '210-219': 50.5, '220-229': 84.92, '230-239': 80.5, '240-249': 65.5, '250-259': 74.87, '260-269': 80.0, '270-279': 78.89, '280-289': 82.5, '290-299': 79.9, 'old': 72.34, 'new': 80.43}
2023-09-09 08:48:51,128 [trainer.py] => CNN top1 curve: [87.17, 87.74, 85.09, 81.34, 79.53, 76.78, 75.77, 74.08, 73.1, 73.15]
2023-09-09 08:48:51,128 [trainer.py] => CNN top5 curve: [98.83, 97.91, 97.05, 95.78, 94.66, 93.35, 92.84, 91.48, 91.61, 91.63]

2023-09-09 08:48:51,128 [trainer.py] => Average Accuracy (CNN): 79.375
2023-09-09 08:52:17,769 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 08:52:17,769 [trainer.py] => prefix:  
2023-09-09 08:52:17,769 [trainer.py] => dataset: omnibenchmark
2023-09-09 08:52:17,769 [trainer.py] => memory_size: 0
2023-09-09 08:52:17,769 [trainer.py] => memory_per_class: 0
2023-09-09 08:52:17,769 [trainer.py] => fixed_memory: False
2023-09-09 08:52:17,769 [trainer.py] => shuffle: True
2023-09-09 08:52:17,769 [trainer.py] => init_cls: 30
2023-09-09 08:52:17,769 [trainer.py] => increment: 30
2023-09-09 08:52:17,769 [trainer.py] => model_name: adam_adapter
2023-09-09 08:52:17,769 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 08:52:17,769 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-09-09 08:52:17,769 [trainer.py] => seed: 1993
2023-09-09 08:52:17,769 [trainer.py] => tuned_epoch: 20
2023-09-09 08:52:17,769 [trainer.py] => init_lr: 0.04
2023-09-09 08:52:17,769 [trainer.py] => batch_size: 192
2023-09-09 08:52:17,769 [trainer.py] => weight_decay: 0.0005
2023-09-09 08:52:17,769 [trainer.py] => min_lr: 0
2023-09-09 08:52:17,769 [trainer.py] => ffn_num: 64
2023-09-09 08:52:17,769 [trainer.py] => optimizer: sgd
2023-09-09 08:52:17,769 [trainer.py] => vpt_type: shallow
2023-09-09 08:52:17,769 [trainer.py] => prompt_token_num: 5
2023-09-09 08:52:17,960 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 08:52:19,948 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 08:52:20,454 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 08:52:20,514 [trainer.py] => All params: 86988288
2023-09-09 08:52:20,515 [trainer.py] => Trainable params: 1189632
2023-09-09 08:52:20,669 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:12:14,945 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.341, Train_accy 90.22, Test_accy 92.17
2023-09-09 09:12:16,334 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:12:16,849 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:12:17,578 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:12:18,336 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:13:47,777 [trainer.py] => No NME accuracy.
2023-09-09 09:13:47,778 [trainer.py] => CNN: {'total': 89.33, '00-09': 89.0, '10-19': 90.5, '20-29': 88.5, 'old': 0, 'new': 89.33}
2023-09-09 09:13:47,778 [trainer.py] => CNN top1 curve: [89.33]
2023-09-09 09:13:47,778 [trainer.py] => CNN top5 curve: [99.0]

2023-09-09 09:13:47,778 [trainer.py] => Average Accuracy (CNN): 89.33
2023-09-09 09:13:47,780 [trainer.py] => All params: 172833025
2023-09-09 09:13:47,781 [trainer.py] => Trainable params: 87034369
2023-09-09 09:13:47,783 [adam_adapter.py] => Learning on 30-60
2023-09-09 09:15:16,497 [trainer.py] => No NME accuracy.
2023-09-09 09:15:16,497 [trainer.py] => CNN: {'total': 89.24, '00-09': 84.0, '10-19': 90.5, '20-29': 85.5, '30-39': 92.0, '40-49': 89.0, '50-59': 94.47, 'old': 86.67, 'new': 91.82}
2023-09-09 09:15:16,497 [trainer.py] => CNN top1 curve: [89.33, 89.24]
2023-09-09 09:15:16,497 [trainer.py] => CNN top5 curve: [99.0, 98.67]

2023-09-09 09:15:16,497 [trainer.py] => Average Accuracy (CNN): 89.285
2023-09-09 09:15:16,499 [trainer.py] => All params: 172879105
2023-09-09 09:15:16,500 [trainer.py] => Trainable params: 87080449
2023-09-09 09:15:16,501 [adam_adapter.py] => Learning on 60-90
2023-09-09 09:16:51,864 [trainer.py] => No NME accuracy.
2023-09-09 09:16:51,864 [trainer.py] => CNN: {'total': 86.6, '00-09': 81.5, '10-19': 84.5, '20-29': 82.0, '30-39': 89.5, '40-49': 87.0, '50-59': 93.47, '60-69': 93.5, '70-79': 88.0, '80-89': 79.9, 'old': 86.32, 'new': 87.15}
2023-09-09 09:16:51,864 [trainer.py] => CNN top1 curve: [89.33, 89.24, 86.6]
2023-09-09 09:16:51,864 [trainer.py] => CNN top5 curve: [99.0, 98.67, 97.55]

2023-09-09 09:16:51,864 [trainer.py] => Average Accuracy (CNN): 88.38999999999999
2023-09-09 09:16:51,866 [trainer.py] => All params: 172925185
2023-09-09 09:16:51,868 [trainer.py] => Trainable params: 87126529
2023-09-09 09:16:51,869 [adam_adapter.py] => Learning on 90-120
2023-09-09 09:18:37,363 [trainer.py] => No NME accuracy.
2023-09-09 09:18:37,363 [trainer.py] => CNN: {'total': 82.63, '00-09': 79.5, '10-19': 82.5, '20-29': 80.0, '30-39': 88.5, '40-49': 84.5, '50-59': 85.93, '60-69': 88.5, '70-79': 84.5, '80-89': 76.38, '90-99': 80.9, '100-109': 79.4, '110-119': 80.9, 'old': 83.37, 'new': 80.4}
2023-09-09 09:18:37,363 [trainer.py] => CNN top1 curve: [89.33, 89.24, 86.6, 82.63]
2023-09-09 09:18:37,363 [trainer.py] => CNN top5 curve: [99.0, 98.67, 97.55, 96.62]

2023-09-09 09:18:37,363 [trainer.py] => Average Accuracy (CNN): 86.94999999999999
2023-09-09 09:18:37,364 [trainer.py] => All params: 172971265
2023-09-09 09:18:37,365 [trainer.py] => Trainable params: 87172609
2023-09-09 09:18:37,367 [adam_adapter.py] => Learning on 120-150
2023-09-09 09:20:30,148 [trainer.py] => No NME accuracy.
2023-09-09 09:20:30,148 [trainer.py] => CNN: {'total': 80.46, '00-09': 77.5, '10-19': 81.5, '20-29': 78.5, '30-39': 87.5, '40-49': 81.5, '50-59': 80.9, '60-69': 85.0, '70-79': 83.5, '80-89': 74.37, '90-99': 79.9, '100-109': 76.38, '110-119': 75.38, '120-129': 84.0, '130-139': 81.0, '140-149': 79.9, 'old': 80.17, 'new': 81.64}
2023-09-09 09:20:30,148 [trainer.py] => CNN top1 curve: [89.33, 89.24, 86.6, 82.63, 80.46]
2023-09-09 09:20:30,148 [trainer.py] => CNN top5 curve: [99.0, 98.67, 97.55, 96.62, 95.26]

2023-09-09 09:20:30,148 [trainer.py] => Average Accuracy (CNN): 85.65199999999999
2023-09-09 09:20:30,149 [trainer.py] => All params: 173017345
2023-09-09 09:20:30,150 [trainer.py] => Trainable params: 87218689
2023-09-09 09:20:30,152 [adam_adapter.py] => Learning on 150-180
2023-09-09 09:22:24,060 [trainer.py] => No NME accuracy.
2023-09-09 09:22:24,060 [trainer.py] => CNN: {'total': 77.64, '00-09': 75.0, '10-19': 78.5, '20-29': 76.0, '30-39': 84.5, '40-49': 78.0, '50-59': 79.4, '60-69': 83.5, '70-79': 80.5, '80-89': 71.36, '90-99': 79.9, '100-109': 73.87, '110-119': 74.37, '120-129': 83.5, '130-139': 80.0, '140-149': 78.39, '150-159': 79.4, '160-169': 74.87, '170-179': 66.5, 'old': 78.46, 'new': 73.58}
2023-09-09 09:22:24,060 [trainer.py] => CNN top1 curve: [89.33, 89.24, 86.6, 82.63, 80.46, 77.64]
2023-09-09 09:22:24,060 [trainer.py] => CNN top5 curve: [99.0, 98.67, 97.55, 96.62, 95.26, 93.99]

2023-09-09 09:22:24,060 [trainer.py] => Average Accuracy (CNN): 84.31666666666665
2023-09-09 09:22:24,061 [trainer.py] => All params: 173063425
2023-09-09 09:22:24,062 [trainer.py] => Trainable params: 87264769
2023-09-09 09:22:24,064 [adam_adapter.py] => Learning on 180-210
2023-09-09 09:24:22,739 [trainer.py] => No NME accuracy.
2023-09-09 09:24:22,739 [trainer.py] => CNN: {'total': 76.89, '00-09': 74.5, '10-19': 78.5, '20-29': 76.0, '30-39': 84.5, '40-49': 76.5, '50-59': 78.89, '60-69': 81.0, '70-79': 80.0, '80-89': 71.36, '90-99': 77.39, '100-109': 73.37, '110-119': 70.85, '120-129': 83.5, '130-139': 79.0, '140-149': 78.39, '150-159': 75.88, '160-169': 74.37, '170-179': 66.5, '180-189': 75.76, '190-199': 84.0, '200-209': 74.37, 'old': 76.7, 'new': 78.06}
2023-09-09 09:24:22,740 [trainer.py] => CNN top1 curve: [89.33, 89.24, 86.6, 82.63, 80.46, 77.64, 76.89]
2023-09-09 09:24:22,740 [trainer.py] => CNN top5 curve: [99.0, 98.67, 97.55, 96.62, 95.26, 93.99, 93.55]

2023-09-09 09:24:22,740 [trainer.py] => Average Accuracy (CNN): 83.25571428571428
2023-09-09 09:24:22,741 [trainer.py] => All params: 173109505
2023-09-09 09:24:22,741 [trainer.py] => Trainable params: 87310849
2023-09-09 09:24:22,744 [adam_adapter.py] => Learning on 210-240
2023-09-09 09:26:27,994 [trainer.py] => No NME accuracy.
2023-09-09 09:26:27,995 [trainer.py] => CNN: {'total': 75.25, '00-09': 73.0, '10-19': 76.0, '20-29': 75.5, '30-39': 84.5, '40-49': 75.0, '50-59': 78.89, '60-69': 81.0, '70-79': 80.0, '80-89': 71.36, '90-99': 77.39, '100-109': 65.83, '110-119': 70.35, '120-129': 81.0, '130-139': 79.0, '140-149': 77.39, '150-159': 75.38, '160-169': 74.37, '170-179': 64.0, '180-189': 75.76, '190-199': 83.5, '200-209': 70.35, '210-219': 50.0, '220-229': 85.93, '230-239': 80.5, 'old': 75.7, 'new': 72.12}
2023-09-09 09:26:27,995 [trainer.py] => CNN top1 curve: [89.33, 89.24, 86.6, 82.63, 80.46, 77.64, 76.89, 75.25]
2023-09-09 09:26:27,995 [trainer.py] => CNN top5 curve: [99.0, 98.67, 97.55, 96.62, 95.26, 93.99, 93.55, 92.25]

2023-09-09 09:26:27,995 [trainer.py] => Average Accuracy (CNN): 82.255
2023-09-09 09:26:27,996 [trainer.py] => All params: 173155585
2023-09-09 09:26:27,996 [trainer.py] => Trainable params: 87356929
2023-09-09 09:26:27,999 [adam_adapter.py] => Learning on 240-270
2023-09-09 09:28:36,695 [trainer.py] => No NME accuracy.
2023-09-09 09:28:36,696 [trainer.py] => CNN: {'total': 74.35, '00-09': 73.0, '10-19': 76.0, '20-29': 75.5, '30-39': 83.5, '40-49': 75.0, '50-59': 78.39, '60-69': 79.5, '70-79': 79.5, '80-89': 70.35, '90-99': 75.88, '100-109': 64.82, '110-119': 66.83, '120-129': 75.5, '130-139': 78.0, '140-149': 73.37, '150-159': 72.86, '160-169': 74.37, '170-179': 64.0, '180-189': 75.76, '190-199': 82.0, '200-209': 67.34, '210-219': 50.0, '220-229': 85.43, '230-239': 80.5, '240-249': 70.0, '250-259': 76.88, '260-269': 83.0, 'old': 74.06, 'new': 76.63}
2023-09-09 09:28:36,696 [trainer.py] => CNN top1 curve: [89.33, 89.24, 86.6, 82.63, 80.46, 77.64, 76.89, 75.25, 74.35]
2023-09-09 09:28:36,696 [trainer.py] => CNN top5 curve: [99.0, 98.67, 97.55, 96.62, 95.26, 93.99, 93.55, 92.25, 92.39]

2023-09-09 09:28:36,696 [trainer.py] => Average Accuracy (CNN): 81.37666666666667
2023-09-09 09:28:36,697 [trainer.py] => All params: 173201665
2023-09-09 09:28:36,699 [trainer.py] => Trainable params: 87403009
2023-09-09 09:28:36,702 [adam_adapter.py] => Learning on 270-300
2023-09-09 09:30:45,013 [trainer.py] => No NME accuracy.
2023-09-09 09:30:45,013 [trainer.py] => CNN: {'total': 74.35, '00-09': 73.0, '10-19': 76.0, '20-29': 75.0, '30-39': 83.5, '40-49': 72.5, '50-59': 78.39, '60-69': 79.0, '70-79': 79.5, '80-89': 70.35, '90-99': 72.36, '100-109': 64.32, '110-119': 65.33, '120-129': 75.5, '130-139': 77.5, '140-149': 73.37, '150-159': 72.86, '160-169': 72.86, '170-179': 62.5, '180-189': 75.25, '190-199': 81.5, '200-209': 66.83, '210-219': 49.0, '220-229': 85.43, '230-239': 80.0, '240-249': 67.5, '250-259': 76.38, '260-269': 82.0, '270-279': 79.9, '280-289': 84.0, '290-299': 78.89, 'old': 73.62, 'new': 80.94}
2023-09-09 09:30:45,013 [trainer.py] => CNN top1 curve: [89.33, 89.24, 86.6, 82.63, 80.46, 77.64, 76.89, 75.25, 74.35, 74.35]
2023-09-09 09:30:45,013 [trainer.py] => CNN top5 curve: [99.0, 98.67, 97.55, 96.62, 95.26, 93.99, 93.55, 92.25, 92.39, 92.28]

2023-09-09 09:30:45,014 [trainer.py] => Average Accuracy (CNN): 80.674
2023-09-09 09:32:47,762 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:32:47,762 [trainer.py] => prefix:  
2023-09-09 09:32:47,762 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:32:47,762 [trainer.py] => memory_size: 0
2023-09-09 09:32:47,762 [trainer.py] => memory_per_class: 0
2023-09-09 09:32:47,762 [trainer.py] => fixed_memory: False
2023-09-09 09:32:47,762 [trainer.py] => shuffle: True
2023-09-09 09:32:47,762 [trainer.py] => init_cls: 30
2023-09-09 09:32:47,762 [trainer.py] => increment: 30
2023-09-09 09:32:47,762 [trainer.py] => model_name: adam_adapter
2023-09-09 09:32:47,763 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:32:47,763 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:32:47,763 [trainer.py] => seed: 1993
2023-09-09 09:32:47,763 [trainer.py] => tuned_epoch: 20
2023-09-09 09:32:47,763 [trainer.py] => init_lr: 0.02
2023-09-09 09:32:47,763 [trainer.py] => batch_size: 96
2023-09-09 09:32:47,763 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:32:47,763 [trainer.py] => min_lr: 0
2023-09-09 09:32:47,763 [trainer.py] => ffn_num: 64
2023-09-09 09:32:47,763 [trainer.py] => optimizer: sgd
2023-09-09 09:32:47,763 [trainer.py] => vpt_type: shallow
2023-09-09 09:32:47,763 [trainer.py] => prompt_token_num: 5
2023-09-09 09:32:47,954 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:32:49,795 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:32:50,288 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:32:50,359 [trainer.py] => All params: 86988288
2023-09-09 09:32:50,359 [trainer.py] => Trainable params: 1189632
2023-09-09 09:32:50,511 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:35:58,390 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:35:58,390 [trainer.py] => prefix:  
2023-09-09 09:35:58,390 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:35:58,390 [trainer.py] => memory_size: 0
2023-09-09 09:35:58,390 [trainer.py] => memory_per_class: 0
2023-09-09 09:35:58,391 [trainer.py] => fixed_memory: False
2023-09-09 09:35:58,391 [trainer.py] => shuffle: True
2023-09-09 09:35:58,391 [trainer.py] => init_cls: 30
2023-09-09 09:35:58,391 [trainer.py] => increment: 30
2023-09-09 09:35:58,391 [trainer.py] => model_name: adam_adapter
2023-09-09 09:35:58,391 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:35:58,391 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:35:58,391 [trainer.py] => seed: 1993
2023-09-09 09:35:58,391 [trainer.py] => tuned_epoch: 20
2023-09-09 09:35:58,391 [trainer.py] => init_lr: 0.02
2023-09-09 09:35:58,391 [trainer.py] => batch_size: 96
2023-09-09 09:35:58,392 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:35:58,392 [trainer.py] => min_lr: 0
2023-09-09 09:35:58,392 [trainer.py] => ffn_num: 64
2023-09-09 09:35:58,392 [trainer.py] => optimizer: sgd
2023-09-09 09:35:58,392 [trainer.py] => vpt_type: shallow
2023-09-09 09:35:58,392 [trainer.py] => prompt_token_num: 5
2023-09-09 09:35:59,086 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:36:01,540 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:36:02,046 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:36:02,154 [trainer.py] => All params: 86988288
2023-09-09 09:36:02,164 [trainer.py] => Trainable params: 1189632
2023-09-09 09:36:02,303 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:36:27,019 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:36:27,019 [trainer.py] => prefix:  
2023-09-09 09:36:27,020 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:36:27,020 [trainer.py] => memory_size: 0
2023-09-09 09:36:27,020 [trainer.py] => memory_per_class: 0
2023-09-09 09:36:27,020 [trainer.py] => fixed_memory: False
2023-09-09 09:36:27,020 [trainer.py] => shuffle: True
2023-09-09 09:36:27,020 [trainer.py] => init_cls: 30
2023-09-09 09:36:27,020 [trainer.py] => increment: 30
2023-09-09 09:36:27,020 [trainer.py] => model_name: adam_adapter
2023-09-09 09:36:27,020 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:36:27,020 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:36:27,020 [trainer.py] => seed: 1993
2023-09-09 09:36:27,021 [trainer.py] => tuned_epoch: 20
2023-09-09 09:36:27,021 [trainer.py] => init_lr: 0.02
2023-09-09 09:36:27,021 [trainer.py] => batch_size: 96
2023-09-09 09:36:27,021 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:36:27,021 [trainer.py] => min_lr: 0
2023-09-09 09:36:27,021 [trainer.py] => ffn_num: 64
2023-09-09 09:36:27,021 [trainer.py] => optimizer: sgd
2023-09-09 09:36:27,021 [trainer.py] => vpt_type: shallow
2023-09-09 09:36:27,021 [trainer.py] => prompt_token_num: 5
2023-09-09 09:36:27,722 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:36:30,206 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:36:30,723 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:36:30,837 [trainer.py] => All params: 86988288
2023-09-09 09:36:30,851 [trainer.py] => Trainable params: 1189632
2023-09-09 09:36:31,000 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:37:50,821 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:37:50,821 [trainer.py] => prefix:  
2023-09-09 09:37:50,821 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:37:50,821 [trainer.py] => memory_size: 0
2023-09-09 09:37:50,821 [trainer.py] => memory_per_class: 0
2023-09-09 09:37:50,821 [trainer.py] => fixed_memory: False
2023-09-09 09:37:50,821 [trainer.py] => shuffle: True
2023-09-09 09:37:50,821 [trainer.py] => init_cls: 30
2023-09-09 09:37:50,821 [trainer.py] => increment: 30
2023-09-09 09:37:50,822 [trainer.py] => model_name: adam_adapter
2023-09-09 09:37:50,822 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:37:50,822 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:37:50,822 [trainer.py] => seed: 1993
2023-09-09 09:37:50,822 [trainer.py] => tuned_epoch: 20
2023-09-09 09:37:50,822 [trainer.py] => init_lr: 0.02
2023-09-09 09:37:50,822 [trainer.py] => batch_size: 96
2023-09-09 09:37:50,822 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:37:50,822 [trainer.py] => min_lr: 0
2023-09-09 09:37:50,822 [trainer.py] => ffn_num: 64
2023-09-09 09:37:50,823 [trainer.py] => optimizer: sgd
2023-09-09 09:37:50,823 [trainer.py] => vpt_type: shallow
2023-09-09 09:37:50,823 [trainer.py] => prompt_token_num: 5
2023-09-09 09:37:51,509 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:37:53,995 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:37:54,590 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:37:54,701 [trainer.py] => All params: 86988288
2023-09-09 09:37:54,712 [trainer.py] => Trainable params: 1189632
2023-09-09 09:37:54,842 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:39:17,706 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:39:17,706 [trainer.py] => prefix:  
2023-09-09 09:39:17,706 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:39:17,706 [trainer.py] => memory_size: 0
2023-09-09 09:39:17,707 [trainer.py] => memory_per_class: 0
2023-09-09 09:39:17,707 [trainer.py] => fixed_memory: False
2023-09-09 09:39:17,707 [trainer.py] => shuffle: True
2023-09-09 09:39:17,707 [trainer.py] => init_cls: 30
2023-09-09 09:39:17,707 [trainer.py] => increment: 30
2023-09-09 09:39:17,707 [trainer.py] => model_name: adam_adapter
2023-09-09 09:39:17,707 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:39:17,707 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:39:17,707 [trainer.py] => seed: 1993
2023-09-09 09:39:17,707 [trainer.py] => tuned_epoch: 20
2023-09-09 09:39:17,707 [trainer.py] => init_lr: 0.02
2023-09-09 09:39:17,708 [trainer.py] => batch_size: 96
2023-09-09 09:39:17,708 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:39:17,708 [trainer.py] => min_lr: 0
2023-09-09 09:39:17,708 [trainer.py] => ffn_num: 64
2023-09-09 09:39:17,708 [trainer.py] => optimizer: sgd
2023-09-09 09:39:17,708 [trainer.py] => vpt_type: shallow
2023-09-09 09:39:17,708 [trainer.py] => prompt_token_num: 5
2023-09-09 09:39:18,418 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:39:20,838 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:39:21,353 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:39:21,473 [trainer.py] => All params: 86988288
2023-09-09 09:39:21,484 [trainer.py] => Trainable params: 1189632
2023-09-09 09:39:21,618 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:43:44,828 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:43:44,828 [trainer.py] => prefix:  
2023-09-09 09:43:44,828 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:43:44,829 [trainer.py] => memory_size: 0
2023-09-09 09:43:44,829 [trainer.py] => memory_per_class: 0
2023-09-09 09:43:44,829 [trainer.py] => fixed_memory: False
2023-09-09 09:43:44,829 [trainer.py] => shuffle: True
2023-09-09 09:43:44,829 [trainer.py] => init_cls: 30
2023-09-09 09:43:44,829 [trainer.py] => increment: 30
2023-09-09 09:43:44,829 [trainer.py] => model_name: adam_adapter
2023-09-09 09:43:44,829 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:43:44,829 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:43:44,829 [trainer.py] => seed: 1993
2023-09-09 09:43:44,829 [trainer.py] => tuned_epoch: 20
2023-09-09 09:43:44,829 [trainer.py] => init_lr: 0.02
2023-09-09 09:43:44,829 [trainer.py] => batch_size: 96
2023-09-09 09:43:44,829 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:43:44,829 [trainer.py] => min_lr: 0
2023-09-09 09:43:44,829 [trainer.py] => ffn_num: 64
2023-09-09 09:43:44,829 [trainer.py] => optimizer: sgd
2023-09-09 09:43:44,829 [trainer.py] => vpt_type: shallow
2023-09-09 09:43:44,829 [trainer.py] => prompt_token_num: 5
2023-09-09 09:43:45,023 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:43:46,901 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:43:47,403 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:43:47,473 [trainer.py] => All params: 86988288
2023-09-09 09:43:47,473 [trainer.py] => Trainable params: 1189632
2023-09-09 09:43:47,644 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:47:13,666 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:47:13,666 [trainer.py] => prefix:  
2023-09-09 09:47:13,666 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:47:13,666 [trainer.py] => memory_size: 0
2023-09-09 09:47:13,666 [trainer.py] => memory_per_class: 0
2023-09-09 09:47:13,666 [trainer.py] => fixed_memory: False
2023-09-09 09:47:13,666 [trainer.py] => shuffle: True
2023-09-09 09:47:13,666 [trainer.py] => init_cls: 30
2023-09-09 09:47:13,666 [trainer.py] => increment: 30
2023-09-09 09:47:13,666 [trainer.py] => model_name: adam_adapter
2023-09-09 09:47:13,666 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:47:13,666 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:47:13,666 [trainer.py] => seed: 1993
2023-09-09 09:47:13,666 [trainer.py] => tuned_epoch: 20
2023-09-09 09:47:13,666 [trainer.py] => init_lr: 0.02
2023-09-09 09:47:13,666 [trainer.py] => batch_size: 96
2023-09-09 09:47:13,666 [trainer.py] => use_A: True
2023-09-09 09:47:13,666 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:47:13,666 [trainer.py] => min_lr: 0
2023-09-09 09:47:13,666 [trainer.py] => ffn_num: 64
2023-09-09 09:47:13,666 [trainer.py] => optimizer: sgd
2023-09-09 09:47:13,666 [trainer.py] => vpt_type: shallow
2023-09-09 09:47:13,667 [trainer.py] => prompt_token_num: 5
2023-09-09 09:47:13,857 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:47:15,713 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:47:16,221 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:47:16,291 [trainer.py] => All params: 86988288
2023-09-09 09:47:16,292 [trainer.py] => Trainable params: 1189632
2023-09-09 09:47:16,431 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:49:21,717 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:49:21,717 [trainer.py] => prefix:  
2023-09-09 09:49:21,717 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:49:21,717 [trainer.py] => memory_size: 0
2023-09-09 09:49:21,717 [trainer.py] => memory_per_class: 0
2023-09-09 09:49:21,717 [trainer.py] => fixed_memory: False
2023-09-09 09:49:21,717 [trainer.py] => shuffle: True
2023-09-09 09:49:21,717 [trainer.py] => init_cls: 30
2023-09-09 09:49:21,717 [trainer.py] => increment: 30
2023-09-09 09:49:21,717 [trainer.py] => model_name: adam_adapter
2023-09-09 09:49:21,717 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:49:21,717 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:49:21,717 [trainer.py] => seed: 1993
2023-09-09 09:49:21,717 [trainer.py] => tuned_epoch: 20
2023-09-09 09:49:21,717 [trainer.py] => init_lr: 0.02
2023-09-09 09:49:21,717 [trainer.py] => batch_size: 96
2023-09-09 09:49:21,717 [trainer.py] => use_A: True
2023-09-09 09:49:21,717 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:49:21,717 [trainer.py] => min_lr: 0
2023-09-09 09:49:21,717 [trainer.py] => ffn_num: 64
2023-09-09 09:49:21,717 [trainer.py] => optimizer: sgd
2023-09-09 09:49:21,717 [trainer.py] => vpt_type: shallow
2023-09-09 09:49:21,717 [trainer.py] => prompt_token_num: 5
2023-09-09 09:49:21,909 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:49:23,753 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:49:24,263 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:49:24,324 [trainer.py] => All params: 86988288
2023-09-09 09:49:24,324 [trainer.py] => Trainable params: 1189632
2023-09-09 09:49:24,468 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:53:15,510 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:53:15,510 [trainer.py] => prefix:  
2023-09-09 09:53:15,510 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:53:15,510 [trainer.py] => memory_size: 0
2023-09-09 09:53:15,510 [trainer.py] => memory_per_class: 0
2023-09-09 09:53:15,510 [trainer.py] => fixed_memory: False
2023-09-09 09:53:15,510 [trainer.py] => shuffle: True
2023-09-09 09:53:15,510 [trainer.py] => init_cls: 30
2023-09-09 09:53:15,510 [trainer.py] => increment: 30
2023-09-09 09:53:15,510 [trainer.py] => model_name: adam_adapter
2023-09-09 09:53:15,510 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:53:15,510 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:53:15,510 [trainer.py] => seed: 1993
2023-09-09 09:53:15,510 [trainer.py] => tuned_epoch: 20
2023-09-09 09:53:15,510 [trainer.py] => init_lr: 0.02
2023-09-09 09:53:15,510 [trainer.py] => batch_size: 96
2023-09-09 09:53:15,510 [trainer.py] => use_A: True
2023-09-09 09:53:15,511 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:53:15,511 [trainer.py] => min_lr: 0
2023-09-09 09:53:15,511 [trainer.py] => ffn_num: 64
2023-09-09 09:53:15,511 [trainer.py] => optimizer: sgd
2023-09-09 09:53:15,511 [trainer.py] => vpt_type: shallow
2023-09-09 09:53:15,511 [trainer.py] => prompt_token_num: 5
2023-09-09 09:53:15,702 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:53:17,563 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:53:18,062 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:53:18,123 [trainer.py] => All params: 86988288
2023-09-09 09:53:18,123 [trainer.py] => Trainable params: 1189632
2023-09-09 09:53:18,254 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:53:30,250 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:53:30,250 [trainer.py] => prefix:  
2023-09-09 09:53:30,250 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:53:30,250 [trainer.py] => memory_size: 0
2023-09-09 09:53:30,250 [trainer.py] => memory_per_class: 0
2023-09-09 09:53:30,250 [trainer.py] => fixed_memory: False
2023-09-09 09:53:30,250 [trainer.py] => shuffle: True
2023-09-09 09:53:30,250 [trainer.py] => init_cls: 30
2023-09-09 09:53:30,251 [trainer.py] => increment: 30
2023-09-09 09:53:30,251 [trainer.py] => model_name: adam_adapter
2023-09-09 09:53:30,251 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:53:30,251 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:53:30,251 [trainer.py] => seed: 1993
2023-09-09 09:53:30,251 [trainer.py] => tuned_epoch: 20
2023-09-09 09:53:30,251 [trainer.py] => init_lr: 0.02
2023-09-09 09:53:30,251 [trainer.py] => batch_size: 96
2023-09-09 09:53:30,251 [trainer.py] => use_A: True
2023-09-09 09:53:30,251 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:53:30,251 [trainer.py] => min_lr: 0
2023-09-09 09:53:30,251 [trainer.py] => ffn_num: 64
2023-09-09 09:53:30,251 [trainer.py] => optimizer: sgd
2023-09-09 09:53:30,251 [trainer.py] => vpt_type: shallow
2023-09-09 09:53:30,251 [trainer.py] => prompt_token_num: 5
2023-09-09 09:53:30,444 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:53:32,295 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:53:32,791 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:53:32,860 [trainer.py] => All params: 86988288
2023-09-09 09:53:32,860 [trainer.py] => Trainable params: 1189632
2023-09-09 09:53:33,007 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:56:26,640 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:56:26,640 [trainer.py] => prefix:  
2023-09-09 09:56:26,640 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:56:26,640 [trainer.py] => memory_size: 0
2023-09-09 09:56:26,640 [trainer.py] => memory_per_class: 0
2023-09-09 09:56:26,640 [trainer.py] => fixed_memory: False
2023-09-09 09:56:26,640 [trainer.py] => shuffle: True
2023-09-09 09:56:26,640 [trainer.py] => init_cls: 30
2023-09-09 09:56:26,640 [trainer.py] => increment: 30
2023-09-09 09:56:26,640 [trainer.py] => model_name: adam_adapter
2023-09-09 09:56:26,640 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:56:26,640 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:56:26,640 [trainer.py] => seed: 1993
2023-09-09 09:56:26,640 [trainer.py] => tuned_epoch: 20
2023-09-09 09:56:26,640 [trainer.py] => init_lr: 0.02
2023-09-09 09:56:26,640 [trainer.py] => batch_size: 96
2023-09-09 09:56:26,640 [trainer.py] => use_A: True
2023-09-09 09:56:26,640 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:56:26,640 [trainer.py] => min_lr: 0
2023-09-09 09:56:26,640 [trainer.py] => ffn_num: 64
2023-09-09 09:56:26,640 [trainer.py] => optimizer: sgd
2023-09-09 09:56:26,640 [trainer.py] => vpt_type: shallow
2023-09-09 09:56:26,640 [trainer.py] => prompt_token_num: 5
2023-09-09 09:56:26,830 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:56:28,672 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:56:29,178 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:56:29,245 [trainer.py] => All params: 86988288
2023-09-09 09:56:29,246 [trainer.py] => Trainable params: 1189632
2023-09-09 09:56:29,386 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:58:24,824 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:58:24,824 [trainer.py] => prefix:  
2023-09-09 09:58:24,824 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:58:24,824 [trainer.py] => memory_size: 0
2023-09-09 09:58:24,824 [trainer.py] => memory_per_class: 0
2023-09-09 09:58:24,825 [trainer.py] => fixed_memory: False
2023-09-09 09:58:24,825 [trainer.py] => shuffle: True
2023-09-09 09:58:24,825 [trainer.py] => init_cls: 30
2023-09-09 09:58:24,825 [trainer.py] => increment: 30
2023-09-09 09:58:24,825 [trainer.py] => model_name: adam_adapter
2023-09-09 09:58:24,825 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:58:24,825 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:58:24,825 [trainer.py] => seed: 1993
2023-09-09 09:58:24,825 [trainer.py] => tuned_epoch: 20
2023-09-09 09:58:24,826 [trainer.py] => init_lr: 0.02
2023-09-09 09:58:24,826 [trainer.py] => batch_size: 96
2023-09-09 09:58:24,826 [trainer.py] => use_A: True
2023-09-09 09:58:24,826 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:58:24,826 [trainer.py] => min_lr: 0
2023-09-09 09:58:24,826 [trainer.py] => ffn_num: 64
2023-09-09 09:58:24,826 [trainer.py] => optimizer: sgd
2023-09-09 09:58:24,826 [trainer.py] => vpt_type: shallow
2023-09-09 09:58:24,826 [trainer.py] => prompt_token_num: 5
2023-09-09 09:58:25,512 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:58:27,978 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:58:28,537 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:58:28,651 [trainer.py] => All params: 86988288
2023-09-09 09:58:28,665 [trainer.py] => Trainable params: 1189632
2023-09-09 09:58:28,803 [adam_adapter.py] => Learning on 0-30
2023-09-09 09:59:17,229 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 09:59:17,229 [trainer.py] => prefix:  
2023-09-09 09:59:17,229 [trainer.py] => dataset: omnibenchmark
2023-09-09 09:59:17,229 [trainer.py] => memory_size: 0
2023-09-09 09:59:17,229 [trainer.py] => memory_per_class: 0
2023-09-09 09:59:17,229 [trainer.py] => fixed_memory: False
2023-09-09 09:59:17,229 [trainer.py] => shuffle: True
2023-09-09 09:59:17,229 [trainer.py] => init_cls: 30
2023-09-09 09:59:17,229 [trainer.py] => increment: 30
2023-09-09 09:59:17,229 [trainer.py] => model_name: adam_adapter
2023-09-09 09:59:17,229 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 09:59:17,229 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 09:59:17,229 [trainer.py] => seed: 1993
2023-09-09 09:59:17,229 [trainer.py] => tuned_epoch: 20
2023-09-09 09:59:17,229 [trainer.py] => init_lr: 0.02
2023-09-09 09:59:17,229 [trainer.py] => batch_size: 96
2023-09-09 09:59:17,229 [trainer.py] => use_A: True
2023-09-09 09:59:17,229 [trainer.py] => weight_decay: 0.0005
2023-09-09 09:59:17,229 [trainer.py] => min_lr: 0
2023-09-09 09:59:17,230 [trainer.py] => ffn_num: 64
2023-09-09 09:59:17,230 [trainer.py] => optimizer: sgd
2023-09-09 09:59:17,230 [trainer.py] => vpt_type: shallow
2023-09-09 09:59:17,230 [trainer.py] => prompt_token_num: 5
2023-09-09 09:59:17,424 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 09:59:19,349 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 09:59:19,851 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 09:59:19,925 [trainer.py] => All params: 86988288
2023-09-09 09:59:19,926 [trainer.py] => Trainable params: 1189632
2023-09-09 09:59:20,065 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:00:38,819 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 10:00:38,819 [trainer.py] => prefix:  
2023-09-09 10:00:38,820 [trainer.py] => dataset: omnibenchmark
2023-09-09 10:00:38,820 [trainer.py] => memory_size: 0
2023-09-09 10:00:38,820 [trainer.py] => memory_per_class: 0
2023-09-09 10:00:38,820 [trainer.py] => fixed_memory: False
2023-09-09 10:00:38,820 [trainer.py] => shuffle: True
2023-09-09 10:00:38,820 [trainer.py] => init_cls: 30
2023-09-09 10:00:38,820 [trainer.py] => increment: 30
2023-09-09 10:00:38,820 [trainer.py] => model_name: adam_adapter
2023-09-09 10:00:38,820 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 10:00:38,821 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 10:00:38,821 [trainer.py] => seed: 1993
2023-09-09 10:00:38,821 [trainer.py] => tuned_epoch: 20
2023-09-09 10:00:38,821 [trainer.py] => init_lr: 0.02
2023-09-09 10:00:38,821 [trainer.py] => batch_size: 96
2023-09-09 10:00:38,821 [trainer.py] => use_A: True
2023-09-09 10:00:38,821 [trainer.py] => weight_decay: 0.0005
2023-09-09 10:00:38,821 [trainer.py] => min_lr: 0
2023-09-09 10:00:38,821 [trainer.py] => ffn_num: 64
2023-09-09 10:00:38,822 [trainer.py] => optimizer: sgd
2023-09-09 10:00:38,822 [trainer.py] => vpt_type: shallow
2023-09-09 10:00:38,822 [trainer.py] => prompt_token_num: 5
2023-09-09 10:00:39,528 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 10:00:41,995 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:00:42,509 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:00:42,623 [trainer.py] => All params: 86988288
2023-09-09 10:00:42,637 [trainer.py] => Trainable params: 1189632
2023-09-09 10:00:42,776 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:02:03,554 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 10:02:03,554 [trainer.py] => prefix:  
2023-09-09 10:02:03,554 [trainer.py] => dataset: omnibenchmark
2023-09-09 10:02:03,554 [trainer.py] => memory_size: 0
2023-09-09 10:02:03,554 [trainer.py] => memory_per_class: 0
2023-09-09 10:02:03,554 [trainer.py] => fixed_memory: False
2023-09-09 10:02:03,554 [trainer.py] => shuffle: True
2023-09-09 10:02:03,554 [trainer.py] => init_cls: 30
2023-09-09 10:02:03,554 [trainer.py] => increment: 30
2023-09-09 10:02:03,554 [trainer.py] => model_name: adam_adapter
2023-09-09 10:02:03,554 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 10:02:03,554 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 10:02:03,554 [trainer.py] => seed: 1993
2023-09-09 10:02:03,554 [trainer.py] => tuned_epoch: 20
2023-09-09 10:02:03,554 [trainer.py] => init_lr: 0.02
2023-09-09 10:02:03,554 [trainer.py] => batch_size: 96
2023-09-09 10:02:03,554 [trainer.py] => use_A: True
2023-09-09 10:02:03,554 [trainer.py] => weight_decay: 0.0005
2023-09-09 10:02:03,554 [trainer.py] => min_lr: 0
2023-09-09 10:02:03,554 [trainer.py] => ffn_num: 64
2023-09-09 10:02:03,555 [trainer.py] => optimizer: sgd
2023-09-09 10:02:03,555 [trainer.py] => vpt_type: shallow
2023-09-09 10:02:03,555 [trainer.py] => prompt_token_num: 5
2023-09-09 10:02:03,746 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 10:02:05,636 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:02:06,141 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:02:06,207 [trainer.py] => All params: 86988288
2023-09-09 10:02:06,208 [trainer.py] => Trainable params: 1189632
2023-09-09 10:02:06,349 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:05:27,271 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 10:05:27,271 [trainer.py] => prefix:  
2023-09-09 10:05:27,271 [trainer.py] => dataset: omnibenchmark
2023-09-09 10:05:27,271 [trainer.py] => memory_size: 0
2023-09-09 10:05:27,271 [trainer.py] => memory_per_class: 0
2023-09-09 10:05:27,271 [trainer.py] => fixed_memory: False
2023-09-09 10:05:27,271 [trainer.py] => shuffle: True
2023-09-09 10:05:27,271 [trainer.py] => init_cls: 30
2023-09-09 10:05:27,271 [trainer.py] => increment: 30
2023-09-09 10:05:27,271 [trainer.py] => model_name: adam_adapter
2023-09-09 10:05:27,271 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 10:05:27,271 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 10:05:27,271 [trainer.py] => seed: 1993
2023-09-09 10:05:27,271 [trainer.py] => tuned_epoch: 20
2023-09-09 10:05:27,271 [trainer.py] => init_lr: 0.02
2023-09-09 10:05:27,271 [trainer.py] => batch_size: 96
2023-09-09 10:05:27,271 [trainer.py] => use_A: True
2023-09-09 10:05:27,271 [trainer.py] => weight_decay: 0.0005
2023-09-09 10:05:27,271 [trainer.py] => min_lr: 0
2023-09-09 10:05:27,271 [trainer.py] => ffn_num: 64
2023-09-09 10:05:27,271 [trainer.py] => optimizer: sgd
2023-09-09 10:05:27,271 [trainer.py] => vpt_type: shallow
2023-09-09 10:05:27,271 [trainer.py] => prompt_token_num: 5
2023-09-09 10:05:27,462 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 10:05:29,336 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:05:29,838 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:05:29,900 [trainer.py] => All params: 86988288
2023-09-09 10:05:29,901 [trainer.py] => Trainable params: 1189632
2023-09-09 10:05:30,036 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:06:48,666 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 10:06:48,666 [trainer.py] => prefix:  
2023-09-09 10:06:48,666 [trainer.py] => dataset: omnibenchmark
2023-09-09 10:06:48,666 [trainer.py] => memory_size: 0
2023-09-09 10:06:48,666 [trainer.py] => memory_per_class: 0
2023-09-09 10:06:48,666 [trainer.py] => fixed_memory: False
2023-09-09 10:06:48,666 [trainer.py] => shuffle: True
2023-09-09 10:06:48,666 [trainer.py] => init_cls: 30
2023-09-09 10:06:48,666 [trainer.py] => increment: 30
2023-09-09 10:06:48,666 [trainer.py] => model_name: adam_adapter
2023-09-09 10:06:48,666 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 10:06:48,666 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 10:06:48,666 [trainer.py] => seed: 1993
2023-09-09 10:06:48,666 [trainer.py] => tuned_epoch: 1
2023-09-09 10:06:48,666 [trainer.py] => init_lr: 0.02
2023-09-09 10:06:48,666 [trainer.py] => batch_size: 96
2023-09-09 10:06:48,667 [trainer.py] => use_A: True
2023-09-09 10:06:48,667 [trainer.py] => weight_decay: 0.0005
2023-09-09 10:06:48,667 [trainer.py] => min_lr: 0
2023-09-09 10:06:48,667 [trainer.py] => ffn_num: 64
2023-09-09 10:06:48,667 [trainer.py] => optimizer: sgd
2023-09-09 10:06:48,667 [trainer.py] => vpt_type: shallow
2023-09-09 10:06:48,667 [trainer.py] => prompt_token_num: 5
2023-09-09 10:06:48,858 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 10:06:50,724 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:06:51,221 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:06:51,289 [trainer.py] => All params: 86988288
2023-09-09 10:06:51,289 [trainer.py] => Trainable params: 1189632
2023-09-09 10:06:51,437 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:07:50,477 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.027, Train_accy 40.32, Test_accy 70.83
2023-09-09 10:07:51,579 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:07:51,838 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:07:52,541 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:07:52,791 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:09:03,769 [trainer.py] => No NME accuracy.
2023-09-09 10:09:03,769 [trainer.py] => CNN: {'total': 82.67, '00-09': 87.0, '10-19': 82.5, '20-29': 78.5, 'old': 0, 'new': 82.67}
2023-09-09 10:09:03,770 [trainer.py] => CNN top1 curve: [82.67]
2023-09-09 10:09:03,770 [trainer.py] => CNN top5 curve: [97.67]

2023-09-09 10:09:03,770 [trainer.py] => Average Accuracy (CNN): 82.67
2023-09-09 10:09:03,771 [trainer.py] => All params: 172833025
2023-09-09 10:09:03,773 [trainer.py] => Trainable params: 87034369
2023-09-09 10:09:03,774 [adam_adapter.py] => Learning on 30-60
2023-09-09 10:10:19,360 [trainer.py] => No NME accuracy.
2023-09-09 10:10:19,361 [trainer.py] => CNN: {'total': 80.15, '00-09': 82.0, '10-19': 79.0, '20-29': 72.5, '30-39': 79.0, '40-49': 80.0, '50-59': 88.44, 'old': 77.83, 'new': 82.47}
2023-09-09 10:10:19,361 [trainer.py] => CNN top1 curve: [82.67, 80.15]
2023-09-09 10:10:19,361 [trainer.py] => CNN top5 curve: [97.67, 95.91]

2023-09-09 10:10:19,361 [trainer.py] => Average Accuracy (CNN): 81.41
2023-09-09 10:10:19,362 [trainer.py] => All params: 172879105
2023-09-09 10:10:19,363 [trainer.py] => Trainable params: 87080449
2023-09-09 10:10:19,364 [adam_adapter.py] => Learning on 60-90
2023-09-09 10:10:35,792 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 10:10:35,792 [trainer.py] => prefix:  
2023-09-09 10:10:35,792 [trainer.py] => dataset: omnibenchmark
2023-09-09 10:10:35,792 [trainer.py] => memory_size: 0
2023-09-09 10:10:35,792 [trainer.py] => memory_per_class: 0
2023-09-09 10:10:35,792 [trainer.py] => fixed_memory: False
2023-09-09 10:10:35,792 [trainer.py] => shuffle: True
2023-09-09 10:10:35,792 [trainer.py] => init_cls: 30
2023-09-09 10:10:35,792 [trainer.py] => increment: 30
2023-09-09 10:10:35,792 [trainer.py] => model_name: adam_adapter
2023-09-09 10:10:35,792 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 10:10:35,792 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 10:10:35,792 [trainer.py] => seed: 1993
2023-09-09 10:10:35,792 [trainer.py] => tuned_epoch: 20
2023-09-09 10:10:35,792 [trainer.py] => init_lr: 0.02
2023-09-09 10:10:35,792 [trainer.py] => batch_size: 96
2023-09-09 10:10:35,792 [trainer.py] => use_A: True
2023-09-09 10:10:35,792 [trainer.py] => weight_decay: 0.0005
2023-09-09 10:10:35,792 [trainer.py] => min_lr: 0
2023-09-09 10:10:35,793 [trainer.py] => ffn_num: 64
2023-09-09 10:10:35,793 [trainer.py] => optimizer: sgd
2023-09-09 10:10:35,793 [trainer.py] => vpt_type: shallow
2023-09-09 10:10:35,793 [trainer.py] => prompt_token_num: 5
2023-09-09 10:10:35,985 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 10:10:37,930 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:10:38,431 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:10:38,501 [trainer.py] => All params: 86988288
2023-09-09 10:10:38,501 [trainer.py] => Trainable params: 1189632
2023-09-09 10:10:38,650 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:13:41,853 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 10:13:41,853 [trainer.py] => prefix:  
2023-09-09 10:13:41,853 [trainer.py] => dataset: omnibenchmark
2023-09-09 10:13:41,853 [trainer.py] => memory_size: 0
2023-09-09 10:13:41,853 [trainer.py] => memory_per_class: 0
2023-09-09 10:13:41,853 [trainer.py] => fixed_memory: False
2023-09-09 10:13:41,853 [trainer.py] => shuffle: True
2023-09-09 10:13:41,853 [trainer.py] => init_cls: 30
2023-09-09 10:13:41,853 [trainer.py] => increment: 30
2023-09-09 10:13:41,853 [trainer.py] => model_name: adam_adapter
2023-09-09 10:13:41,853 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 10:13:41,853 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 10:13:41,853 [trainer.py] => seed: 1993
2023-09-09 10:13:41,853 [trainer.py] => tuned_epoch: 1
2023-09-09 10:13:41,853 [trainer.py] => init_lr: 0.02
2023-09-09 10:13:41,853 [trainer.py] => batch_size: 96
2023-09-09 10:13:41,853 [trainer.py] => use_A: True
2023-09-09 10:13:41,853 [trainer.py] => weight_decay: 0.0005
2023-09-09 10:13:41,853 [trainer.py] => min_lr: 0
2023-09-09 10:13:41,853 [trainer.py] => ffn_num: 64
2023-09-09 10:13:41,853 [trainer.py] => optimizer: sgd
2023-09-09 10:13:41,853 [trainer.py] => vpt_type: shallow
2023-09-09 10:13:41,853 [trainer.py] => prompt_token_num: 5
2023-09-09 10:13:42,044 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 10:13:43,879 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:13:44,426 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:13:44,496 [trainer.py] => All params: 86988288
2023-09-09 10:13:44,496 [trainer.py] => Trainable params: 1189632
2023-09-09 10:13:44,645 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:19:13,056 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 10:19:13,056 [trainer.py] => prefix:  
2023-09-09 10:19:13,056 [trainer.py] => dataset: omnibenchmark
2023-09-09 10:19:13,056 [trainer.py] => memory_size: 0
2023-09-09 10:19:13,056 [trainer.py] => memory_per_class: 0
2023-09-09 10:19:13,056 [trainer.py] => fixed_memory: False
2023-09-09 10:19:13,056 [trainer.py] => shuffle: True
2023-09-09 10:19:13,056 [trainer.py] => init_cls: 30
2023-09-09 10:19:13,056 [trainer.py] => increment: 30
2023-09-09 10:19:13,056 [trainer.py] => model_name: adam_adapter
2023-09-09 10:19:13,056 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 10:19:13,056 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 10:19:13,056 [trainer.py] => seed: 1993
2023-09-09 10:19:13,056 [trainer.py] => tuned_epoch: 1
2023-09-09 10:19:13,056 [trainer.py] => init_lr: 0.02
2023-09-09 10:19:13,056 [trainer.py] => batch_size: 96
2023-09-09 10:19:13,056 [trainer.py] => use_A: True
2023-09-09 10:19:13,056 [trainer.py] => weight_decay: 0.0005
2023-09-09 10:19:13,056 [trainer.py] => min_lr: 0
2023-09-09 10:19:13,056 [trainer.py] => ffn_num: 64
2023-09-09 10:19:13,056 [trainer.py] => optimizer: sgd
2023-09-09 10:19:13,056 [trainer.py] => vpt_type: shallow
2023-09-09 10:19:13,056 [trainer.py] => prompt_token_num: 5
2023-09-09 10:19:13,248 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 10:19:15,107 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:19:15,614 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:19:15,674 [trainer.py] => All params: 86988288
2023-09-09 10:19:15,675 [trainer.py] => Trainable params: 1189632
2023-09-09 10:19:15,809 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:20:24,838 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 10:20:24,838 [trainer.py] => prefix:  
2023-09-09 10:20:24,838 [trainer.py] => dataset: omnibenchmark
2023-09-09 10:20:24,838 [trainer.py] => memory_size: 0
2023-09-09 10:20:24,838 [trainer.py] => memory_per_class: 0
2023-09-09 10:20:24,838 [trainer.py] => fixed_memory: False
2023-09-09 10:20:24,838 [trainer.py] => shuffle: True
2023-09-09 10:20:24,838 [trainer.py] => init_cls: 30
2023-09-09 10:20:24,838 [trainer.py] => increment: 30
2023-09-09 10:20:24,838 [trainer.py] => model_name: adam_adapter
2023-09-09 10:20:24,838 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 10:20:24,838 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 10:20:24,838 [trainer.py] => seed: 1993
2023-09-09 10:20:24,838 [trainer.py] => tuned_epoch: 1
2023-09-09 10:20:24,838 [trainer.py] => init_lr: 0.02
2023-09-09 10:20:24,838 [trainer.py] => batch_size: 96
2023-09-09 10:20:24,838 [trainer.py] => use_A: True
2023-09-09 10:20:24,838 [trainer.py] => weight_decay: 0.0005
2023-09-09 10:20:24,838 [trainer.py] => min_lr: 0
2023-09-09 10:20:24,838 [trainer.py] => ffn_num: 64
2023-09-09 10:20:24,838 [trainer.py] => optimizer: sgd
2023-09-09 10:20:24,838 [trainer.py] => vpt_type: shallow
2023-09-09 10:20:24,838 [trainer.py] => prompt_token_num: 5
2023-09-09 10:20:25,029 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 10:20:26,863 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:20:27,372 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:20:27,433 [trainer.py] => All params: 86988288
2023-09-09 10:20:27,434 [trainer.py] => Trainable params: 1189632
2023-09-09 10:20:27,592 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:21:27,062 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 3.027, Train_accy 40.32, Test_accy 80.50
2023-09-09 10:21:28,177 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:21:28,438 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:21:29,109 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:21:29,399 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:22:49,777 [trainer.py] => No NME accuracy.
2023-09-09 10:22:49,778 [trainer.py] => CNN: {'total': 87.0, '00-09': 90.5, '10-19': 87.5, '20-29': 83.0, 'old': 0, 'new': 87.0}
2023-09-09 10:22:49,778 [trainer.py] => CNN top1 curve: [87.0]
2023-09-09 10:22:49,778 [trainer.py] => CNN top5 curve: [98.83]

2023-09-09 10:22:49,778 [trainer.py] => Average Accuracy (CNN): 87.0
2023-09-09 10:22:49,779 [trainer.py] => All params: 172833025
2023-09-09 10:22:49,781 [trainer.py] => Trainable params: 87034369
2023-09-09 10:22:49,783 [adam_adapter.py] => Learning on 30-60
2023-09-09 10:23:00,899 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-09 10:23:00,899 [trainer.py] => prefix:  
2023-09-09 10:23:00,899 [trainer.py] => dataset: omnibenchmark
2023-09-09 10:23:00,899 [trainer.py] => memory_size: 0
2023-09-09 10:23:00,899 [trainer.py] => memory_per_class: 0
2023-09-09 10:23:00,899 [trainer.py] => fixed_memory: False
2023-09-09 10:23:00,899 [trainer.py] => shuffle: True
2023-09-09 10:23:00,899 [trainer.py] => init_cls: 30
2023-09-09 10:23:00,899 [trainer.py] => increment: 30
2023-09-09 10:23:00,899 [trainer.py] => model_name: adam_adapter
2023-09-09 10:23:00,899 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-09 10:23:00,899 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-09 10:23:00,899 [trainer.py] => seed: 1993
2023-09-09 10:23:00,899 [trainer.py] => tuned_epoch: 20
2023-09-09 10:23:00,899 [trainer.py] => init_lr: 0.02
2023-09-09 10:23:00,899 [trainer.py] => batch_size: 96
2023-09-09 10:23:00,899 [trainer.py] => use_A: True
2023-09-09 10:23:00,899 [trainer.py] => weight_decay: 0.0005
2023-09-09 10:23:00,899 [trainer.py] => min_lr: 0
2023-09-09 10:23:00,899 [trainer.py] => ffn_num: 64
2023-09-09 10:23:00,900 [trainer.py] => optimizer: sgd
2023-09-09 10:23:00,900 [trainer.py] => vpt_type: shallow
2023-09-09 10:23:00,900 [trainer.py] => prompt_token_num: 5
2023-09-09 10:23:01,091 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-09 10:23:02,904 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:23:03,399 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:23:03,472 [trainer.py] => All params: 86988288
2023-09-09 10:23:03,473 [trainer.py] => Trainable params: 1189632
2023-09-09 10:23:03,610 [adam_adapter.py] => Learning on 0-30
2023-09-09 10:43:02,361 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.396, Train_accy 88.48, Test_accy 90.67
2023-09-09 10:43:03,458 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:43:04,643 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:43:05,332 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-09 10:43:05,586 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-09 10:44:25,696 [trainer.py] => No NME accuracy.
2023-09-09 10:44:25,696 [trainer.py] => CNN: {'total': 89.17, '00-09': 91.5, '10-19': 90.5, '20-29': 85.5, 'old': 0, 'new': 89.17}
2023-09-09 10:44:25,696 [trainer.py] => CNN top1 curve: [89.17]
2023-09-09 10:44:25,696 [trainer.py] => CNN top5 curve: [98.83]

2023-09-09 10:44:25,696 [trainer.py] => Average Accuracy (CNN): 89.17
2023-09-09 10:44:25,698 [trainer.py] => All params: 172833025
2023-09-09 10:44:25,699 [trainer.py] => Trainable params: 87034369
2023-09-09 10:44:25,701 [adam_adapter.py] => Learning on 30-60
2023-09-09 10:45:52,507 [trainer.py] => No NME accuracy.
2023-09-09 10:45:52,507 [trainer.py] => CNN: {'total': 88.74, '00-09': 86.0, '10-19': 89.5, '20-29': 82.5, '30-39': 91.5, '40-49': 88.5, '50-59': 94.47, 'old': 86.0, 'new': 91.49}
2023-09-09 10:45:52,507 [trainer.py] => CNN top1 curve: [89.17, 88.74]
2023-09-09 10:45:52,507 [trainer.py] => CNN top5 curve: [98.83, 98.17]

2023-09-09 10:45:52,507 [trainer.py] => Average Accuracy (CNN): 88.955
2023-09-09 10:45:52,509 [trainer.py] => All params: 172879105
2023-09-09 10:45:52,510 [trainer.py] => Trainable params: 87080449
2023-09-09 10:45:52,512 [adam_adapter.py] => Learning on 60-90
2023-09-09 10:47:22,718 [trainer.py] => No NME accuracy.
2023-09-09 10:47:22,719 [trainer.py] => CNN: {'total': 86.04, '00-09': 83.0, '10-19': 85.0, '20-29': 80.0, '30-39': 90.0, '40-49': 86.5, '50-59': 92.96, '60-69': 92.5, '70-79': 86.0, '80-89': 78.39, 'old': 86.24, 'new': 85.64}
2023-09-09 10:47:22,720 [trainer.py] => CNN top1 curve: [89.17, 88.74, 86.04]
2023-09-09 10:47:22,720 [trainer.py] => CNN top5 curve: [98.83, 98.17, 97.27]

2023-09-09 10:47:22,720 [trainer.py] => Average Accuracy (CNN): 87.98333333333333
2023-09-09 10:47:22,722 [trainer.py] => All params: 172925185
2023-09-09 10:47:22,723 [trainer.py] => Trainable params: 87126529
2023-09-09 10:47:22,726 [adam_adapter.py] => Learning on 90-120
2023-09-09 10:49:00,994 [trainer.py] => No NME accuracy.
2023-09-09 10:49:00,995 [trainer.py] => CNN: {'total': 82.3, '00-09': 81.0, '10-19': 82.5, '20-29': 79.5, '30-39': 89.0, '40-49': 84.5, '50-59': 86.93, '60-69': 87.5, '70-79': 83.5, '80-89': 76.38, '90-99': 80.9, '100-109': 78.39, '110-119': 77.39, 'old': 83.43, 'new': 78.89}
2023-09-09 10:49:00,995 [trainer.py] => CNN top1 curve: [89.17, 88.74, 86.04, 82.3]
2023-09-09 10:49:00,995 [trainer.py] => CNN top5 curve: [98.83, 98.17, 97.27, 96.08]

2023-09-09 10:49:00,995 [trainer.py] => Average Accuracy (CNN): 86.5625
2023-09-09 10:49:00,996 [trainer.py] => All params: 172971265
2023-09-09 10:49:00,997 [trainer.py] => Trainable params: 87172609
2023-09-09 10:49:00,998 [adam_adapter.py] => Learning on 120-150
2023-09-09 10:50:45,543 [trainer.py] => No NME accuracy.
2023-09-09 10:50:45,543 [trainer.py] => CNN: {'total': 80.29, '00-09': 78.0, '10-19': 82.0, '20-29': 78.5, '30-39': 87.5, '40-49': 82.0, '50-59': 80.9, '60-69': 84.5, '70-79': 83.0, '80-89': 74.87, '90-99': 80.4, '100-109': 75.38, '110-119': 71.86, '120-129': 85.0, '130-139': 79.5, '140-149': 80.9, 'old': 79.92, 'new': 81.8}
2023-09-09 10:50:45,543 [trainer.py] => CNN top1 curve: [89.17, 88.74, 86.04, 82.3, 80.29]
2023-09-09 10:50:45,543 [trainer.py] => CNN top5 curve: [98.83, 98.17, 97.27, 96.08, 94.89]

2023-09-09 10:50:45,544 [trainer.py] => Average Accuracy (CNN): 85.308
2023-09-09 10:50:45,545 [trainer.py] => All params: 173017345
2023-09-09 10:50:45,547 [trainer.py] => Trainable params: 87218689
2023-09-09 10:50:45,549 [adam_adapter.py] => Learning on 150-180
2023-09-09 10:52:30,318 [trainer.py] => No NME accuracy.
2023-09-09 10:52:30,318 [trainer.py] => CNN: {'total': 77.56, '00-09': 75.5, '10-19': 79.0, '20-29': 76.0, '30-39': 84.5, '40-49': 78.5, '50-59': 79.4, '60-69': 83.0, '70-79': 80.0, '80-89': 72.86, '90-99': 80.4, '100-109': 73.37, '110-119': 70.35, '120-129': 84.5, '130-139': 79.5, '140-149': 79.9, '150-159': 79.9, '160-169': 72.86, '170-179': 66.5, 'old': 78.46, 'new': 73.08}
2023-09-09 10:52:30,318 [trainer.py] => CNN top1 curve: [89.17, 88.74, 86.04, 82.3, 80.29, 77.56]
2023-09-09 10:52:30,318 [trainer.py] => CNN top5 curve: [98.83, 98.17, 97.27, 96.08, 94.89, 93.76]

2023-09-09 10:52:30,318 [trainer.py] => Average Accuracy (CNN): 84.01666666666667
2023-09-09 10:52:30,319 [trainer.py] => All params: 173063425
2023-09-09 10:52:30,320 [trainer.py] => Trainable params: 87264769
2023-09-09 10:52:30,322 [adam_adapter.py] => Learning on 180-210
2023-09-09 10:54:18,448 [trainer.py] => No NME accuracy.
2023-09-09 10:54:18,448 [trainer.py] => CNN: {'total': 76.53, '00-09': 74.5, '10-19': 79.0, '20-29': 75.5, '30-39': 84.5, '40-49': 76.5, '50-59': 78.89, '60-69': 81.5, '70-79': 80.0, '80-89': 72.36, '90-99': 77.39, '100-109': 73.37, '110-119': 66.83, '120-129': 84.0, '130-139': 78.5, '140-149': 79.9, '150-159': 76.88, '160-169': 71.86, '170-179': 66.0, '180-189': 74.75, '190-199': 82.5, '200-209': 72.36, 'old': 76.53, 'new': 76.55}
2023-09-09 10:54:18,448 [trainer.py] => CNN top1 curve: [89.17, 88.74, 86.04, 82.3, 80.29, 77.56, 76.53]
2023-09-09 10:54:18,448 [trainer.py] => CNN top5 curve: [98.83, 98.17, 97.27, 96.08, 94.89, 93.76, 93.34]

2023-09-09 10:54:18,449 [trainer.py] => Average Accuracy (CNN): 82.94714285714285
2023-09-09 10:54:18,449 [trainer.py] => All params: 173109505
2023-09-09 10:54:18,450 [trainer.py] => Trainable params: 87310849
2023-09-09 10:54:18,452 [adam_adapter.py] => Learning on 210-240
2023-09-09 10:56:14,643 [trainer.py] => No NME accuracy.
2023-09-09 10:56:14,643 [trainer.py] => CNN: {'total': 74.87, '00-09': 73.5, '10-19': 77.0, '20-29': 74.0, '30-39': 84.5, '40-49': 74.5, '50-59': 78.89, '60-69': 81.0, '70-79': 80.0, '80-89': 72.36, '90-99': 77.39, '100-109': 64.82, '110-119': 65.83, '120-129': 81.0, '130-139': 78.5, '140-149': 79.4, '150-159': 76.38, '160-169': 71.36, '170-179': 64.0, '180-189': 73.74, '190-199': 82.5, '200-209': 68.84, '210-219': 50.5, '220-229': 86.43, '230-239': 80.5, 'old': 75.22, 'new': 72.45}
2023-09-09 10:56:14,643 [trainer.py] => CNN top1 curve: [89.17, 88.74, 86.04, 82.3, 80.29, 77.56, 76.53, 74.87]
2023-09-09 10:56:14,643 [trainer.py] => CNN top5 curve: [98.83, 98.17, 97.27, 96.08, 94.89, 93.76, 93.34, 92.02]

2023-09-09 10:56:14,643 [trainer.py] => Average Accuracy (CNN): 81.9375
2023-09-09 10:56:14,644 [trainer.py] => All params: 173155585
2023-09-09 10:56:14,645 [trainer.py] => Trainable params: 87356929
2023-09-09 10:56:14,647 [adam_adapter.py] => Learning on 240-270
2023-09-09 10:58:14,425 [trainer.py] => No NME accuracy.
2023-09-09 10:58:14,425 [trainer.py] => CNN: {'total': 73.99, '00-09': 73.0, '10-19': 76.0, '20-29': 74.0, '30-39': 84.0, '40-49': 74.5, '50-59': 78.39, '60-69': 79.5, '70-79': 79.5, '80-89': 71.86, '90-99': 75.88, '100-109': 63.32, '110-119': 62.81, '120-129': 75.5, '130-139': 77.5, '140-149': 74.37, '150-159': 73.87, '160-169': 71.36, '170-179': 64.0, '180-189': 73.74, '190-199': 81.0, '200-209': 65.83, '210-219': 50.5, '220-229': 85.93, '230-239': 80.5, '240-249': 71.0, '250-259': 76.88, '260-269': 83.0, 'old': 73.62, 'new': 76.96}
2023-09-09 10:58:14,425 [trainer.py] => CNN top1 curve: [89.17, 88.74, 86.04, 82.3, 80.29, 77.56, 76.53, 74.87, 73.99]
2023-09-09 10:58:14,425 [trainer.py] => CNN top5 curve: [98.83, 98.17, 97.27, 96.08, 94.89, 93.76, 93.34, 92.02, 92.07]

2023-09-09 10:58:14,426 [trainer.py] => Average Accuracy (CNN): 81.05444444444444
2023-09-09 10:58:14,427 [trainer.py] => All params: 173201665
2023-09-09 10:58:14,429 [trainer.py] => Trainable params: 87403009
2023-09-09 10:58:14,432 [adam_adapter.py] => Learning on 270-300
2023-09-09 11:00:12,489 [trainer.py] => No NME accuracy.
2023-09-09 11:00:12,489 [trainer.py] => CNN: {'total': 73.95, '00-09': 72.0, '10-19': 76.0, '20-29': 73.5, '30-39': 84.0, '40-49': 71.5, '50-59': 77.39, '60-69': 79.5, '70-79': 79.5, '80-89': 71.86, '90-99': 72.86, '100-109': 62.81, '110-119': 62.31, '120-129': 75.5, '130-139': 76.5, '140-149': 73.87, '150-159': 73.87, '160-169': 69.85, '170-179': 63.0, '180-189': 73.23, '190-199': 80.5, '200-209': 65.33, '210-219': 50.0, '220-229': 85.93, '230-239': 79.5, '240-249': 68.0, '250-259': 76.38, '260-269': 81.5, '270-279': 79.9, '280-289': 82.5, '290-299': 79.9, 'old': 73.19, 'new': 80.77}
2023-09-09 11:00:12,489 [trainer.py] => CNN top1 curve: [89.17, 88.74, 86.04, 82.3, 80.29, 77.56, 76.53, 74.87, 73.99, 73.95]
2023-09-09 11:00:12,489 [trainer.py] => CNN top5 curve: [98.83, 98.17, 97.27, 96.08, 94.89, 93.76, 93.34, 92.02, 92.07, 91.88]

2023-09-09 11:00:12,489 [trainer.py] => Average Accuracy (CNN): 80.34400000000001
