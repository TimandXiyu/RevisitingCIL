2023-09-10 05:54:30,016 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 05:54:30,016 [trainer.py] => prefix:  
2023-09-10 05:54:30,016 [trainer.py] => dataset: omnibenchmark
2023-09-10 05:54:30,016 [trainer.py] => memory_size: 0
2023-09-10 05:54:30,016 [trainer.py] => memory_per_class: 0
2023-09-10 05:54:30,016 [trainer.py] => fixed_memory: False
2023-09-10 05:54:30,016 [trainer.py] => shuffle: True
2023-09-10 05:54:30,016 [trainer.py] => init_cls: 30
2023-09-10 05:54:30,017 [trainer.py] => increment: 30
2023-09-10 05:54:30,017 [trainer.py] => model_name: adam_adapter
2023-09-10 05:54:30,017 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 05:54:30,017 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 05:54:30,017 [trainer.py] => seed: 1993
2023-09-10 05:54:30,017 [trainer.py] => tuned_epoch: 20
2023-09-10 05:54:30,017 [trainer.py] => init_lr: 0.02
2023-09-10 05:54:30,017 [trainer.py] => batch_size: 96
2023-09-10 05:54:30,017 [trainer.py] => use_A: True
2023-09-10 05:54:30,017 [trainer.py] => weight_decay: 0.0005
2023-09-10 05:54:30,017 [trainer.py] => min_lr: 0
2023-09-10 05:54:30,017 [trainer.py] => ffn_num: 64
2023-09-10 05:54:30,017 [trainer.py] => optimizer: sgd
2023-09-10 05:54:30,017 [trainer.py] => vpt_type: shallow
2023-09-10 05:54:30,017 [trainer.py] => prompt_token_num: 5
2023-09-10 05:54:31,788 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 05:54:36,186 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 05:54:36,689 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 05:54:36,751 [trainer.py] => All params: 86988288
2023-09-10 05:54:36,751 [trainer.py] => Trainable params: 1189632
2023-09-10 05:54:36,857 [adam_adapter.py] => Learning on 0-30
2023-09-10 05:55:22,984 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.022, Train_accy 54.71, Test_accy 80.33
2023-09-10 05:56:02,111 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 1.097, Train_accy 79.69, Test_accy 86.67
2023-09-10 05:56:41,772 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 0.613, Train_accy 82.14, Test_accy 88.33
2023-09-10 05:57:21,505 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 0.534, Train_accy 83.68, Test_accy 87.50
2023-09-10 05:58:01,004 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 0.503, Train_accy 84.24, Test_accy 90.00
2023-09-10 05:58:40,730 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 0.476, Train_accy 86.31, Test_accy 89.83
2023-09-10 05:59:20,932 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 0.481, Train_accy 86.06, Test_accy 89.83
2023-09-10 06:00:00,745 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 0.460, Train_accy 86.49, Test_accy 90.33
2023-09-10 06:00:40,488 [adam_adapter.py] => Task 0, Epoch 9/20 => Loss 0.472, Train_accy 86.44, Test_accy 91.50
2023-09-10 06:01:20,202 [adam_adapter.py] => Task 0, Epoch 10/20 => Loss 0.469, Train_accy 86.41, Test_accy 90.67
2023-09-10 06:02:00,121 [adam_adapter.py] => Task 0, Epoch 11/20 => Loss 0.461, Train_accy 86.86, Test_accy 91.00
2023-09-10 06:02:40,027 [adam_adapter.py] => Task 0, Epoch 12/20 => Loss 0.461, Train_accy 86.96, Test_accy 90.83
2023-09-10 06:03:19,677 [adam_adapter.py] => Task 0, Epoch 13/20 => Loss 0.449, Train_accy 86.79, Test_accy 90.83
2023-09-10 06:03:59,520 [adam_adapter.py] => Task 0, Epoch 14/20 => Loss 0.458, Train_accy 86.67, Test_accy 91.00
2023-09-10 06:04:39,227 [adam_adapter.py] => Task 0, Epoch 15/20 => Loss 0.464, Train_accy 86.64, Test_accy 91.17
2023-09-10 06:05:19,121 [adam_adapter.py] => Task 0, Epoch 16/20 => Loss 0.463, Train_accy 86.41, Test_accy 91.17
2023-09-10 06:05:59,156 [adam_adapter.py] => Task 0, Epoch 17/20 => Loss 0.452, Train_accy 87.06, Test_accy 91.17
2023-09-10 06:06:39,085 [adam_adapter.py] => Task 0, Epoch 18/20 => Loss 0.457, Train_accy 86.76, Test_accy 91.17
2023-09-10 06:07:18,793 [adam_adapter.py] => Task 0, Epoch 19/20 => Loss 0.454, Train_accy 87.12, Test_accy 91.17
2023-09-10 06:07:58,430 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.458, Train_accy 86.55, Test_accy 91.17
2023-09-10 06:07:59,555 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 06:08:00,048 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 06:08:00,778 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 06:08:01,039 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 06:08:53,742 [trainer.py] => No NME accuracy.
2023-09-10 06:08:53,743 [trainer.py] => CNN: {'total': 89.17, '00-09': 91.0, '10-19': 90.0, '20-29': 86.5, 'old': 0, 'new': 89.17}
2023-09-10 06:08:53,743 [trainer.py] => CNN top1 curve: [89.17]
2023-09-10 06:08:53,743 [trainer.py] => CNN top5 curve: [99.17]

2023-09-10 06:08:53,743 [trainer.py] => Average Accuracy (CNN): 89.17
2023-09-10 06:08:53,744 [trainer.py] => All params: 172833025
2023-09-10 06:08:53,746 [trainer.py] => Trainable params: 87034369
2023-09-10 06:08:53,747 [adam_adapter.py] => Learning on 30-60
2023-09-10 06:09:38,375 [trainer.py] => No NME accuracy.
2023-09-10 06:09:38,375 [trainer.py] => CNN: {'total': 88.91, '00-09': 86.0, '10-19': 90.0, '20-29': 83.5, '30-39': 91.5, '40-49': 88.0, '50-59': 94.47, 'old': 86.5, 'new': 91.32}
2023-09-10 06:09:38,376 [trainer.py] => CNN top1 curve: [89.17, 88.91]
2023-09-10 06:09:38,376 [trainer.py] => CNN top5 curve: [99.17, 98.42]

2023-09-10 06:09:38,376 [trainer.py] => Average Accuracy (CNN): 89.03999999999999
2023-09-10 06:09:38,378 [trainer.py] => All params: 172879105
2023-09-10 06:09:38,379 [trainer.py] => Trainable params: 87080449
2023-09-10 06:09:38,381 [adam_adapter.py] => Learning on 60-90
2023-09-10 06:10:25,692 [trainer.py] => No NME accuracy.
2023-09-10 06:10:25,692 [trainer.py] => CNN: {'total': 85.93, '00-09': 83.0, '10-19': 84.5, '20-29': 79.5, '30-39': 88.5, '40-49': 86.0, '50-59': 92.96, '60-69': 92.0, '70-79': 88.0, '80-89': 78.89, 'old': 85.74, 'new': 86.31}
2023-09-10 06:10:25,692 [trainer.py] => CNN top1 curve: [89.17, 88.91, 85.93]
2023-09-10 06:10:25,692 [trainer.py] => CNN top5 curve: [99.17, 98.42, 97.39]

2023-09-10 06:10:25,692 [trainer.py] => Average Accuracy (CNN): 88.00333333333333
2023-09-10 06:10:25,693 [trainer.py] => All params: 172925185
2023-09-10 06:10:25,694 [trainer.py] => Trainable params: 87126529
2023-09-10 06:10:25,695 [adam_adapter.py] => Learning on 90-120
2023-09-10 06:11:17,611 [trainer.py] => No NME accuracy.
2023-09-10 06:11:17,611 [trainer.py] => CNN: {'total': 82.0, '00-09': 80.0, '10-19': 81.0, '20-29': 79.0, '30-39': 87.0, '40-49': 83.5, '50-59': 85.93, '60-69': 87.5, '70-79': 85.0, '80-89': 75.88, '90-99': 80.9, '100-109': 78.89, '110-119': 79.4, 'old': 82.76, 'new': 79.73}
2023-09-10 06:11:17,611 [trainer.py] => CNN top1 curve: [89.17, 88.91, 85.93, 82.0]
2023-09-10 06:11:17,611 [trainer.py] => CNN top5 curve: [99.17, 98.42, 97.39, 96.28]

2023-09-10 06:11:17,611 [trainer.py] => Average Accuracy (CNN): 86.5025
2023-09-10 06:11:17,612 [trainer.py] => All params: 172971265
2023-09-10 06:11:17,613 [trainer.py] => Trainable params: 87172609
2023-09-10 06:11:17,614 [adam_adapter.py] => Learning on 120-150
2023-09-10 06:12:12,519 [trainer.py] => No NME accuracy.
2023-09-10 06:12:12,519 [trainer.py] => CNN: {'total': 79.99, '00-09': 77.0, '10-19': 80.5, '20-29': 78.0, '30-39': 85.5, '40-49': 81.0, '50-59': 80.4, '60-69': 83.0, '70-79': 84.0, '80-89': 73.87, '90-99': 80.4, '100-109': 75.88, '110-119': 73.87, '120-129': 84.5, '130-139': 80.5, '140-149': 81.41, 'old': 79.46, 'new': 82.14}
2023-09-10 06:12:12,519 [trainer.py] => CNN top1 curve: [89.17, 88.91, 85.93, 82.0, 79.99]
2023-09-10 06:12:12,519 [trainer.py] => CNN top5 curve: [99.17, 98.42, 97.39, 96.28, 94.99]

2023-09-10 06:12:12,519 [trainer.py] => Average Accuracy (CNN): 85.2
2023-09-10 06:12:12,521 [trainer.py] => All params: 173017345
2023-09-10 06:12:12,522 [trainer.py] => Trainable params: 87218689
2023-09-10 06:12:12,525 [adam_adapter.py] => Learning on 150-180
2023-09-10 06:13:09,210 [trainer.py] => No NME accuracy.
2023-09-10 06:13:09,210 [trainer.py] => CNN: {'total': 77.12, '00-09': 74.5, '10-19': 77.0, '20-29': 74.5, '30-39': 84.0, '40-49': 78.0, '50-59': 78.89, '60-69': 81.5, '70-79': 81.0, '80-89': 70.35, '90-99': 79.9, '100-109': 73.37, '110-119': 72.86, '120-129': 84.0, '130-139': 80.0, '140-149': 79.4, '150-159': 78.39, '160-169': 74.37, '170-179': 66.0, 'old': 77.96, 'new': 72.91}
2023-09-10 06:13:09,210 [trainer.py] => CNN top1 curve: [89.17, 88.91, 85.93, 82.0, 79.99, 77.12]
2023-09-10 06:13:09,210 [trainer.py] => CNN top5 curve: [99.17, 98.42, 97.39, 96.28, 94.99, 93.71]

2023-09-10 06:13:09,210 [trainer.py] => Average Accuracy (CNN): 83.85333333333334
2023-09-10 06:13:09,212 [trainer.py] => All params: 173063425
2023-09-10 06:13:09,214 [trainer.py] => Trainable params: 87264769
2023-09-10 06:13:09,217 [adam_adapter.py] => Learning on 180-210
2023-09-10 06:14:10,067 [trainer.py] => No NME accuracy.
2023-09-10 06:14:10,068 [trainer.py] => CNN: {'total': 76.39, '00-09': 74.0, '10-19': 77.0, '20-29': 74.5, '30-39': 84.0, '40-49': 77.0, '50-59': 78.39, '60-69': 79.0, '70-79': 80.5, '80-89': 70.35, '90-99': 77.39, '100-109': 72.86, '110-119': 69.35, '120-129': 84.0, '130-139': 79.0, '140-149': 79.4, '150-159': 75.38, '160-169': 73.37, '170-179': 66.0, '180-189': 75.25, '190-199': 83.5, '200-209': 73.87, 'old': 76.2, 'new': 77.55}
2023-09-10 06:14:10,068 [trainer.py] => CNN top1 curve: [89.17, 88.91, 85.93, 82.0, 79.99, 77.12, 76.39]
2023-09-10 06:14:10,068 [trainer.py] => CNN top5 curve: [99.17, 98.42, 97.39, 96.28, 94.99, 93.71, 93.29]

2023-09-10 06:14:10,068 [trainer.py] => Average Accuracy (CNN): 82.78714285714285
2023-09-10 06:14:10,070 [trainer.py] => All params: 173109505
2023-09-10 06:14:10,071 [trainer.py] => Trainable params: 87310849
2023-09-10 06:14:10,074 [adam_adapter.py] => Learning on 210-240
2023-09-10 06:15:14,398 [trainer.py] => No NME accuracy.
2023-09-10 06:15:14,398 [trainer.py] => CNN: {'total': 74.71, '00-09': 73.0, '10-19': 74.5, '20-29': 73.0, '30-39': 84.0, '40-49': 75.5, '50-59': 78.39, '60-69': 78.5, '70-79': 80.5, '80-89': 70.35, '90-99': 77.39, '100-109': 65.83, '110-119': 68.34, '120-129': 81.0, '130-139': 79.0, '140-149': 78.39, '150-159': 75.38, '160-169': 73.37, '170-179': 63.5, '180-189': 74.75, '190-199': 83.0, '200-209': 69.35, '210-219': 49.0, '220-229': 86.43, '230-239': 80.5, 'old': 75.1, 'new': 71.95}
2023-09-10 06:15:14,398 [trainer.py] => CNN top1 curve: [89.17, 88.91, 85.93, 82.0, 79.99, 77.12, 76.39, 74.71]
2023-09-10 06:15:14,398 [trainer.py] => CNN top5 curve: [99.17, 98.42, 97.39, 96.28, 94.99, 93.71, 93.29, 91.94]

2023-09-10 06:15:14,398 [trainer.py] => Average Accuracy (CNN): 81.7775
2023-09-10 06:15:14,399 [trainer.py] => All params: 173155585
2023-09-10 06:15:14,400 [trainer.py] => Trainable params: 87356929
2023-09-10 06:15:14,403 [adam_adapter.py] => Learning on 240-270
2023-09-10 06:16:21,656 [trainer.py] => No NME accuracy.
2023-09-10 06:16:21,656 [trainer.py] => CNN: {'total': 73.81, '00-09': 72.5, '10-19': 73.5, '20-29': 73.0, '30-39': 83.0, '40-49': 75.5, '50-59': 77.89, '60-69': 76.5, '70-79': 80.0, '80-89': 69.35, '90-99': 75.88, '100-109': 64.32, '110-119': 65.33, '120-129': 75.5, '130-139': 78.0, '140-149': 73.37, '150-159': 72.86, '160-169': 73.37, '170-179': 63.5, '180-189': 74.75, '190-199': 81.5, '200-209': 66.83, '210-219': 49.0, '220-229': 85.93, '230-239': 80.5, '240-249': 71.5, '250-259': 76.88, '260-269': 82.5, 'old': 73.41, 'new': 76.96}
2023-09-10 06:16:21,656 [trainer.py] => CNN top1 curve: [89.17, 88.91, 85.93, 82.0, 79.99, 77.12, 76.39, 74.71, 73.81]
2023-09-10 06:16:21,656 [trainer.py] => CNN top5 curve: [99.17, 98.42, 97.39, 96.28, 94.99, 93.71, 93.29, 91.94, 91.94]

2023-09-10 06:16:21,656 [trainer.py] => Average Accuracy (CNN): 80.89222222222222
2023-09-10 06:16:21,658 [trainer.py] => All params: 173201665
2023-09-10 06:16:21,659 [trainer.py] => Trainable params: 87403009
2023-09-10 06:16:21,662 [adam_adapter.py] => Learning on 270-300
2023-09-10 06:17:29,213 [trainer.py] => No NME accuracy.
2023-09-10 06:17:29,214 [trainer.py] => CNN: {'total': 73.8, '00-09': 72.0, '10-19': 73.5, '20-29': 72.0, '30-39': 83.0, '40-49': 73.0, '50-59': 77.39, '60-69': 76.5, '70-79': 80.0, '80-89': 69.35, '90-99': 72.36, '100-109': 63.82, '110-119': 64.32, '120-129': 75.5, '130-139': 77.5, '140-149': 73.37, '150-159': 72.86, '160-169': 71.86, '170-179': 63.0, '180-189': 74.24, '190-199': 81.0, '200-209': 66.83, '210-219': 48.0, '220-229': 85.93, '230-239': 79.5, '240-249': 68.0, '250-259': 76.38, '260-269': 81.5, '270-279': 78.89, '280-289': 82.0, '290-299': 80.4, 'old': 73.06, 'new': 80.43}
2023-09-10 06:17:29,214 [trainer.py] => CNN top1 curve: [89.17, 88.91, 85.93, 82.0, 79.99, 77.12, 76.39, 74.71, 73.81, 73.8]
2023-09-10 06:17:29,214 [trainer.py] => CNN top5 curve: [99.17, 98.42, 97.39, 96.28, 94.99, 93.71, 93.29, 91.94, 91.94, 91.95]

2023-09-10 06:17:29,214 [trainer.py] => Average Accuracy (CNN): 80.18299999999999
2023-09-10 07:56:53,223 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 07:56:53,223 [trainer.py] => prefix:  
2023-09-10 07:56:53,223 [trainer.py] => dataset: omnibenchmark
2023-09-10 07:56:53,223 [trainer.py] => memory_size: 0
2023-09-10 07:56:53,223 [trainer.py] => memory_per_class: 0
2023-09-10 07:56:53,223 [trainer.py] => fixed_memory: False
2023-09-10 07:56:53,223 [trainer.py] => shuffle: True
2023-09-10 07:56:53,223 [trainer.py] => init_cls: 30
2023-09-10 07:56:53,223 [trainer.py] => increment: 30
2023-09-10 07:56:53,223 [trainer.py] => model_name: adam_adapter
2023-09-10 07:56:53,223 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 07:56:53,223 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 07:56:53,223 [trainer.py] => seed: 1993
2023-09-10 07:56:53,223 [trainer.py] => tuned_epoch: 20
2023-09-10 07:56:53,223 [trainer.py] => init_lr: 0.02
2023-09-10 07:56:53,223 [trainer.py] => batch_size: 96
2023-09-10 07:56:53,223 [trainer.py] => use_A: True
2023-09-10 07:56:53,223 [trainer.py] => weight_decay: 0.0005
2023-09-10 07:56:53,223 [trainer.py] => min_lr: 0
2023-09-10 07:56:53,223 [trainer.py] => ffn_num: 64
2023-09-10 07:56:53,223 [trainer.py] => optimizer: sgd
2023-09-10 07:56:53,223 [trainer.py] => vpt_type: shallow
2023-09-10 07:56:53,224 [trainer.py] => prompt_token_num: 5
2023-09-10 07:56:56,180 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 07:56:58,204 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 07:56:58,706 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 07:56:58,776 [trainer.py] => All params: 86988288
2023-09-10 07:56:58,777 [trainer.py] => Trainable params: 1189632
2023-09-10 07:56:58,881 [adam_adapter.py] => Learning on 0-30
2023-09-10 07:57:16,341 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 07:57:16,341 [trainer.py] => prefix:  
2023-09-10 07:57:16,341 [trainer.py] => dataset: omnibenchmark
2023-09-10 07:57:16,341 [trainer.py] => memory_size: 0
2023-09-10 07:57:16,341 [trainer.py] => memory_per_class: 0
2023-09-10 07:57:16,341 [trainer.py] => fixed_memory: False
2023-09-10 07:57:16,341 [trainer.py] => shuffle: True
2023-09-10 07:57:16,341 [trainer.py] => init_cls: 30
2023-09-10 07:57:16,341 [trainer.py] => increment: 30
2023-09-10 07:57:16,341 [trainer.py] => model_name: adam_adapter
2023-09-10 07:57:16,341 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 07:57:16,341 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 07:57:16,341 [trainer.py] => seed: 1993
2023-09-10 07:57:16,341 [trainer.py] => tuned_epoch: 20
2023-09-10 07:57:16,341 [trainer.py] => init_lr: 0.02
2023-09-10 07:57:16,341 [trainer.py] => batch_size: 96
2023-09-10 07:57:16,341 [trainer.py] => use_A: True
2023-09-10 07:57:16,341 [trainer.py] => weight_decay: 0.0005
2023-09-10 07:57:16,341 [trainer.py] => min_lr: 0
2023-09-10 07:57:16,341 [trainer.py] => ffn_num: 64
2023-09-10 07:57:16,341 [trainer.py] => optimizer: sgd
2023-09-10 07:57:16,341 [trainer.py] => vpt_type: shallow
2023-09-10 07:57:16,341 [trainer.py] => prompt_token_num: 5
2023-09-10 07:57:16,540 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 07:57:18,589 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 07:57:19,098 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 07:57:19,167 [trainer.py] => All params: 86988288
2023-09-10 07:57:19,168 [trainer.py] => Trainable params: 1189632
2023-09-10 07:57:19,277 [adam_adapter.py] => Learning on 0-30
2023-09-10 07:57:58,911 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.022, Train_accy 54.71, Test_accy 80.33
2023-09-10 07:58:38,155 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 1.099, Train_accy 79.69, Test_accy 86.67
2023-09-10 07:59:18,140 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 0.615, Train_accy 82.21, Test_accy 88.33
2023-09-10 07:59:57,689 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 0.537, Train_accy 83.66, Test_accy 87.33
2023-09-10 08:00:37,193 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 0.506, Train_accy 84.25, Test_accy 91.00
2023-09-10 08:01:17,284 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 0.480, Train_accy 85.70, Test_accy 90.00
2023-09-10 08:01:57,555 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 0.471, Train_accy 85.47, Test_accy 90.50
2023-09-10 08:02:37,710 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 0.439, Train_accy 86.55, Test_accy 90.33
2023-09-10 08:03:17,515 [adam_adapter.py] => Task 0, Epoch 9/20 => Loss 0.440, Train_accy 86.67, Test_accy 91.33
2023-09-10 08:03:57,378 [adam_adapter.py] => Task 0, Epoch 10/20 => Loss 0.433, Train_accy 86.93, Test_accy 91.67
2023-09-10 08:04:37,356 [adam_adapter.py] => Task 0, Epoch 11/20 => Loss 0.418, Train_accy 87.54, Test_accy 90.67
2023-09-10 08:05:17,259 [adam_adapter.py] => Task 0, Epoch 12/20 => Loss 0.409, Train_accy 88.14, Test_accy 91.67
2023-09-10 08:05:57,214 [adam_adapter.py] => Task 0, Epoch 13/20 => Loss 0.395, Train_accy 87.90, Test_accy 91.67
2023-09-10 08:06:37,195 [adam_adapter.py] => Task 0, Epoch 14/20 => Loss 0.397, Train_accy 87.81, Test_accy 90.33
2023-09-10 08:07:17,038 [adam_adapter.py] => Task 0, Epoch 15/20 => Loss 0.401, Train_accy 88.10, Test_accy 91.17
2023-09-10 08:07:56,991 [adam_adapter.py] => Task 0, Epoch 16/20 => Loss 0.398, Train_accy 87.93, Test_accy 91.50
2023-09-10 08:08:36,851 [adam_adapter.py] => Task 0, Epoch 17/20 => Loss 0.386, Train_accy 88.57, Test_accy 91.50
2023-09-10 08:09:16,795 [adam_adapter.py] => Task 0, Epoch 18/20 => Loss 0.388, Train_accy 88.14, Test_accy 91.83
2023-09-10 08:09:56,521 [adam_adapter.py] => Task 0, Epoch 19/20 => Loss 0.384, Train_accy 88.52, Test_accy 92.00
2023-09-10 08:10:36,366 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.387, Train_accy 88.50, Test_accy 92.00
2023-09-10 08:10:37,517 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:10:38,011 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:10:38,782 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:10:39,043 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:11:32,402 [trainer.py] => No NME accuracy.
2023-09-10 08:11:32,402 [trainer.py] => CNN: {'total': 87.83, '00-09': 88.0, '10-19': 90.0, '20-29': 85.5, 'old': 0, 'new': 87.83}
2023-09-10 08:11:32,402 [trainer.py] => CNN top1 curve: [87.83]
2023-09-10 08:11:32,402 [trainer.py] => CNN top5 curve: [99.17]

2023-09-10 08:11:32,402 [trainer.py] => Average Accuracy (CNN): 87.83
2023-09-10 08:11:32,404 [trainer.py] => All params: 172833025
2023-09-10 08:11:32,404 [trainer.py] => Trainable params: 87034369
2023-09-10 08:11:32,405 [adam_adapter.py] => Learning on 30-60
2023-09-10 08:12:16,985 [trainer.py] => No NME accuracy.
2023-09-10 08:12:16,985 [trainer.py] => CNN: {'total': 87.49, '00-09': 83.5, '10-19': 90.0, '20-29': 83.5, '30-39': 89.0, '40-49': 85.5, '50-59': 93.47, 'old': 85.67, 'new': 89.32}
2023-09-10 08:12:16,985 [trainer.py] => CNN top1 curve: [87.83, 87.49]
2023-09-10 08:12:16,985 [trainer.py] => CNN top5 curve: [99.17, 98.33]

2023-09-10 08:12:16,985 [trainer.py] => Average Accuracy (CNN): 87.66
2023-09-10 08:12:16,987 [trainer.py] => All params: 172879105
2023-09-10 08:12:16,988 [trainer.py] => Trainable params: 87080449
2023-09-10 08:12:16,990 [adam_adapter.py] => Learning on 60-90
2023-09-10 08:13:03,890 [trainer.py] => No NME accuracy.
2023-09-10 08:13:03,890 [trainer.py] => CNN: {'total': 84.04, '00-09': 81.5, '10-19': 84.0, '20-29': 81.5, '30-39': 83.5, '40-49': 83.5, '50-59': 92.46, '60-69': 87.5, '70-79': 88.0, '80-89': 74.37, 'old': 84.4, 'new': 83.31}
2023-09-10 08:13:03,890 [trainer.py] => CNN top1 curve: [87.83, 87.49, 84.04]
2023-09-10 08:13:03,890 [trainer.py] => CNN top5 curve: [99.17, 98.33, 96.33]

2023-09-10 08:13:03,890 [trainer.py] => Average Accuracy (CNN): 86.45333333333333
2023-09-10 08:13:03,891 [trainer.py] => All params: 172925185
2023-09-10 08:13:03,892 [trainer.py] => Trainable params: 87126529
2023-09-10 08:13:03,894 [adam_adapter.py] => Learning on 90-120
2023-09-10 08:13:55,695 [trainer.py] => No NME accuracy.
2023-09-10 08:13:55,695 [trainer.py] => CNN: {'total': 80.21, '00-09': 76.0, '10-19': 77.5, '20-29': 79.5, '30-39': 83.0, '40-49': 80.5, '50-59': 86.93, '60-69': 84.5, '70-79': 84.5, '80-89': 71.86, '90-99': 79.9, '100-109': 79.4, '110-119': 78.89, 'old': 80.48, 'new': 79.4}
2023-09-10 08:13:55,695 [trainer.py] => CNN top1 curve: [87.83, 87.49, 84.04, 80.21]
2023-09-10 08:13:55,695 [trainer.py] => CNN top5 curve: [99.17, 98.33, 96.33, 95.28]

2023-09-10 08:13:55,695 [trainer.py] => Average Accuracy (CNN): 84.8925
2023-09-10 08:13:55,696 [trainer.py] => All params: 172971265
2023-09-10 08:13:55,697 [trainer.py] => Trainable params: 87172609
2023-09-10 08:13:55,699 [adam_adapter.py] => Learning on 120-150
2023-09-10 08:14:50,550 [trainer.py] => No NME accuracy.
2023-09-10 08:14:50,550 [trainer.py] => CNN: {'total': 77.96, '00-09': 75.5, '10-19': 77.5, '20-29': 79.0, '30-39': 82.5, '40-49': 75.5, '50-59': 80.4, '60-69': 81.0, '70-79': 84.5, '80-89': 70.35, '90-99': 79.9, '100-109': 74.87, '110-119': 68.84, '120-129': 85.0, '130-139': 76.0, '140-149': 78.39, 'old': 77.49, 'new': 79.8}
2023-09-10 08:14:50,550 [trainer.py] => CNN top1 curve: [87.83, 87.49, 84.04, 80.21, 77.96]
2023-09-10 08:14:50,550 [trainer.py] => CNN top5 curve: [99.17, 98.33, 96.33, 95.28, 93.59]

2023-09-10 08:14:50,550 [trainer.py] => Average Accuracy (CNN): 83.506
2023-09-10 08:14:50,552 [trainer.py] => All params: 173017345
2023-09-10 08:14:50,554 [trainer.py] => Trainable params: 87218689
2023-09-10 08:14:50,557 [adam_adapter.py] => Learning on 150-180
2023-09-10 08:15:46,918 [trainer.py] => No NME accuracy.
2023-09-10 08:15:46,919 [trainer.py] => CNN: {'total': 75.36, '00-09': 73.0, '10-19': 75.0, '20-29': 77.5, '30-39': 82.0, '40-49': 72.5, '50-59': 75.38, '60-69': 79.5, '70-79': 84.0, '80-89': 68.84, '90-99': 79.9, '100-109': 72.86, '110-119': 65.33, '120-129': 85.0, '130-139': 76.0, '140-149': 78.39, '150-159': 82.41, '160-169': 68.84, '170-179': 60.0, 'old': 76.35, 'new': 70.4}
2023-09-10 08:15:46,919 [trainer.py] => CNN top1 curve: [87.83, 87.49, 84.04, 80.21, 77.96, 75.36]
2023-09-10 08:15:46,919 [trainer.py] => CNN top5 curve: [99.17, 98.33, 96.33, 95.28, 93.59, 92.46]

2023-09-10 08:15:46,919 [trainer.py] => Average Accuracy (CNN): 82.14833333333333
2023-09-10 08:15:46,921 [trainer.py] => All params: 173063425
2023-09-10 08:15:46,922 [trainer.py] => Trainable params: 87264769
2023-09-10 08:15:46,924 [adam_adapter.py] => Learning on 180-210
2023-09-10 08:16:46,565 [trainer.py] => No NME accuracy.
2023-09-10 08:16:46,565 [trainer.py] => CNN: {'total': 74.27, '00-09': 73.0, '10-19': 74.0, '20-29': 76.5, '30-39': 82.0, '40-49': 71.5, '50-59': 74.87, '60-69': 76.0, '70-79': 82.0, '80-89': 68.84, '90-99': 77.89, '100-109': 72.36, '110-119': 56.78, '120-129': 84.5, '130-139': 75.0, '140-149': 78.39, '150-159': 80.4, '160-169': 67.34, '170-179': 59.5, '180-189': 75.25, '190-199': 87.5, '200-209': 65.83, 'old': 73.94, 'new': 76.21}
2023-09-10 08:16:46,565 [trainer.py] => CNN top1 curve: [87.83, 87.49, 84.04, 80.21, 77.96, 75.36, 74.27]
2023-09-10 08:16:46,565 [trainer.py] => CNN top5 curve: [99.17, 98.33, 96.33, 95.28, 93.59, 92.46, 92.0]

2023-09-10 08:16:46,565 [trainer.py] => Average Accuracy (CNN): 81.02285714285713
2023-09-10 08:16:46,567 [trainer.py] => All params: 173109505
2023-09-10 08:16:46,569 [trainer.py] => Trainable params: 87310849
2023-09-10 08:16:46,572 [adam_adapter.py] => Learning on 210-240
2023-09-10 08:17:50,669 [trainer.py] => No NME accuracy.
2023-09-10 08:17:50,669 [trainer.py] => CNN: {'total': 72.66, '00-09': 72.5, '10-19': 72.5, '20-29': 74.5, '30-39': 82.0, '40-49': 67.5, '50-59': 74.37, '60-69': 74.5, '70-79': 82.0, '80-89': 68.84, '90-99': 77.89, '100-109': 65.33, '110-119': 55.78, '120-129': 83.0, '130-139': 75.0, '140-149': 78.39, '150-159': 79.9, '160-169': 66.83, '170-179': 58.0, '180-189': 75.25, '190-199': 87.5, '200-209': 61.31, '210-219': 49.0, '220-229': 83.42, '230-239': 78.5, 'old': 73.0, 'new': 70.28}
2023-09-10 08:17:50,669 [trainer.py] => CNN top1 curve: [87.83, 87.49, 84.04, 80.21, 77.96, 75.36, 74.27, 72.66]
2023-09-10 08:17:50,669 [trainer.py] => CNN top5 curve: [99.17, 98.33, 96.33, 95.28, 93.59, 92.46, 92.0, 90.91]

2023-09-10 08:17:50,669 [trainer.py] => Average Accuracy (CNN): 79.97749999999999
2023-09-10 08:17:50,670 [trainer.py] => All params: 173155585
2023-09-10 08:17:50,671 [trainer.py] => Trainable params: 87356929
2023-09-10 08:17:50,678 [adam_adapter.py] => Learning on 240-270
2023-09-10 08:18:57,665 [trainer.py] => No NME accuracy.
2023-09-10 08:18:57,665 [trainer.py] => CNN: {'total': 71.56, '00-09': 72.5, '10-19': 72.0, '20-29': 74.5, '30-39': 81.5, '40-49': 67.5, '50-59': 74.37, '60-69': 73.5, '70-79': 81.5, '80-89': 65.83, '90-99': 73.87, '100-109': 63.32, '110-119': 53.77, '120-129': 82.0, '130-139': 71.5, '140-149': 74.87, '150-159': 77.89, '160-169': 66.83, '170-179': 58.0, '180-189': 75.25, '190-199': 84.0, '200-209': 56.28, '210-219': 49.0, '220-229': 81.91, '230-239': 78.5, '240-249': 69.5, '250-259': 69.35, '260-269': 83.0, 'old': 71.26, 'new': 73.96}
2023-09-10 08:18:57,665 [trainer.py] => CNN top1 curve: [87.83, 87.49, 84.04, 80.21, 77.96, 75.36, 74.27, 72.66, 71.56]
2023-09-10 08:18:57,665 [trainer.py] => CNN top5 curve: [99.17, 98.33, 96.33, 95.28, 93.59, 92.46, 92.0, 90.91, 91.07]

2023-09-10 08:18:57,665 [trainer.py] => Average Accuracy (CNN): 79.04222222222221
2023-09-10 08:18:57,667 [trainer.py] => All params: 173201665
2023-09-10 08:18:57,668 [trainer.py] => Trainable params: 87403009
2023-09-10 08:18:57,672 [adam_adapter.py] => Learning on 270-300
2023-09-10 08:20:05,282 [trainer.py] => No NME accuracy.
2023-09-10 08:20:05,282 [trainer.py] => CNN: {'total': 71.6, '00-09': 72.5, '10-19': 72.0, '20-29': 74.0, '30-39': 81.5, '40-49': 64.5, '50-59': 73.87, '60-69': 73.0, '70-79': 81.5, '80-89': 65.83, '90-99': 66.83, '100-109': 62.81, '110-119': 53.77, '120-129': 82.0, '130-139': 71.5, '140-149': 74.37, '150-159': 77.39, '160-169': 64.32, '170-179': 57.5, '180-189': 74.75, '190-199': 84.0, '200-209': 56.28, '210-219': 47.5, '220-229': 81.91, '230-239': 78.0, '240-249': 68.5, '250-259': 67.84, '260-269': 81.0, '270-279': 74.87, '280-289': 80.5, '290-299': 83.42, 'old': 70.71, 'new': 79.6}
2023-09-10 08:20:05,282 [trainer.py] => CNN top1 curve: [87.83, 87.49, 84.04, 80.21, 77.96, 75.36, 74.27, 72.66, 71.56, 71.6]
2023-09-10 08:20:05,282 [trainer.py] => CNN top5 curve: [99.17, 98.33, 96.33, 95.28, 93.59, 92.46, 92.0, 90.91, 91.07, 90.99]

2023-09-10 08:20:05,282 [trainer.py] => Average Accuracy (CNN): 78.29799999999999
2023-09-10 08:24:13,393 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:24:13,394 [trainer.py] => prefix:  
2023-09-10 08:24:13,394 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:24:13,394 [trainer.py] => memory_size: 0
2023-09-10 08:24:13,394 [trainer.py] => memory_per_class: 0
2023-09-10 08:24:13,394 [trainer.py] => fixed_memory: False
2023-09-10 08:24:13,394 [trainer.py] => shuffle: True
2023-09-10 08:24:13,394 [trainer.py] => init_cls: 30
2023-09-10 08:24:13,394 [trainer.py] => increment: 30
2023-09-10 08:24:13,394 [trainer.py] => model_name: adam_adapter
2023-09-10 08:24:13,394 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:24:13,394 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:24:13,394 [trainer.py] => seed: 1993
2023-09-10 08:24:13,394 [trainer.py] => tuned_epoch: 20
2023-09-10 08:24:13,394 [trainer.py] => init_lr: 0.02
2023-09-10 08:24:13,394 [trainer.py] => batch_size: 96
2023-09-10 08:24:13,394 [trainer.py] => use_A: True
2023-09-10 08:24:13,394 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:24:13,395 [trainer.py] => min_lr: 0
2023-09-10 08:24:13,395 [trainer.py] => ffn_num: 64
2023-09-10 08:24:13,395 [trainer.py] => optimizer: sgd
2023-09-10 08:24:13,395 [trainer.py] => vpt_type: shallow
2023-09-10 08:24:13,395 [trainer.py] => prompt_token_num: 5
2023-09-10 08:24:13,595 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:24:15,605 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:24:16,117 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:24:16,179 [trainer.py] => All params: 86988288
2023-09-10 08:24:16,180 [trainer.py] => Trainable params: 1189632
2023-09-10 08:24:16,291 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:24:46,616 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:24:46,617 [trainer.py] => prefix:  
2023-09-10 08:24:46,617 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:24:46,617 [trainer.py] => memory_size: 0
2023-09-10 08:24:46,617 [trainer.py] => memory_per_class: 0
2023-09-10 08:24:46,617 [trainer.py] => fixed_memory: False
2023-09-10 08:24:46,617 [trainer.py] => shuffle: True
2023-09-10 08:24:46,617 [trainer.py] => init_cls: 30
2023-09-10 08:24:46,617 [trainer.py] => increment: 30
2023-09-10 08:24:46,617 [trainer.py] => model_name: adam_adapter
2023-09-10 08:24:46,617 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:24:46,617 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:24:46,617 [trainer.py] => seed: 1993
2023-09-10 08:24:46,617 [trainer.py] => tuned_epoch: 20
2023-09-10 08:24:46,617 [trainer.py] => init_lr: 0.02
2023-09-10 08:24:46,617 [trainer.py] => batch_size: 96
2023-09-10 08:24:46,617 [trainer.py] => use_A: False
2023-09-10 08:24:46,617 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:24:46,617 [trainer.py] => min_lr: 0
2023-09-10 08:24:46,617 [trainer.py] => ffn_num: 64
2023-09-10 08:24:46,618 [trainer.py] => optimizer: sgd
2023-09-10 08:24:46,618 [trainer.py] => vpt_type: shallow
2023-09-10 08:24:46,618 [trainer.py] => prompt_token_num: 5
2023-09-10 08:24:46,816 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:24:48,807 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:24:49,338 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:24:49,407 [trainer.py] => All params: 86988288
2023-09-10 08:24:49,408 [trainer.py] => Trainable params: 1189632
2023-09-10 08:24:49,515 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:25:03,241 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:25:03,241 [trainer.py] => prefix:  
2023-09-10 08:25:03,241 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:25:03,241 [trainer.py] => memory_size: 0
2023-09-10 08:25:03,241 [trainer.py] => memory_per_class: 0
2023-09-10 08:25:03,241 [trainer.py] => fixed_memory: False
2023-09-10 08:25:03,241 [trainer.py] => shuffle: True
2023-09-10 08:25:03,241 [trainer.py] => init_cls: 30
2023-09-10 08:25:03,241 [trainer.py] => increment: 30
2023-09-10 08:25:03,241 [trainer.py] => model_name: adam_adapter
2023-09-10 08:25:03,241 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:25:03,241 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:25:03,241 [trainer.py] => seed: 1993
2023-09-10 08:25:03,241 [trainer.py] => tuned_epoch: 20
2023-09-10 08:25:03,241 [trainer.py] => init_lr: 0.02
2023-09-10 08:25:03,241 [trainer.py] => batch_size: 96
2023-09-10 08:25:03,241 [trainer.py] => use_A: False
2023-09-10 08:25:03,241 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:25:03,241 [trainer.py] => min_lr: 0
2023-09-10 08:25:03,241 [trainer.py] => ffn_num: 64
2023-09-10 08:25:03,241 [trainer.py] => optimizer: sgd
2023-09-10 08:25:03,241 [trainer.py] => vpt_type: shallow
2023-09-10 08:25:03,242 [trainer.py] => prompt_token_num: 5
2023-09-10 08:25:03,440 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:25:05,366 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:25:05,894 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:25:05,962 [trainer.py] => All params: 86988288
2023-09-10 08:25:05,963 [trainer.py] => Trainable params: 1189632
2023-09-10 08:25:06,073 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:25:31,851 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:25:31,851 [trainer.py] => prefix:  
2023-09-10 08:25:31,851 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:25:31,851 [trainer.py] => memory_size: 0
2023-09-10 08:25:31,851 [trainer.py] => memory_per_class: 0
2023-09-10 08:25:31,852 [trainer.py] => fixed_memory: False
2023-09-10 08:25:31,852 [trainer.py] => shuffle: True
2023-09-10 08:25:31,852 [trainer.py] => init_cls: 30
2023-09-10 08:25:31,852 [trainer.py] => increment: 30
2023-09-10 08:25:31,852 [trainer.py] => model_name: adam_adapter
2023-09-10 08:25:31,852 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:25:31,852 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:25:31,852 [trainer.py] => seed: 1993
2023-09-10 08:25:31,852 [trainer.py] => tuned_epoch: 20
2023-09-10 08:25:31,852 [trainer.py] => init_lr: 0.02
2023-09-10 08:25:31,853 [trainer.py] => batch_size: 96
2023-09-10 08:25:31,853 [trainer.py] => use_A: False
2023-09-10 08:25:31,853 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:25:31,853 [trainer.py] => min_lr: 0
2023-09-10 08:25:31,853 [trainer.py] => ffn_num: 64
2023-09-10 08:25:31,853 [trainer.py] => optimizer: sgd
2023-09-10 08:25:31,853 [trainer.py] => vpt_type: shallow
2023-09-10 08:25:31,853 [trainer.py] => prompt_token_num: 5
2023-09-10 08:25:32,578 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:25:35,237 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:25:35,746 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:25:35,861 [trainer.py] => All params: 86988288
2023-09-10 08:25:35,875 [trainer.py] => Trainable params: 1189632
2023-09-10 08:25:35,983 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:28:24,587 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:28:24,587 [trainer.py] => prefix:  
2023-09-10 08:28:24,588 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:28:24,588 [trainer.py] => memory_size: 0
2023-09-10 08:28:24,588 [trainer.py] => memory_per_class: 0
2023-09-10 08:28:24,588 [trainer.py] => fixed_memory: False
2023-09-10 08:28:24,588 [trainer.py] => shuffle: True
2023-09-10 08:28:24,588 [trainer.py] => init_cls: 30
2023-09-10 08:28:24,588 [trainer.py] => increment: 30
2023-09-10 08:28:24,588 [trainer.py] => model_name: adam_adapter
2023-09-10 08:28:24,588 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:28:24,588 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:28:24,589 [trainer.py] => seed: 1993
2023-09-10 08:28:24,589 [trainer.py] => tuned_epoch: 20
2023-09-10 08:28:24,589 [trainer.py] => init_lr: 0.02
2023-09-10 08:28:24,589 [trainer.py] => batch_size: 96
2023-09-10 08:28:24,589 [trainer.py] => use_A: False
2023-09-10 08:28:24,589 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:28:24,589 [trainer.py] => min_lr: 0
2023-09-10 08:28:24,589 [trainer.py] => ffn_num: 64
2023-09-10 08:28:24,589 [trainer.py] => optimizer: sgd
2023-09-10 08:28:24,589 [trainer.py] => vpt_type: shallow
2023-09-10 08:28:24,589 [trainer.py] => prompt_token_num: 5
2023-09-10 08:28:25,317 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:28:28,036 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:28:28,549 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:28:28,661 [trainer.py] => All params: 86988288
2023-09-10 08:28:28,673 [trainer.py] => Trainable params: 1189632
2023-09-10 08:28:28,786 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:29:44,744 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:29:44,744 [trainer.py] => prefix:  
2023-09-10 08:29:44,744 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:29:44,744 [trainer.py] => memory_size: 0
2023-09-10 08:29:44,744 [trainer.py] => memory_per_class: 0
2023-09-10 08:29:44,744 [trainer.py] => fixed_memory: False
2023-09-10 08:29:44,744 [trainer.py] => shuffle: True
2023-09-10 08:29:44,744 [trainer.py] => init_cls: 30
2023-09-10 08:29:44,744 [trainer.py] => increment: 30
2023-09-10 08:29:44,744 [trainer.py] => model_name: adam_adapter
2023-09-10 08:29:44,744 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:29:44,744 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:29:44,744 [trainer.py] => seed: 1993
2023-09-10 08:29:44,744 [trainer.py] => tuned_epoch: 20
2023-09-10 08:29:44,744 [trainer.py] => init_lr: 0.02
2023-09-10 08:29:44,744 [trainer.py] => batch_size: 96
2023-09-10 08:29:44,744 [trainer.py] => use_A: False
2023-09-10 08:29:44,744 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:29:44,744 [trainer.py] => min_lr: 0
2023-09-10 08:29:44,744 [trainer.py] => ffn_num: 64
2023-09-10 08:29:44,744 [trainer.py] => optimizer: sgd
2023-09-10 08:29:44,744 [trainer.py] => vpt_type: shallow
2023-09-10 08:29:44,744 [trainer.py] => prompt_token_num: 5
2023-09-10 08:29:44,950 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:29:46,926 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:29:47,420 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:29:47,489 [trainer.py] => All params: 86988288
2023-09-10 08:29:47,489 [trainer.py] => Trainable params: 1189632
2023-09-10 08:29:47,581 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:30:45,630 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:30:45,630 [trainer.py] => prefix:  
2023-09-10 08:30:45,630 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:30:45,630 [trainer.py] => memory_size: 0
2023-09-10 08:30:45,630 [trainer.py] => memory_per_class: 0
2023-09-10 08:30:45,630 [trainer.py] => fixed_memory: False
2023-09-10 08:30:45,630 [trainer.py] => shuffle: True
2023-09-10 08:30:45,630 [trainer.py] => init_cls: 30
2023-09-10 08:30:45,630 [trainer.py] => increment: 30
2023-09-10 08:30:45,630 [trainer.py] => model_name: adam_adapter
2023-09-10 08:30:45,630 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:30:45,630 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:30:45,630 [trainer.py] => seed: 1993
2023-09-10 08:30:45,630 [trainer.py] => tuned_epoch: 20
2023-09-10 08:30:45,630 [trainer.py] => init_lr: 0.02
2023-09-10 08:30:45,630 [trainer.py] => batch_size: 96
2023-09-10 08:30:45,630 [trainer.py] => use_A: False
2023-09-10 08:30:45,630 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:30:45,630 [trainer.py] => min_lr: 0
2023-09-10 08:30:45,630 [trainer.py] => ffn_num: 64
2023-09-10 08:30:45,630 [trainer.py] => optimizer: sgd
2023-09-10 08:30:45,630 [trainer.py] => vpt_type: shallow
2023-09-10 08:30:45,630 [trainer.py] => prompt_token_num: 5
2023-09-10 08:30:45,832 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:30:47,924 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:30:48,431 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:30:48,497 [trainer.py] => All params: 86988288
2023-09-10 08:30:48,498 [trainer.py] => Trainable params: 1189632
2023-09-10 08:30:48,601 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:34:34,435 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:34:34,435 [trainer.py] => prefix:  
2023-09-10 08:34:34,435 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:34:34,435 [trainer.py] => memory_size: 0
2023-09-10 08:34:34,435 [trainer.py] => memory_per_class: 0
2023-09-10 08:34:34,435 [trainer.py] => fixed_memory: False
2023-09-10 08:34:34,436 [trainer.py] => shuffle: True
2023-09-10 08:34:34,436 [trainer.py] => init_cls: 30
2023-09-10 08:34:34,436 [trainer.py] => increment: 30
2023-09-10 08:34:34,436 [trainer.py] => model_name: adam_adapter
2023-09-10 08:34:34,436 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:34:34,436 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:34:34,436 [trainer.py] => seed: 1993
2023-09-10 08:34:34,436 [trainer.py] => tuned_epoch: 20
2023-09-10 08:34:34,436 [trainer.py] => init_lr: 0.02
2023-09-10 08:34:34,436 [trainer.py] => batch_size: 96
2023-09-10 08:34:34,437 [trainer.py] => use_A: False
2023-09-10 08:34:34,437 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:34:34,437 [trainer.py] => min_lr: 0
2023-09-10 08:34:34,437 [trainer.py] => ffn_num: 64
2023-09-10 08:34:34,437 [trainer.py] => optimizer: sgd
2023-09-10 08:34:34,437 [trainer.py] => vpt_type: shallow
2023-09-10 08:34:34,437 [trainer.py] => prompt_token_num: 5
2023-09-10 08:34:35,155 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:34:38,036 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:34:38,552 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:34:38,661 [trainer.py] => All params: 86988288
2023-09-10 08:34:38,675 [trainer.py] => Trainable params: 1189632
2023-09-10 08:34:38,781 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:35:42,129 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:35:42,130 [trainer.py] => prefix:  
2023-09-10 08:35:42,130 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:35:42,130 [trainer.py] => memory_size: 0
2023-09-10 08:35:42,130 [trainer.py] => memory_per_class: 0
2023-09-10 08:35:42,130 [trainer.py] => fixed_memory: False
2023-09-10 08:35:42,130 [trainer.py] => shuffle: True
2023-09-10 08:35:42,130 [trainer.py] => init_cls: 30
2023-09-10 08:35:42,130 [trainer.py] => increment: 30
2023-09-10 08:35:42,130 [trainer.py] => model_name: adam_adapter
2023-09-10 08:35:42,130 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:35:42,131 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:35:42,131 [trainer.py] => seed: 1993
2023-09-10 08:35:42,131 [trainer.py] => tuned_epoch: 20
2023-09-10 08:35:42,131 [trainer.py] => init_lr: 0.02
2023-09-10 08:35:42,131 [trainer.py] => batch_size: 96
2023-09-10 08:35:42,131 [trainer.py] => use_A: False
2023-09-10 08:35:42,131 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:35:42,131 [trainer.py] => min_lr: 0
2023-09-10 08:35:42,131 [trainer.py] => ffn_num: 64
2023-09-10 08:35:42,131 [trainer.py] => optimizer: sgd
2023-09-10 08:35:42,131 [trainer.py] => vpt_type: shallow
2023-09-10 08:35:42,132 [trainer.py] => prompt_token_num: 5
2023-09-10 08:35:42,851 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:35:45,500 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:35:46,024 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:35:46,139 [trainer.py] => All params: 86988288
2023-09-10 08:35:46,150 [trainer.py] => Trainable params: 1189632
2023-09-10 08:35:46,255 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:37:23,756 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:37:23,756 [trainer.py] => prefix:  
2023-09-10 08:37:23,756 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:37:23,756 [trainer.py] => memory_size: 0
2023-09-10 08:37:23,756 [trainer.py] => memory_per_class: 0
2023-09-10 08:37:23,756 [trainer.py] => fixed_memory: False
2023-09-10 08:37:23,756 [trainer.py] => shuffle: True
2023-09-10 08:37:23,756 [trainer.py] => init_cls: 30
2023-09-10 08:37:23,756 [trainer.py] => increment: 30
2023-09-10 08:37:23,756 [trainer.py] => model_name: adam_adapter
2023-09-10 08:37:23,756 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:37:23,756 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:37:23,756 [trainer.py] => seed: 1993
2023-09-10 08:37:23,756 [trainer.py] => tuned_epoch: 20
2023-09-10 08:37:23,756 [trainer.py] => init_lr: 0.02
2023-09-10 08:37:23,756 [trainer.py] => batch_size: 96
2023-09-10 08:37:23,756 [trainer.py] => use_A: False
2023-09-10 08:37:23,756 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:37:23,756 [trainer.py] => min_lr: 0
2023-09-10 08:37:23,756 [trainer.py] => ffn_num: 64
2023-09-10 08:37:23,756 [trainer.py] => optimizer: sgd
2023-09-10 08:37:23,756 [trainer.py] => vpt_type: shallow
2023-09-10 08:37:23,756 [trainer.py] => prompt_token_num: 5
2023-09-10 08:37:23,955 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:37:25,932 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:37:26,443 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:37:26,515 [trainer.py] => All params: 86988288
2023-09-10 08:37:26,516 [trainer.py] => Trainable params: 1189632
2023-09-10 08:37:26,621 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:38:14,550 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:38:14,550 [trainer.py] => prefix:  
2023-09-10 08:38:14,551 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:38:14,551 [trainer.py] => memory_size: 0
2023-09-10 08:38:14,551 [trainer.py] => memory_per_class: 0
2023-09-10 08:38:14,551 [trainer.py] => fixed_memory: False
2023-09-10 08:38:14,551 [trainer.py] => shuffle: True
2023-09-10 08:38:14,551 [trainer.py] => init_cls: 30
2023-09-10 08:38:14,551 [trainer.py] => increment: 30
2023-09-10 08:38:14,551 [trainer.py] => model_name: adam_adapter
2023-09-10 08:38:14,551 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:38:14,551 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:38:14,551 [trainer.py] => seed: 1993
2023-09-10 08:38:14,551 [trainer.py] => tuned_epoch: 20
2023-09-10 08:38:14,551 [trainer.py] => init_lr: 0.02
2023-09-10 08:38:14,551 [trainer.py] => batch_size: 96
2023-09-10 08:38:14,551 [trainer.py] => use_A: False
2023-09-10 08:38:14,551 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:38:14,551 [trainer.py] => min_lr: 0
2023-09-10 08:38:14,551 [trainer.py] => ffn_num: 64
2023-09-10 08:38:14,551 [trainer.py] => optimizer: sgd
2023-09-10 08:38:14,551 [trainer.py] => vpt_type: shallow
2023-09-10 08:38:14,551 [trainer.py] => prompt_token_num: 5
2023-09-10 08:38:14,749 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:38:16,781 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:38:17,284 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:38:17,346 [trainer.py] => All params: 86988288
2023-09-10 08:38:17,346 [trainer.py] => Trainable params: 1189632
2023-09-10 08:38:17,455 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:40:29,881 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:40:29,881 [trainer.py] => prefix:  
2023-09-10 08:40:29,881 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:40:29,881 [trainer.py] => memory_size: 0
2023-09-10 08:40:29,881 [trainer.py] => memory_per_class: 0
2023-09-10 08:40:29,881 [trainer.py] => fixed_memory: False
2023-09-10 08:40:29,881 [trainer.py] => shuffle: True
2023-09-10 08:40:29,881 [trainer.py] => init_cls: 30
2023-09-10 08:40:29,881 [trainer.py] => increment: 30
2023-09-10 08:40:29,881 [trainer.py] => model_name: adam_adapter
2023-09-10 08:40:29,881 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:40:29,881 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:40:29,881 [trainer.py] => seed: 1993
2023-09-10 08:40:29,881 [trainer.py] => tuned_epoch: 20
2023-09-10 08:40:29,881 [trainer.py] => init_lr: 0.02
2023-09-10 08:40:29,882 [trainer.py] => batch_size: 96
2023-09-10 08:40:29,882 [trainer.py] => use_A: False
2023-09-10 08:40:29,882 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:40:29,882 [trainer.py] => min_lr: 0
2023-09-10 08:40:29,882 [trainer.py] => ffn_num: 64
2023-09-10 08:40:29,882 [trainer.py] => optimizer: sgd
2023-09-10 08:40:29,882 [trainer.py] => vpt_type: shallow
2023-09-10 08:40:29,882 [trainer.py] => prompt_token_num: 5
2023-09-10 08:40:30,077 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:40:32,318 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:40:32,827 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:40:32,901 [trainer.py] => All params: 86988288
2023-09-10 08:40:32,902 [trainer.py] => Trainable params: 1189632
2023-09-10 08:40:33,013 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:42:05,493 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 08:42:05,493 [trainer.py] => prefix:  
2023-09-10 08:42:05,493 [trainer.py] => dataset: omnibenchmark
2023-09-10 08:42:05,493 [trainer.py] => memory_size: 0
2023-09-10 08:42:05,493 [trainer.py] => memory_per_class: 0
2023-09-10 08:42:05,493 [trainer.py] => fixed_memory: False
2023-09-10 08:42:05,493 [trainer.py] => shuffle: True
2023-09-10 08:42:05,493 [trainer.py] => init_cls: 30
2023-09-10 08:42:05,493 [trainer.py] => increment: 30
2023-09-10 08:42:05,493 [trainer.py] => model_name: adam_adapter
2023-09-10 08:42:05,493 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 08:42:05,493 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 08:42:05,493 [trainer.py] => seed: 1993
2023-09-10 08:42:05,494 [trainer.py] => tuned_epoch: 20
2023-09-10 08:42:05,494 [trainer.py] => init_lr: 0.02
2023-09-10 08:42:05,494 [trainer.py] => batch_size: 96
2023-09-10 08:42:05,494 [trainer.py] => use_A: False
2023-09-10 08:42:05,494 [trainer.py] => weight_decay: 0.0005
2023-09-10 08:42:05,494 [trainer.py] => min_lr: 0
2023-09-10 08:42:05,494 [trainer.py] => ffn_num: 64
2023-09-10 08:42:05,494 [trainer.py] => optimizer: sgd
2023-09-10 08:42:05,494 [trainer.py] => vpt_type: shallow
2023-09-10 08:42:05,494 [trainer.py] => prompt_token_num: 5
2023-09-10 08:42:05,690 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 08:42:07,645 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:42:08,151 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:42:08,220 [trainer.py] => All params: 86988288
2023-09-10 08:42:08,220 [trainer.py] => Trainable params: 1189632
2023-09-10 08:42:08,320 [adam_adapter.py] => Learning on 0-30
2023-09-10 08:42:48,310 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.209, Train_accy 35.71, Test_accy 78.17
2023-09-10 08:43:27,803 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 1.853, Train_accy 67.72, Test_accy 83.00
2023-09-10 08:44:07,727 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 0.867, Train_accy 75.67, Test_accy 86.33
2023-09-10 08:44:47,184 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 0.735, Train_accy 77.68, Test_accy 86.33
2023-09-10 08:45:27,197 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 0.683, Train_accy 78.82, Test_accy 90.67
2023-09-10 08:46:07,248 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 0.649, Train_accy 80.02, Test_accy 90.50
2023-09-10 08:46:47,163 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 0.613, Train_accy 81.04, Test_accy 89.17
2023-09-10 08:47:27,064 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 0.598, Train_accy 81.35, Test_accy 90.83
2023-09-10 08:48:07,010 [adam_adapter.py] => Task 0, Epoch 9/20 => Loss 0.594, Train_accy 81.40, Test_accy 90.67
2023-09-10 08:48:47,191 [adam_adapter.py] => Task 0, Epoch 10/20 => Loss 0.566, Train_accy 82.17, Test_accy 91.17
2023-09-10 08:49:27,042 [adam_adapter.py] => Task 0, Epoch 11/20 => Loss 0.553, Train_accy 82.56, Test_accy 90.50
2023-09-10 08:50:07,108 [adam_adapter.py] => Task 0, Epoch 12/20 => Loss 0.552, Train_accy 82.94, Test_accy 92.50
2023-09-10 08:50:46,981 [adam_adapter.py] => Task 0, Epoch 13/20 => Loss 0.539, Train_accy 82.81, Test_accy 91.17
2023-09-10 08:51:27,150 [adam_adapter.py] => Task 0, Epoch 14/20 => Loss 0.536, Train_accy 83.16, Test_accy 90.00
2023-09-10 08:52:07,236 [adam_adapter.py] => Task 0, Epoch 15/20 => Loss 0.547, Train_accy 83.13, Test_accy 90.83
2023-09-10 08:52:47,106 [adam_adapter.py] => Task 0, Epoch 16/20 => Loss 0.539, Train_accy 83.09, Test_accy 91.83
2023-09-10 08:53:27,007 [adam_adapter.py] => Task 0, Epoch 17/20 => Loss 0.522, Train_accy 84.10, Test_accy 92.00
2023-09-10 08:54:07,097 [adam_adapter.py] => Task 0, Epoch 18/20 => Loss 0.525, Train_accy 83.95, Test_accy 92.17
2023-09-10 08:54:46,950 [adam_adapter.py] => Task 0, Epoch 19/20 => Loss 0.525, Train_accy 84.33, Test_accy 92.00
2023-09-10 08:55:26,918 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.534, Train_accy 83.84, Test_accy 92.00
2023-09-10 08:55:28,089 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:55:28,598 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:55:29,400 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 08:55:29,662 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 08:56:22,221 [trainer.py] => No NME accuracy.
2023-09-10 08:56:22,221 [trainer.py] => CNN: {'total': 89.17, '00-09': 90.0, '10-19': 90.0, '20-29': 87.5, 'old': 0, 'new': 89.17}
2023-09-10 08:56:22,222 [trainer.py] => CNN top1 curve: [89.17]
2023-09-10 08:56:22,222 [trainer.py] => CNN top5 curve: [99.0]

2023-09-10 08:56:22,222 [trainer.py] => Average Accuracy (CNN): 89.17
2023-09-10 08:56:22,223 [trainer.py] => All params: 172833025
2023-09-10 08:56:22,223 [trainer.py] => Trainable params: 87034369
2023-09-10 08:56:22,225 [adam_adapter.py] => Learning on 30-60
2023-09-10 08:57:06,678 [trainer.py] => No NME accuracy.
2023-09-10 08:57:06,678 [trainer.py] => CNN: {'total': 89.16, '00-09': 85.0, '10-19': 90.0, '20-29': 84.0, '30-39': 92.5, '40-49': 88.5, '50-59': 94.97, 'old': 86.33, 'new': 91.99}
2023-09-10 08:57:06,678 [trainer.py] => CNN top1 curve: [89.17, 89.16]
2023-09-10 08:57:06,678 [trainer.py] => CNN top5 curve: [99.0, 98.58]

2023-09-10 08:57:06,678 [trainer.py] => Average Accuracy (CNN): 89.16499999999999
2023-09-10 08:57:06,680 [trainer.py] => All params: 172879105
2023-09-10 08:57:06,681 [trainer.py] => Trainable params: 87080449
2023-09-10 08:57:06,683 [adam_adapter.py] => Learning on 60-90
2023-09-10 08:57:53,838 [trainer.py] => No NME accuracy.
2023-09-10 08:57:53,839 [trainer.py] => CNN: {'total': 86.48, '00-09': 83.0, '10-19': 85.0, '20-29': 81.0, '30-39': 90.0, '40-49': 86.5, '50-59': 93.47, '60-69': 92.5, '70-79': 88.0, '80-89': 78.89, 'old': 86.49, 'new': 86.48}
2023-09-10 08:57:53,839 [trainer.py] => CNN top1 curve: [89.17, 89.16, 86.48]
2023-09-10 08:57:53,839 [trainer.py] => CNN top5 curve: [99.0, 98.58, 97.72]

2023-09-10 08:57:53,839 [trainer.py] => Average Accuracy (CNN): 88.27
2023-09-10 08:57:53,840 [trainer.py] => All params: 172925185
2023-09-10 08:57:53,842 [trainer.py] => Trainable params: 87126529
2023-09-10 08:57:53,844 [adam_adapter.py] => Learning on 90-120
2023-09-10 08:58:45,672 [trainer.py] => No NME accuracy.
2023-09-10 08:58:45,672 [trainer.py] => CNN: {'total': 82.51, '00-09': 81.0, '10-19': 82.0, '20-29': 80.0, '30-39': 89.0, '40-49': 85.0, '50-59': 86.93, '60-69': 87.5, '70-79': 84.5, '80-89': 75.88, '90-99': 81.41, '100-109': 77.89, '110-119': 78.89, 'old': 83.54, 'new': 79.4}
2023-09-10 08:58:45,672 [trainer.py] => CNN top1 curve: [89.17, 89.16, 86.48, 82.51]
2023-09-10 08:58:45,672 [trainer.py] => CNN top5 curve: [99.0, 98.58, 97.72, 96.37]

2023-09-10 08:58:45,672 [trainer.py] => Average Accuracy (CNN): 86.83
2023-09-10 08:58:45,673 [trainer.py] => All params: 172971265
2023-09-10 08:58:45,674 [trainer.py] => Trainable params: 87172609
2023-09-10 08:58:45,676 [adam_adapter.py] => Learning on 120-150
2023-09-10 08:59:40,628 [trainer.py] => No NME accuracy.
2023-09-10 08:59:40,628 [trainer.py] => CNN: {'total': 80.46, '00-09': 78.5, '10-19': 81.5, '20-29': 79.0, '30-39': 88.0, '40-49': 82.5, '50-59': 79.9, '60-69': 84.0, '70-79': 83.5, '80-89': 73.87, '90-99': 80.4, '100-109': 75.38, '110-119': 72.86, '120-129': 84.0, '130-139': 81.5, '140-149': 81.91, 'old': 79.96, 'new': 82.47}
2023-09-10 08:59:40,628 [trainer.py] => CNN top1 curve: [89.17, 89.16, 86.48, 82.51, 80.46]
2023-09-10 08:59:40,628 [trainer.py] => CNN top5 curve: [99.0, 98.58, 97.72, 96.37, 95.22]

2023-09-10 08:59:40,629 [trainer.py] => Average Accuracy (CNN): 85.556
2023-09-10 08:59:40,630 [trainer.py] => All params: 173017345
2023-09-10 08:59:40,632 [trainer.py] => Trainable params: 87218689
2023-09-10 08:59:40,635 [adam_adapter.py] => Learning on 150-180
2023-09-10 09:00:37,535 [trainer.py] => No NME accuracy.
2023-09-10 09:00:37,535 [trainer.py] => CNN: {'total': 77.67, '00-09': 76.0, '10-19': 78.5, '20-29': 76.0, '30-39': 85.0, '40-49': 79.0, '50-59': 78.39, '60-69': 83.5, '70-79': 80.5, '80-89': 71.36, '90-99': 79.9, '100-109': 73.87, '110-119': 71.36, '120-129': 83.5, '130-139': 80.0, '140-149': 79.9, '150-159': 79.9, '160-169': 73.87, '170-179': 67.5, 'old': 78.46, 'new': 73.75}
2023-09-10 09:00:37,535 [trainer.py] => CNN top1 curve: [89.17, 89.16, 86.48, 82.51, 80.46, 77.67]
2023-09-10 09:00:37,535 [trainer.py] => CNN top5 curve: [99.0, 98.58, 97.72, 96.37, 95.22, 93.76]

2023-09-10 09:00:37,535 [trainer.py] => Average Accuracy (CNN): 84.24166666666666
2023-09-10 09:00:37,537 [trainer.py] => All params: 173063425
2023-09-10 09:00:37,538 [trainer.py] => Trainable params: 87264769
2023-09-10 09:00:37,542 [adam_adapter.py] => Learning on 180-210
2023-09-10 09:01:37,890 [trainer.py] => No NME accuracy.
2023-09-10 09:01:37,890 [trainer.py] => CNN: {'total': 76.77, '00-09': 75.0, '10-19': 78.0, '20-29': 75.5, '30-39': 85.0, '40-49': 78.0, '50-59': 77.89, '60-69': 80.5, '70-79': 80.5, '80-89': 71.36, '90-99': 77.39, '100-109': 73.37, '110-119': 68.34, '120-129': 83.5, '130-139': 79.0, '140-149': 79.9, '150-159': 77.39, '160-169': 72.86, '170-179': 67.0, '180-189': 74.24, '190-199': 83.5, '200-209': 73.87, 'old': 76.7, 'new': 77.22}
2023-09-10 09:01:37,890 [trainer.py] => CNN top1 curve: [89.17, 89.16, 86.48, 82.51, 80.46, 77.67, 76.77]
2023-09-10 09:01:37,890 [trainer.py] => CNN top5 curve: [99.0, 98.58, 97.72, 96.37, 95.22, 93.76, 93.32]

2023-09-10 09:01:37,890 [trainer.py] => Average Accuracy (CNN): 83.17428571428572
2023-09-10 09:01:37,891 [trainer.py] => All params: 173109505
2023-09-10 09:01:37,893 [trainer.py] => Trainable params: 87310849
2023-09-10 09:01:37,896 [adam_adapter.py] => Learning on 210-240
2023-09-10 09:02:42,523 [trainer.py] => No NME accuracy.
2023-09-10 09:02:42,524 [trainer.py] => CNN: {'total': 75.17, '00-09': 74.0, '10-19': 75.5, '20-29': 74.5, '30-39': 85.0, '40-49': 76.5, '50-59': 77.89, '60-69': 80.0, '70-79': 80.5, '80-89': 71.36, '90-99': 77.39, '100-109': 65.83, '110-119': 68.34, '120-129': 79.5, '130-139': 79.0, '140-149': 78.89, '150-159': 76.88, '160-169': 72.86, '170-179': 65.0, '180-189': 74.24, '190-199': 83.0, '200-209': 70.35, '210-219': 49.0, '220-229': 87.94, '230-239': 80.5, 'old': 75.56, 'new': 72.45}
2023-09-10 09:02:42,524 [trainer.py] => CNN top1 curve: [89.17, 89.16, 86.48, 82.51, 80.46, 77.67, 76.77, 75.17]
2023-09-10 09:02:42,524 [trainer.py] => CNN top5 curve: [99.0, 98.58, 97.72, 96.37, 95.22, 93.76, 93.32, 92.23]

2023-09-10 09:02:42,524 [trainer.py] => Average Accuracy (CNN): 82.17375
2023-09-10 09:02:42,525 [trainer.py] => All params: 173155585
2023-09-10 09:02:42,527 [trainer.py] => Trainable params: 87356929
2023-09-10 09:02:42,531 [adam_adapter.py] => Learning on 240-270
2023-09-10 09:03:49,197 [trainer.py] => No NME accuracy.
2023-09-10 09:03:49,197 [trainer.py] => CNN: {'total': 74.44, '00-09': 74.0, '10-19': 75.5, '20-29': 74.5, '30-39': 84.0, '40-49': 76.5, '50-59': 77.89, '60-69': 78.0, '70-79': 80.0, '80-89': 70.85, '90-99': 75.88, '100-109': 64.82, '110-119': 64.82, '120-129': 75.0, '130-139': 79.0, '140-149': 74.37, '150-159': 73.87, '160-169': 72.86, '170-179': 65.0, '180-189': 74.24, '190-199': 82.0, '200-209': 67.84, '210-219': 49.0, '220-229': 87.94, '230-239': 80.0, '240-249': 71.5, '250-259': 77.39, '260-269': 83.0, 'old': 74.08, 'new': 77.3}
2023-09-10 09:03:49,197 [trainer.py] => CNN top1 curve: [89.17, 89.16, 86.48, 82.51, 80.46, 77.67, 76.77, 75.17, 74.44]
2023-09-10 09:03:49,197 [trainer.py] => CNN top5 curve: [99.0, 98.58, 97.72, 96.37, 95.22, 93.76, 93.32, 92.23, 92.33]

2023-09-10 09:03:49,197 [trainer.py] => Average Accuracy (CNN): 81.31444444444443
2023-09-10 09:03:49,199 [trainer.py] => All params: 173201665
2023-09-10 09:03:49,201 [trainer.py] => Trainable params: 87403009
2023-09-10 09:03:49,204 [adam_adapter.py] => Learning on 270-300
2023-09-10 09:04:57,242 [trainer.py] => No NME accuracy.
2023-09-10 09:04:57,243 [trainer.py] => CNN: {'total': 74.3, '00-09': 73.0, '10-19': 75.5, '20-29': 74.0, '30-39': 84.0, '40-49': 73.5, '50-59': 77.39, '60-69': 77.5, '70-79': 80.0, '80-89': 70.85, '90-99': 72.36, '100-109': 64.32, '110-119': 63.32, '120-129': 75.0, '130-139': 78.5, '140-149': 74.37, '150-159': 73.87, '160-169': 71.36, '170-179': 64.0, '180-189': 73.23, '190-199': 81.5, '200-209': 67.34, '210-219': 48.5, '220-229': 87.44, '230-239': 79.5, '240-249': 68.5, '250-259': 76.88, '260-269': 82.0, '270-279': 78.39, '280-289': 83.0, '290-299': 79.9, 'old': 73.62, 'new': 80.43}
2023-09-10 09:04:57,243 [trainer.py] => CNN top1 curve: [89.17, 89.16, 86.48, 82.51, 80.46, 77.67, 76.77, 75.17, 74.44, 74.3]
2023-09-10 09:04:57,243 [trainer.py] => CNN top5 curve: [99.0, 98.58, 97.72, 96.37, 95.22, 93.76, 93.32, 92.23, 92.33, 92.11]

2023-09-10 09:04:57,243 [trainer.py] => Average Accuracy (CNN): 80.61299999999999
2023-09-10 09:08:14,180 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 09:08:14,180 [trainer.py] => prefix:  
2023-09-10 09:08:14,180 [trainer.py] => dataset: omnibenchmark
2023-09-10 09:08:14,180 [trainer.py] => memory_size: 0
2023-09-10 09:08:14,180 [trainer.py] => memory_per_class: 0
2023-09-10 09:08:14,180 [trainer.py] => fixed_memory: False
2023-09-10 09:08:14,180 [trainer.py] => shuffle: True
2023-09-10 09:08:14,180 [trainer.py] => init_cls: 30
2023-09-10 09:08:14,180 [trainer.py] => increment: 30
2023-09-10 09:08:14,180 [trainer.py] => model_name: adam_adapter
2023-09-10 09:08:14,180 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 09:08:14,180 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 09:08:14,180 [trainer.py] => seed: 1993
2023-09-10 09:08:14,180 [trainer.py] => tuned_epoch: 20
2023-09-10 09:08:14,180 [trainer.py] => init_lr: 0.02
2023-09-10 09:08:14,180 [trainer.py] => batch_size: 96
2023-09-10 09:08:14,180 [trainer.py] => use_A: False
2023-09-10 09:08:14,180 [trainer.py] => weight_decay: 0.0005
2023-09-10 09:08:14,180 [trainer.py] => min_lr: 0
2023-09-10 09:08:14,180 [trainer.py] => ffn_num: 64
2023-09-10 09:08:14,180 [trainer.py] => optimizer: sgd
2023-09-10 09:08:14,180 [trainer.py] => vpt_type: shallow
2023-09-10 09:08:14,180 [trainer.py] => prompt_token_num: 5
2023-09-10 09:08:14,383 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 09:08:16,423 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 09:08:16,911 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 09:08:16,976 [trainer.py] => All params: 86988288
2023-09-10 09:08:16,976 [trainer.py] => Trainable params: 1189632
2023-09-10 09:08:17,078 [adam_adapter.py] => Learning on 0-30
2023-09-10 09:08:56,960 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.206, Train_accy 36.86, Test_accy 78.83
2023-09-10 09:09:36,519 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 1.849, Train_accy 67.14, Test_accy 82.83
2023-09-10 09:10:16,683 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 0.881, Train_accy 75.04, Test_accy 87.33
2023-09-10 09:10:56,388 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 0.752, Train_accy 77.38, Test_accy 86.17
2023-09-10 09:11:36,205 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 0.706, Train_accy 77.79, Test_accy 90.50
2023-09-10 09:12:16,353 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 0.662, Train_accy 79.51, Test_accy 89.50
2023-09-10 09:12:56,299 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 0.635, Train_accy 80.16, Test_accy 88.83
2023-09-10 09:13:36,324 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 0.620, Train_accy 80.70, Test_accy 90.67
2023-09-10 09:14:16,591 [adam_adapter.py] => Task 0, Epoch 9/20 => Loss 0.615, Train_accy 81.25, Test_accy 91.33
2023-09-10 09:14:56,806 [adam_adapter.py] => Task 0, Epoch 10/20 => Loss 0.590, Train_accy 81.21, Test_accy 92.50
2023-09-10 09:15:36,882 [adam_adapter.py] => Task 0, Epoch 11/20 => Loss 0.581, Train_accy 81.73, Test_accy 90.67
2023-09-10 09:16:16,994 [adam_adapter.py] => Task 0, Epoch 12/20 => Loss 0.580, Train_accy 81.95, Test_accy 92.67
2023-09-10 09:16:57,120 [adam_adapter.py] => Task 0, Epoch 13/20 => Loss 0.576, Train_accy 82.31, Test_accy 92.00
2023-09-10 09:17:37,309 [adam_adapter.py] => Task 0, Epoch 14/20 => Loss 0.556, Train_accy 82.78, Test_accy 91.00
2023-09-10 09:18:17,565 [adam_adapter.py] => Task 0, Epoch 15/20 => Loss 0.564, Train_accy 82.22, Test_accy 91.50
2023-09-10 09:18:57,652 [adam_adapter.py] => Task 0, Epoch 16/20 => Loss 0.563, Train_accy 82.50, Test_accy 92.17
2023-09-10 09:19:37,689 [adam_adapter.py] => Task 0, Epoch 17/20 => Loss 0.550, Train_accy 82.81, Test_accy 92.50
2023-09-10 09:20:17,734 [adam_adapter.py] => Task 0, Epoch 18/20 => Loss 0.543, Train_accy 83.23, Test_accy 92.50
2023-09-10 09:20:57,761 [adam_adapter.py] => Task 0, Epoch 19/20 => Loss 0.554, Train_accy 83.46, Test_accy 92.50
2023-09-10 09:21:37,797 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.549, Train_accy 82.97, Test_accy 92.50
2023-09-10 09:21:38,929 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 09:21:39,435 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 09:21:40,157 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 09:21:40,408 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 09:22:33,353 [trainer.py] => No NME accuracy.
2023-09-10 09:22:33,353 [trainer.py] => CNN: {'total': 89.17, '00-09': 90.0, '10-19': 90.0, '20-29': 87.5, 'old': 0, 'new': 89.17}
2023-09-10 09:22:33,353 [trainer.py] => CNN top1 curve: [89.17]
2023-09-10 09:22:33,353 [trainer.py] => CNN top5 curve: [99.17]

2023-09-10 09:22:33,353 [trainer.py] => Average Accuracy (CNN): 89.17
2023-09-10 09:22:33,354 [trainer.py] => All params: 172833025
2023-09-10 09:22:33,355 [trainer.py] => Trainable params: 87034369
2023-09-10 09:22:33,356 [adam_adapter.py] => Learning on 30-60
2023-09-10 09:23:18,240 [trainer.py] => No NME accuracy.
2023-09-10 09:23:18,240 [trainer.py] => CNN: {'total': 89.07, '00-09': 84.5, '10-19': 90.0, '20-29': 84.5, '30-39': 92.0, '40-49': 88.5, '50-59': 94.97, 'old': 86.33, 'new': 91.82}
2023-09-10 09:23:18,240 [trainer.py] => CNN top1 curve: [89.17, 89.07]
2023-09-10 09:23:18,240 [trainer.py] => CNN top5 curve: [99.17, 98.75]

2023-09-10 09:23:18,240 [trainer.py] => Average Accuracy (CNN): 89.12
2023-09-10 09:23:18,242 [trainer.py] => All params: 172879105
2023-09-10 09:23:18,243 [trainer.py] => Trainable params: 87080449
2023-09-10 09:23:18,246 [adam_adapter.py] => Learning on 60-90
2023-09-10 09:24:05,620 [trainer.py] => No NME accuracy.
2023-09-10 09:24:05,620 [trainer.py] => CNN: {'total': 86.43, '00-09': 82.5, '10-19': 84.5, '20-29': 81.5, '30-39': 89.5, '40-49': 86.5, '50-59': 93.47, '60-69': 92.5, '70-79': 88.5, '80-89': 78.89, 'old': 86.32, 'new': 86.64}
2023-09-10 09:24:05,620 [trainer.py] => CNN top1 curve: [89.17, 89.07, 86.43]
2023-09-10 09:24:05,620 [trainer.py] => CNN top5 curve: [99.17, 98.75, 97.66]

2023-09-10 09:24:05,621 [trainer.py] => Average Accuracy (CNN): 88.22333333333334
2023-09-10 09:24:05,622 [trainer.py] => All params: 172925185
2023-09-10 09:24:05,623 [trainer.py] => Trainable params: 87126529
2023-09-10 09:24:05,625 [adam_adapter.py] => Learning on 90-120
2023-09-10 09:24:57,122 [trainer.py] => No NME accuracy.
2023-09-10 09:24:57,122 [trainer.py] => CNN: {'total': 82.42, '00-09': 80.5, '10-19': 81.5, '20-29': 81.0, '30-39': 88.0, '40-49': 84.0, '50-59': 86.93, '60-69': 87.0, '70-79': 84.5, '80-89': 75.88, '90-99': 81.41, '100-109': 79.4, '110-119': 78.89, 'old': 83.26, 'new': 79.9}
2023-09-10 09:24:57,122 [trainer.py] => CNN top1 curve: [89.17, 89.07, 86.43, 82.42]
2023-09-10 09:24:57,123 [trainer.py] => CNN top5 curve: [99.17, 98.75, 97.66, 96.41]

2023-09-10 09:24:57,123 [trainer.py] => Average Accuracy (CNN): 86.77250000000001
2023-09-10 09:24:57,124 [trainer.py] => All params: 172971265
2023-09-10 09:24:57,124 [trainer.py] => Trainable params: 87172609
2023-09-10 09:24:57,126 [adam_adapter.py] => Learning on 120-150
2023-09-10 09:25:51,850 [trainer.py] => No NME accuracy.
2023-09-10 09:25:51,851 [trainer.py] => CNN: {'total': 80.39, '00-09': 77.5, '10-19': 81.5, '20-29': 79.5, '30-39': 87.5, '40-49': 81.5, '50-59': 80.4, '60-69': 83.5, '70-79': 83.5, '80-89': 73.87, '90-99': 80.4, '100-109': 75.88, '110-119': 72.86, '120-129': 84.0, '130-139': 81.0, '140-149': 82.91, 'old': 79.83, 'new': 82.64}
2023-09-10 09:25:51,851 [trainer.py] => CNN top1 curve: [89.17, 89.07, 86.43, 82.42, 80.39]
2023-09-10 09:25:51,851 [trainer.py] => CNN top5 curve: [99.17, 98.75, 97.66, 96.41, 95.09]

2023-09-10 09:25:51,851 [trainer.py] => Average Accuracy (CNN): 85.49600000000001
2023-09-10 09:25:51,852 [trainer.py] => All params: 173017345
2023-09-10 09:25:51,853 [trainer.py] => Trainable params: 87218689
2023-09-10 09:25:51,854 [adam_adapter.py] => Learning on 150-180
2023-09-10 09:26:48,428 [trainer.py] => No NME accuracy.
2023-09-10 09:26:48,429 [trainer.py] => CNN: {'total': 77.53, '00-09': 75.5, '10-19': 77.0, '20-29': 76.0, '30-39': 85.0, '40-49': 78.0, '50-59': 78.89, '60-69': 83.0, '70-79': 80.5, '80-89': 71.36, '90-99': 79.9, '100-109': 73.37, '110-119': 70.85, '120-129': 83.5, '130-139': 80.0, '140-149': 81.41, '150-159': 78.89, '160-169': 74.87, '170-179': 67.5, 'old': 78.29, 'new': 73.75}
2023-09-10 09:26:48,429 [trainer.py] => CNN top1 curve: [89.17, 89.07, 86.43, 82.42, 80.39, 77.53]
2023-09-10 09:26:48,429 [trainer.py] => CNN top5 curve: [99.17, 98.75, 97.66, 96.41, 95.09, 93.82]

2023-09-10 09:26:48,429 [trainer.py] => Average Accuracy (CNN): 84.16833333333334
2023-09-10 09:26:48,431 [trainer.py] => All params: 173063425
2023-09-10 09:26:48,431 [trainer.py] => Trainable params: 87264769
2023-09-10 09:26:48,433 [adam_adapter.py] => Learning on 180-210
2023-09-10 09:27:48,204 [trainer.py] => No NME accuracy.
2023-09-10 09:27:48,204 [trainer.py] => CNN: {'total': 76.7, '00-09': 74.5, '10-19': 76.5, '20-29': 76.0, '30-39': 85.0, '40-49': 77.0, '50-59': 78.39, '60-69': 80.0, '70-79': 80.5, '80-89': 70.35, '90-99': 77.39, '100-109': 72.86, '110-119': 67.84, '120-129': 83.5, '130-139': 79.0, '140-149': 81.41, '150-159': 75.88, '160-169': 73.87, '170-179': 67.5, '180-189': 75.25, '190-199': 83.5, '200-209': 74.37, 'old': 76.53, 'new': 77.72}
2023-09-10 09:27:48,204 [trainer.py] => CNN top1 curve: [89.17, 89.07, 86.43, 82.42, 80.39, 77.53, 76.7]
2023-09-10 09:27:48,204 [trainer.py] => CNN top5 curve: [99.17, 98.75, 97.66, 96.41, 95.09, 93.82, 93.41]

2023-09-10 09:27:48,204 [trainer.py] => Average Accuracy (CNN): 83.10142857142857
2023-09-10 09:27:48,205 [trainer.py] => All params: 173109505
2023-09-10 09:27:48,206 [trainer.py] => Trainable params: 87310849
2023-09-10 09:27:48,208 [adam_adapter.py] => Learning on 210-240
2023-09-10 09:28:52,498 [trainer.py] => No NME accuracy.
2023-09-10 09:28:52,498 [trainer.py] => CNN: {'total': 75.19, '00-09': 73.5, '10-19': 74.0, '20-29': 75.5, '30-39': 85.0, '40-49': 75.5, '50-59': 78.39, '60-69': 79.5, '70-79': 80.5, '80-89': 70.35, '90-99': 77.39, '100-109': 65.83, '110-119': 67.84, '120-129': 79.5, '130-139': 79.0, '140-149': 80.9, '150-159': 75.38, '160-169': 73.87, '170-179': 65.5, '180-189': 75.25, '190-199': 83.0, '200-209': 70.85, '210-219': 50.0, '220-229': 86.93, '230-239': 81.0, 'old': 75.56, 'new': 72.62}
2023-09-10 09:28:52,498 [trainer.py] => CNN top1 curve: [89.17, 89.07, 86.43, 82.42, 80.39, 77.53, 76.7, 75.19]
2023-09-10 09:28:52,498 [trainer.py] => CNN top5 curve: [99.17, 98.75, 97.66, 96.41, 95.09, 93.82, 93.41, 92.4]

2023-09-10 09:28:52,498 [trainer.py] => Average Accuracy (CNN): 82.11250000000001
2023-09-10 09:28:52,500 [trainer.py] => All params: 173155585
2023-09-10 09:28:52,500 [trainer.py] => Trainable params: 87356929
2023-09-10 09:28:52,506 [adam_adapter.py] => Learning on 240-270
2023-09-10 09:29:59,428 [trainer.py] => No NME accuracy.
2023-09-10 09:29:59,428 [trainer.py] => CNN: {'total': 74.42, '00-09': 73.5, '10-19': 73.5, '20-29': 75.5, '30-39': 84.0, '40-49': 75.5, '50-59': 77.89, '60-69': 78.0, '70-79': 80.0, '80-89': 69.85, '90-99': 75.88, '100-109': 65.33, '110-119': 64.82, '120-129': 75.0, '130-139': 78.5, '140-149': 75.88, '150-159': 72.86, '160-169': 73.87, '170-179': 65.5, '180-189': 75.25, '190-199': 82.0, '200-209': 67.84, '210-219': 50.0, '220-229': 86.93, '230-239': 80.5, '240-249': 71.5, '250-259': 77.39, '260-269': 82.5, 'old': 74.08, 'new': 77.13}
2023-09-10 09:29:59,428 [trainer.py] => CNN top1 curve: [89.17, 89.07, 86.43, 82.42, 80.39, 77.53, 76.7, 75.19, 74.42]
2023-09-10 09:29:59,428 [trainer.py] => CNN top5 curve: [99.17, 98.75, 97.66, 96.41, 95.09, 93.82, 93.41, 92.4, 92.54]

2023-09-10 09:29:59,428 [trainer.py] => Average Accuracy (CNN): 81.25777777777779
2023-09-10 09:29:59,430 [trainer.py] => All params: 173201665
2023-09-10 09:29:59,431 [trainer.py] => Trainable params: 87403009
2023-09-10 09:29:59,434 [adam_adapter.py] => Learning on 270-300
2023-09-10 09:31:07,002 [trainer.py] => No NME accuracy.
2023-09-10 09:31:07,003 [trainer.py] => CNN: {'total': 74.25, '00-09': 73.0, '10-19': 73.5, '20-29': 75.0, '30-39': 84.0, '40-49': 72.5, '50-59': 77.39, '60-69': 77.5, '70-79': 80.0, '80-89': 69.85, '90-99': 72.36, '100-109': 64.32, '110-119': 63.32, '120-129': 75.0, '130-139': 78.0, '140-149': 75.88, '150-159': 72.86, '160-169': 72.36, '170-179': 64.5, '180-189': 74.24, '190-199': 81.5, '200-209': 67.34, '210-219': 49.5, '220-229': 86.43, '230-239': 80.0, '240-249': 68.5, '250-259': 76.88, '260-269': 81.5, '270-279': 77.89, '280-289': 82.0, '290-299': 80.4, 'old': 73.6, 'new': 80.1}
2023-09-10 09:31:07,003 [trainer.py] => CNN top1 curve: [89.17, 89.07, 86.43, 82.42, 80.39, 77.53, 76.7, 75.19, 74.42, 74.25]
2023-09-10 09:31:07,003 [trainer.py] => CNN top5 curve: [99.17, 98.75, 97.66, 96.41, 95.09, 93.82, 93.41, 92.4, 92.54, 92.25]

2023-09-10 09:31:07,003 [trainer.py] => Average Accuracy (CNN): 80.557
2023-09-10 09:52:58,941 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 09:52:58,941 [trainer.py] => prefix:  
2023-09-10 09:52:58,942 [trainer.py] => dataset: omnibenchmark
2023-09-10 09:52:58,942 [trainer.py] => memory_size: 0
2023-09-10 09:52:58,942 [trainer.py] => memory_per_class: 0
2023-09-10 09:52:58,942 [trainer.py] => fixed_memory: False
2023-09-10 09:52:58,942 [trainer.py] => shuffle: True
2023-09-10 09:52:58,942 [trainer.py] => init_cls: 30
2023-09-10 09:52:58,942 [trainer.py] => increment: 30
2023-09-10 09:52:58,942 [trainer.py] => model_name: adam_adapter
2023-09-10 09:52:58,942 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 09:52:58,942 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 09:52:58,942 [trainer.py] => seed: 1993
2023-09-10 09:52:58,942 [trainer.py] => tuned_epoch: 20
2023-09-10 09:52:58,942 [trainer.py] => init_lr: 0.02
2023-09-10 09:52:58,942 [trainer.py] => batch_size: 96
2023-09-10 09:52:58,942 [trainer.py] => use_A: False
2023-09-10 09:52:58,942 [trainer.py] => weight_decay: 0.0005
2023-09-10 09:52:58,942 [trainer.py] => min_lr: 0
2023-09-10 09:52:58,942 [trainer.py] => ffn_num: 64
2023-09-10 09:52:58,942 [trainer.py] => optimizer: sgd
2023-09-10 09:52:58,942 [trainer.py] => vpt_type: shallow
2023-09-10 09:52:58,942 [trainer.py] => prompt_token_num: 5
2023-09-10 09:52:59,143 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 09:53:01,074 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 09:53:01,565 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 09:53:01,632 [trainer.py] => All params: 86988288
2023-09-10 09:53:01,632 [trainer.py] => Trainable params: 1189632
2023-09-10 09:53:01,740 [adam_adapter.py] => Learning on 0-30
2023-09-10 09:53:41,047 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 09:54:19,802 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 0.889, Train_accy 82.64, Test_accy 88.67
2023-09-10 09:54:59,205 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 0.554, Train_accy 85.15, Test_accy 89.83
2023-09-10 09:55:38,485 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 0.489, Train_accy 86.31, Test_accy 87.83
2023-09-10 09:56:17,872 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 0.457, Train_accy 86.68, Test_accy 90.33
2023-09-10 09:56:57,383 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 0.433, Train_accy 87.47, Test_accy 91.00
2023-09-10 09:57:37,109 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 0.410, Train_accy 88.13, Test_accy 91.33
2023-09-10 09:58:16,568 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 0.393, Train_accy 88.22, Test_accy 90.83
2023-09-10 09:58:56,145 [adam_adapter.py] => Task 0, Epoch 9/20 => Loss 0.393, Train_accy 88.73, Test_accy 92.17
2023-09-10 09:59:35,829 [adam_adapter.py] => Task 0, Epoch 10/20 => Loss 0.372, Train_accy 89.00, Test_accy 91.83
2023-09-10 10:00:15,652 [adam_adapter.py] => Task 0, Epoch 11/20 => Loss 0.362, Train_accy 89.23, Test_accy 91.83
2023-09-10 10:00:55,248 [adam_adapter.py] => Task 0, Epoch 12/20 => Loss 0.372, Train_accy 89.00, Test_accy 92.50
2023-09-10 10:01:34,841 [adam_adapter.py] => Task 0, Epoch 13/20 => Loss 0.355, Train_accy 89.64, Test_accy 92.50
2023-09-10 10:02:14,830 [adam_adapter.py] => Task 0, Epoch 14/20 => Loss 0.346, Train_accy 89.99, Test_accy 92.00
2023-09-10 10:02:54,706 [adam_adapter.py] => Task 0, Epoch 15/20 => Loss 0.351, Train_accy 89.86, Test_accy 91.83
2023-09-10 10:03:34,691 [adam_adapter.py] => Task 0, Epoch 16/20 => Loss 0.351, Train_accy 89.71, Test_accy 92.17
2023-09-10 10:04:14,705 [adam_adapter.py] => Task 0, Epoch 17/20 => Loss 0.341, Train_accy 90.28, Test_accy 92.50
2023-09-10 10:04:54,233 [adam_adapter.py] => Task 0, Epoch 18/20 => Loss 0.341, Train_accy 90.06, Test_accy 92.50
2023-09-10 10:05:34,029 [adam_adapter.py] => Task 0, Epoch 19/20 => Loss 0.338, Train_accy 90.49, Test_accy 92.50
2023-09-10 10:06:13,764 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.345, Train_accy 90.13, Test_accy 92.50
2023-09-10 10:06:14,944 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 10:06:15,448 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 10:06:16,192 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 10:06:16,450 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 10:07:09,057 [trainer.py] => No NME accuracy.
2023-09-10 10:07:09,057 [trainer.py] => CNN: {'total': 89.33, '00-09': 88.5, '10-19': 90.5, '20-29': 89.0, 'old': 0, 'new': 89.33}
2023-09-10 10:07:09,057 [trainer.py] => CNN top1 curve: [89.33]
2023-09-10 10:07:09,057 [trainer.py] => CNN top5 curve: [98.83]

2023-09-10 10:07:09,057 [trainer.py] => Average Accuracy (CNN): 89.33
2023-09-10 10:07:09,059 [trainer.py] => All params: 172833025
2023-09-10 10:07:09,060 [trainer.py] => Trainable params: 87034369
2023-09-10 10:07:09,061 [adam_adapter.py] => Learning on 30-60
2023-09-10 10:07:53,909 [trainer.py] => No NME accuracy.
2023-09-10 10:07:53,909 [trainer.py] => CNN: {'total': 89.32, '00-09': 84.0, '10-19': 90.5, '20-29': 86.0, '30-39': 92.0, '40-49': 89.0, '50-59': 94.47, 'old': 86.83, 'new': 91.82}
2023-09-10 10:07:53,909 [trainer.py] => CNN top1 curve: [89.33, 89.32]
2023-09-10 10:07:53,909 [trainer.py] => CNN top5 curve: [98.83, 98.67]

2023-09-10 10:07:53,909 [trainer.py] => Average Accuracy (CNN): 89.32499999999999
2023-09-10 10:07:53,911 [trainer.py] => All params: 172879105
2023-09-10 10:07:53,913 [trainer.py] => Trainable params: 87080449
2023-09-10 10:07:53,915 [adam_adapter.py] => Learning on 60-90
2023-09-10 10:08:41,532 [trainer.py] => No NME accuracy.
2023-09-10 10:08:41,533 [trainer.py] => CNN: {'total': 86.65, '00-09': 81.5, '10-19': 84.5, '20-29': 82.5, '30-39': 89.5, '40-49': 87.0, '50-59': 93.47, '60-69': 93.5, '70-79': 87.5, '80-89': 80.4, 'old': 86.41, 'new': 87.15}
2023-09-10 10:08:41,533 [trainer.py] => CNN top1 curve: [89.33, 89.32, 86.65]
2023-09-10 10:08:41,533 [trainer.py] => CNN top5 curve: [98.83, 98.67, 97.61]

2023-09-10 10:08:41,533 [trainer.py] => Average Accuracy (CNN): 88.43333333333332
2023-09-10 10:08:41,534 [trainer.py] => All params: 172925185
2023-09-10 10:08:41,534 [trainer.py] => Trainable params: 87126529
2023-09-10 10:08:41,538 [adam_adapter.py] => Learning on 90-120
2023-09-10 10:09:33,248 [trainer.py] => No NME accuracy.
2023-09-10 10:09:33,248 [trainer.py] => CNN: {'total': 82.71, '00-09': 79.5, '10-19': 82.5, '20-29': 81.0, '30-39': 88.5, '40-49': 84.5, '50-59': 85.93, '60-69': 88.5, '70-79': 84.0, '80-89': 76.38, '90-99': 80.9, '100-109': 79.4, '110-119': 81.41, 'old': 83.43, 'new': 80.57}
2023-09-10 10:09:33,248 [trainer.py] => CNN top1 curve: [89.33, 89.32, 86.65, 82.71]
2023-09-10 10:09:33,248 [trainer.py] => CNN top5 curve: [98.83, 98.67, 97.61, 96.53]

2023-09-10 10:09:33,248 [trainer.py] => Average Accuracy (CNN): 87.00249999999998
2023-09-10 10:09:33,250 [trainer.py] => All params: 172971265
2023-09-10 10:09:33,251 [trainer.py] => Trainable params: 87172609
2023-09-10 10:09:33,253 [adam_adapter.py] => Learning on 120-150
2023-09-10 10:10:28,060 [trainer.py] => No NME accuracy.
2023-09-10 10:10:28,060 [trainer.py] => CNN: {'total': 80.73, '00-09': 77.5, '10-19': 81.5, '20-29': 79.5, '30-39': 87.5, '40-49': 81.5, '50-59': 80.9, '60-69': 85.5, '70-79': 83.0, '80-89': 74.87, '90-99': 79.9, '100-109': 76.38, '110-119': 75.88, '120-129': 84.0, '130-139': 82.5, '140-149': 80.4, 'old': 80.33, 'new': 82.3}
2023-09-10 10:10:28,060 [trainer.py] => CNN top1 curve: [89.33, 89.32, 86.65, 82.71, 80.73]
2023-09-10 10:10:28,060 [trainer.py] => CNN top5 curve: [98.83, 98.67, 97.61, 96.53, 95.22]

2023-09-10 10:10:28,060 [trainer.py] => Average Accuracy (CNN): 85.74799999999999
2023-09-10 10:10:28,062 [trainer.py] => All params: 173017345
2023-09-10 10:10:28,063 [trainer.py] => Trainable params: 87218689
2023-09-10 10:10:28,066 [adam_adapter.py] => Learning on 150-180
2023-09-10 10:11:24,662 [trainer.py] => No NME accuracy.
2023-09-10 10:11:24,662 [trainer.py] => CNN: {'total': 77.78, '00-09': 75.0, '10-19': 78.5, '20-29': 77.0, '30-39': 84.5, '40-49': 78.0, '50-59': 79.4, '60-69': 83.5, '70-79': 80.5, '80-89': 71.86, '90-99': 79.9, '100-109': 73.87, '110-119': 74.87, '120-129': 83.5, '130-139': 81.5, '140-149': 78.39, '150-159': 77.89, '160-169': 75.38, '170-179': 66.5, 'old': 78.69, 'new': 73.24}
2023-09-10 10:11:24,662 [trainer.py] => CNN top1 curve: [89.33, 89.32, 86.65, 82.71, 80.73, 77.78]
2023-09-10 10:11:24,663 [trainer.py] => CNN top5 curve: [98.83, 98.67, 97.61, 96.53, 95.22, 94.04]

2023-09-10 10:11:24,663 [trainer.py] => Average Accuracy (CNN): 84.42
2023-09-10 10:11:24,664 [trainer.py] => All params: 173063425
2023-09-10 10:11:24,666 [trainer.py] => Trainable params: 87264769
2023-09-10 10:11:24,669 [adam_adapter.py] => Learning on 180-210
2023-09-10 10:12:24,688 [trainer.py] => No NME accuracy.
2023-09-10 10:12:24,688 [trainer.py] => CNN: {'total': 76.99, '00-09': 74.5, '10-19': 78.5, '20-29': 77.0, '30-39': 84.5, '40-49': 76.5, '50-59': 78.89, '60-69': 81.0, '70-79': 80.0, '80-89': 71.86, '90-99': 77.39, '100-109': 73.37, '110-119': 71.36, '120-129': 83.5, '130-139': 80.5, '140-149': 78.39, '150-159': 74.87, '160-169': 74.87, '170-179': 66.5, '180-189': 75.25, '190-199': 84.0, '200-209': 73.87, 'old': 76.87, 'new': 77.72}
2023-09-10 10:12:24,688 [trainer.py] => CNN top1 curve: [89.33, 89.32, 86.65, 82.71, 80.73, 77.78, 76.99]
2023-09-10 10:12:24,688 [trainer.py] => CNN top5 curve: [98.83, 98.67, 97.61, 96.53, 95.22, 94.04, 93.6]

2023-09-10 10:12:24,688 [trainer.py] => Average Accuracy (CNN): 83.35857142857142
2023-09-10 10:12:24,689 [trainer.py] => All params: 173109505
2023-09-10 10:12:24,690 [trainer.py] => Trainable params: 87310849
2023-09-10 10:12:24,692 [adam_adapter.py] => Learning on 210-240
2023-09-10 10:13:29,368 [trainer.py] => No NME accuracy.
2023-09-10 10:13:29,368 [trainer.py] => CNN: {'total': 75.33, '00-09': 73.0, '10-19': 76.0, '20-29': 76.0, '30-39': 84.5, '40-49': 75.0, '50-59': 78.89, '60-69': 81.0, '70-79': 80.0, '80-89': 71.86, '90-99': 77.39, '100-109': 65.83, '110-119': 70.85, '120-129': 80.5, '130-139': 80.5, '140-149': 77.39, '150-159': 74.87, '160-169': 74.87, '170-179': 64.0, '180-189': 74.75, '190-199': 83.5, '200-209': 70.35, '210-219': 50.0, '220-229': 86.43, '230-239': 80.5, 'old': 75.77, 'new': 72.29}
2023-09-10 10:13:29,369 [trainer.py] => CNN top1 curve: [89.33, 89.32, 86.65, 82.71, 80.73, 77.78, 76.99, 75.33]
2023-09-10 10:13:29,369 [trainer.py] => CNN top5 curve: [98.83, 98.67, 97.61, 96.53, 95.22, 94.04, 93.6, 92.19]

2023-09-10 10:13:29,369 [trainer.py] => Average Accuracy (CNN): 82.355
2023-09-10 10:13:29,370 [trainer.py] => All params: 173155585
2023-09-10 10:13:29,370 [trainer.py] => Trainable params: 87356929
2023-09-10 10:13:29,373 [adam_adapter.py] => Learning on 240-270
2023-09-10 10:14:36,311 [trainer.py] => No NME accuracy.
2023-09-10 10:14:36,312 [trainer.py] => CNN: {'total': 74.42, '00-09': 73.0, '10-19': 76.0, '20-29': 76.0, '30-39': 83.5, '40-49': 75.0, '50-59': 78.39, '60-69': 79.5, '70-79': 79.5, '80-89': 70.85, '90-99': 75.88, '100-109': 64.82, '110-119': 67.34, '120-129': 75.5, '130-139': 79.5, '140-149': 72.86, '150-159': 72.36, '160-169': 74.87, '170-179': 64.0, '180-189': 74.75, '190-199': 82.0, '200-209': 67.84, '210-219': 50.0, '220-229': 85.93, '230-239': 80.0, '240-249': 70.5, '250-259': 76.88, '260-269': 82.5, 'old': 74.14, 'new': 76.63}
2023-09-10 10:14:36,312 [trainer.py] => CNN top1 curve: [89.33, 89.32, 86.65, 82.71, 80.73, 77.78, 76.99, 75.33, 74.42]
2023-09-10 10:14:36,312 [trainer.py] => CNN top5 curve: [98.83, 98.67, 97.61, 96.53, 95.22, 94.04, 93.6, 92.19, 92.31]

2023-09-10 10:14:36,312 [trainer.py] => Average Accuracy (CNN): 81.47333333333333
2023-09-10 10:14:36,313 [trainer.py] => All params: 173201665
2023-09-10 10:14:36,315 [trainer.py] => Trainable params: 87403009
2023-09-10 10:14:36,319 [adam_adapter.py] => Learning on 270-300
2023-09-10 10:15:44,028 [trainer.py] => No NME accuracy.
2023-09-10 10:15:44,028 [trainer.py] => CNN: {'total': 74.37, '00-09': 73.0, '10-19': 76.0, '20-29': 75.5, '30-39': 83.5, '40-49': 72.5, '50-59': 77.89, '60-69': 79.5, '70-79': 79.5, '80-89': 70.35, '90-99': 72.36, '100-109': 64.32, '110-119': 65.83, '120-129': 75.5, '130-139': 78.5, '140-149': 72.86, '150-159': 72.36, '160-169': 73.37, '170-179': 62.5, '180-189': 74.24, '190-199': 81.5, '200-209': 67.34, '210-219': 49.0, '220-229': 85.93, '230-239': 79.5, '240-249': 68.0, '250-259': 76.38, '260-269': 81.5, '270-279': 79.9, '280-289': 84.0, '290-299': 78.39, 'old': 73.66, 'new': 80.77}
2023-09-10 10:15:44,029 [trainer.py] => CNN top1 curve: [89.33, 89.32, 86.65, 82.71, 80.73, 77.78, 76.99, 75.33, 74.42, 74.37]
2023-09-10 10:15:44,029 [trainer.py] => CNN top5 curve: [98.83, 98.67, 97.61, 96.53, 95.22, 94.04, 93.6, 92.19, 92.31, 92.26]

2023-09-10 10:15:44,029 [trainer.py] => Average Accuracy (CNN): 80.763
2023-09-10 10:38:43,048 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 10:38:43,048 [trainer.py] => prefix:  
2023-09-10 10:38:43,049 [trainer.py] => dataset: omnibenchmark
2023-09-10 10:38:43,049 [trainer.py] => memory_size: 0
2023-09-10 10:38:43,049 [trainer.py] => memory_per_class: 0
2023-09-10 10:38:43,049 [trainer.py] => fixed_memory: False
2023-09-10 10:38:43,049 [trainer.py] => shuffle: True
2023-09-10 10:38:43,049 [trainer.py] => init_cls: 30
2023-09-10 10:38:43,049 [trainer.py] => increment: 30
2023-09-10 10:38:43,049 [trainer.py] => model_name: adam_adapter
2023-09-10 10:38:43,049 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 10:38:43,049 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 10:38:43,050 [trainer.py] => seed: 1993
2023-09-10 10:38:43,050 [trainer.py] => tuned_epoch: 20
2023-09-10 10:38:43,050 [trainer.py] => init_lr: 0.02
2023-09-10 10:38:43,050 [trainer.py] => batch_size: 96
2023-09-10 10:38:43,050 [trainer.py] => use_A: False
2023-09-10 10:38:43,050 [trainer.py] => weight_decay: 0.0005
2023-09-10 10:38:43,050 [trainer.py] => min_lr: 0
2023-09-10 10:38:43,050 [trainer.py] => ffn_num: 64
2023-09-10 10:38:43,050 [trainer.py] => optimizer: sgd
2023-09-10 10:38:43,050 [trainer.py] => vpt_type: shallow
2023-09-10 10:38:43,050 [trainer.py] => prompt_token_num: 5
2023-09-10 10:38:43,852 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 10:38:46,415 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 10:38:46,932 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 10:38:47,040 [trainer.py] => All params: 86988288
2023-09-10 10:38:47,054 [trainer.py] => Trainable params: 1189632
2023-09-10 10:38:47,170 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:05:52,566 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:05:52,566 [trainer.py] => prefix:  
2023-09-10 11:05:52,566 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:05:52,566 [trainer.py] => memory_size: 0
2023-09-10 11:05:52,567 [trainer.py] => memory_per_class: 0
2023-09-10 11:05:52,567 [trainer.py] => fixed_memory: False
2023-09-10 11:05:52,567 [trainer.py] => shuffle: True
2023-09-10 11:05:52,567 [trainer.py] => init_cls: 30
2023-09-10 11:05:52,567 [trainer.py] => increment: 30
2023-09-10 11:05:52,567 [trainer.py] => model_name: adam_adapter
2023-09-10 11:05:52,567 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:05:52,567 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:05:52,567 [trainer.py] => seed: 1993
2023-09-10 11:05:52,567 [trainer.py] => tuned_epoch: 20
2023-09-10 11:05:52,567 [trainer.py] => init_lr: 0.02
2023-09-10 11:05:52,568 [trainer.py] => batch_size: 96
2023-09-10 11:05:52,568 [trainer.py] => use_A: False
2023-09-10 11:05:52,568 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:05:52,568 [trainer.py] => min_lr: 0
2023-09-10 11:05:52,568 [trainer.py] => ffn_num: 64
2023-09-10 11:05:52,568 [trainer.py] => optimizer: sgd
2023-09-10 11:05:52,568 [trainer.py] => vpt_type: shallow
2023-09-10 11:05:52,568 [trainer.py] => prompt_token_num: 5
2023-09-10 11:05:53,268 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:06:20,781 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:06:20,781 [trainer.py] => prefix:  
2023-09-10 11:06:20,781 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:06:20,781 [trainer.py] => memory_size: 0
2023-09-10 11:06:20,781 [trainer.py] => memory_per_class: 0
2023-09-10 11:06:20,781 [trainer.py] => fixed_memory: False
2023-09-10 11:06:20,781 [trainer.py] => shuffle: True
2023-09-10 11:06:20,781 [trainer.py] => init_cls: 30
2023-09-10 11:06:20,781 [trainer.py] => increment: 30
2023-09-10 11:06:20,781 [trainer.py] => model_name: adam_adapter
2023-09-10 11:06:20,781 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:06:20,781 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:06:20,781 [trainer.py] => seed: 1993
2023-09-10 11:06:20,781 [trainer.py] => tuned_epoch: 20
2023-09-10 11:06:20,781 [trainer.py] => init_lr: 0.02
2023-09-10 11:06:20,781 [trainer.py] => batch_size: 96
2023-09-10 11:06:20,781 [trainer.py] => use_A: False
2023-09-10 11:06:20,781 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:06:20,781 [trainer.py] => min_lr: 0
2023-09-10 11:06:20,781 [trainer.py] => ffn_num: 64
2023-09-10 11:06:20,781 [trainer.py] => optimizer: sgd
2023-09-10 11:06:20,781 [trainer.py] => vpt_type: shallow
2023-09-10 11:06:20,781 [trainer.py] => prompt_token_num: 5
2023-09-10 11:06:20,979 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:08:54,537 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:08:54,537 [trainer.py] => prefix:  
2023-09-10 11:08:54,537 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:08:54,537 [trainer.py] => memory_size: 0
2023-09-10 11:08:54,537 [trainer.py] => memory_per_class: 0
2023-09-10 11:08:54,537 [trainer.py] => fixed_memory: False
2023-09-10 11:08:54,537 [trainer.py] => shuffle: True
2023-09-10 11:08:54,537 [trainer.py] => init_cls: 30
2023-09-10 11:08:54,537 [trainer.py] => increment: 30
2023-09-10 11:08:54,537 [trainer.py] => model_name: adam_adapter
2023-09-10 11:08:54,537 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:08:54,537 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:08:54,537 [trainer.py] => seed: 1993
2023-09-10 11:08:54,537 [trainer.py] => tuned_epoch: 20
2023-09-10 11:08:54,537 [trainer.py] => init_lr: 0.02
2023-09-10 11:08:54,537 [trainer.py] => batch_size: 96
2023-09-10 11:08:54,537 [trainer.py] => use_A: False
2023-09-10 11:08:54,537 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:08:54,537 [trainer.py] => min_lr: 0
2023-09-10 11:08:54,537 [trainer.py] => ffn_num: 64
2023-09-10 11:08:54,538 [trainer.py] => optimizer: sgd
2023-09-10 11:08:54,538 [trainer.py] => vpt_type: shallow
2023-09-10 11:08:54,538 [trainer.py] => prompt_token_num: 5
2023-09-10 11:08:54,751 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:08:56,659 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:08:57,159 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:08:57,219 [trainer.py] => All params: 87431424
2023-09-10 11:08:57,220 [trainer.py] => Trainable params: 1632768
2023-09-10 11:08:57,325 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:09:55,037 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:09:55,037 [trainer.py] => prefix:  
2023-09-10 11:09:55,037 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:09:55,037 [trainer.py] => memory_size: 0
2023-09-10 11:09:55,037 [trainer.py] => memory_per_class: 0
2023-09-10 11:09:55,037 [trainer.py] => fixed_memory: False
2023-09-10 11:09:55,037 [trainer.py] => shuffle: True
2023-09-10 11:09:55,037 [trainer.py] => init_cls: 30
2023-09-10 11:09:55,037 [trainer.py] => increment: 30
2023-09-10 11:09:55,037 [trainer.py] => model_name: adam_adapter
2023-09-10 11:09:55,037 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:09:55,037 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:09:55,037 [trainer.py] => seed: 1993
2023-09-10 11:09:55,037 [trainer.py] => tuned_epoch: 20
2023-09-10 11:09:55,037 [trainer.py] => init_lr: 0.02
2023-09-10 11:09:55,037 [trainer.py] => batch_size: 96
2023-09-10 11:09:55,037 [trainer.py] => use_A: False
2023-09-10 11:09:55,037 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:09:55,037 [trainer.py] => min_lr: 0
2023-09-10 11:09:55,037 [trainer.py] => ffn_num: 64
2023-09-10 11:09:55,038 [trainer.py] => optimizer: sgd
2023-09-10 11:09:55,038 [trainer.py] => vpt_type: shallow
2023-09-10 11:09:55,038 [trainer.py] => prompt_token_num: 5
2023-09-10 11:09:55,232 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:09:57,174 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:09:57,702 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:09:57,769 [trainer.py] => All params: 87431424
2023-09-10 11:09:57,770 [trainer.py] => Trainable params: 1632768
2023-09-10 11:09:57,864 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:10:11,153 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:10:11,153 [trainer.py] => prefix:  
2023-09-10 11:10:11,153 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:10:11,153 [trainer.py] => memory_size: 0
2023-09-10 11:10:11,153 [trainer.py] => memory_per_class: 0
2023-09-10 11:10:11,153 [trainer.py] => fixed_memory: False
2023-09-10 11:10:11,153 [trainer.py] => shuffle: True
2023-09-10 11:10:11,153 [trainer.py] => init_cls: 30
2023-09-10 11:10:11,153 [trainer.py] => increment: 30
2023-09-10 11:10:11,153 [trainer.py] => model_name: adam_adapter
2023-09-10 11:10:11,153 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:10:11,153 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:10:11,153 [trainer.py] => seed: 1993
2023-09-10 11:10:11,153 [trainer.py] => tuned_epoch: 20
2023-09-10 11:10:11,153 [trainer.py] => init_lr: 0.02
2023-09-10 11:10:11,153 [trainer.py] => batch_size: 96
2023-09-10 11:10:11,153 [trainer.py] => use_A: False
2023-09-10 11:10:11,153 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:10:11,153 [trainer.py] => min_lr: 0
2023-09-10 11:10:11,154 [trainer.py] => ffn_num: 64
2023-09-10 11:10:11,154 [trainer.py] => optimizer: sgd
2023-09-10 11:10:11,154 [trainer.py] => vpt_type: shallow
2023-09-10 11:10:11,154 [trainer.py] => prompt_token_num: 5
2023-09-10 11:10:11,350 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:10:13,313 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:10:13,817 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:10:13,889 [trainer.py] => All params: 87431424
2023-09-10 11:10:13,889 [trainer.py] => Trainable params: 1632768
2023-09-10 11:10:14,001 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:11:34,971 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:11:34,972 [trainer.py] => prefix:  
2023-09-10 11:11:34,972 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:11:34,972 [trainer.py] => memory_size: 0
2023-09-10 11:11:34,972 [trainer.py] => memory_per_class: 0
2023-09-10 11:11:34,972 [trainer.py] => fixed_memory: False
2023-09-10 11:11:34,972 [trainer.py] => shuffle: True
2023-09-10 11:11:34,972 [trainer.py] => init_cls: 30
2023-09-10 11:11:34,972 [trainer.py] => increment: 30
2023-09-10 11:11:34,972 [trainer.py] => model_name: adam_adapter
2023-09-10 11:11:34,972 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:11:34,973 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:11:34,973 [trainer.py] => seed: 1993
2023-09-10 11:11:34,973 [trainer.py] => tuned_epoch: 20
2023-09-10 11:11:34,973 [trainer.py] => init_lr: 0.02
2023-09-10 11:11:34,973 [trainer.py] => batch_size: 96
2023-09-10 11:11:34,973 [trainer.py] => use_A: False
2023-09-10 11:11:34,973 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:11:34,973 [trainer.py] => min_lr: 0
2023-09-10 11:11:34,973 [trainer.py] => ffn_num: 64
2023-09-10 11:11:34,973 [trainer.py] => optimizer: sgd
2023-09-10 11:11:34,973 [trainer.py] => vpt_type: shallow
2023-09-10 11:11:34,974 [trainer.py] => prompt_token_num: 5
2023-09-10 11:11:35,697 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:11:38,298 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:11:38,816 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:11:38,922 [trainer.py] => All params: 87431424
2023-09-10 11:11:38,935 [trainer.py] => Trainable params: 1632768
2023-09-10 11:11:39,044 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:13:52,306 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:13:52,306 [trainer.py] => prefix:  
2023-09-10 11:13:52,306 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:13:52,306 [trainer.py] => memory_size: 0
2023-09-10 11:13:52,306 [trainer.py] => memory_per_class: 0
2023-09-10 11:13:52,306 [trainer.py] => fixed_memory: False
2023-09-10 11:13:52,307 [trainer.py] => shuffle: True
2023-09-10 11:13:52,307 [trainer.py] => init_cls: 30
2023-09-10 11:13:52,307 [trainer.py] => increment: 30
2023-09-10 11:13:52,307 [trainer.py] => model_name: adam_adapter
2023-09-10 11:13:52,307 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:13:52,307 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:13:52,307 [trainer.py] => seed: 1993
2023-09-10 11:13:52,307 [trainer.py] => tuned_epoch: 20
2023-09-10 11:13:52,307 [trainer.py] => init_lr: 0.02
2023-09-10 11:13:52,307 [trainer.py] => batch_size: 96
2023-09-10 11:13:52,307 [trainer.py] => use_A: False
2023-09-10 11:13:52,308 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:13:52,308 [trainer.py] => min_lr: 0
2023-09-10 11:13:52,308 [trainer.py] => ffn_num: 64
2023-09-10 11:13:52,308 [trainer.py] => optimizer: sgd
2023-09-10 11:13:52,308 [trainer.py] => vpt_type: shallow
2023-09-10 11:13:52,308 [trainer.py] => prompt_token_num: 5
2023-09-10 11:13:53,028 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:13:55,796 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:13:56,311 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:13:56,413 [trainer.py] => All params: 87431424
2023-09-10 11:13:56,425 [trainer.py] => Trainable params: 1632768
2023-09-10 11:13:56,527 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:16:37,015 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:16:37,015 [trainer.py] => prefix:  
2023-09-10 11:16:37,015 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:16:37,015 [trainer.py] => memory_size: 0
2023-09-10 11:16:37,015 [trainer.py] => memory_per_class: 0
2023-09-10 11:16:37,015 [trainer.py] => fixed_memory: False
2023-09-10 11:16:37,015 [trainer.py] => shuffle: True
2023-09-10 11:16:37,015 [trainer.py] => init_cls: 30
2023-09-10 11:16:37,015 [trainer.py] => increment: 30
2023-09-10 11:16:37,015 [trainer.py] => model_name: adam_adapter
2023-09-10 11:16:37,015 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:16:37,015 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:16:37,015 [trainer.py] => seed: 1993
2023-09-10 11:16:37,015 [trainer.py] => tuned_epoch: 20
2023-09-10 11:16:37,015 [trainer.py] => init_lr: 0.02
2023-09-10 11:16:37,015 [trainer.py] => batch_size: 96
2023-09-10 11:16:37,015 [trainer.py] => use_A: False
2023-09-10 11:16:37,015 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:16:37,015 [trainer.py] => min_lr: 0
2023-09-10 11:16:37,015 [trainer.py] => ffn_num: 64
2023-09-10 11:16:37,015 [trainer.py] => optimizer: sgd
2023-09-10 11:16:37,016 [trainer.py] => vpt_type: shallow
2023-09-10 11:16:37,016 [trainer.py] => prompt_token_num: 5
2023-09-10 11:16:37,212 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:16:39,179 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:16:39,679 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:16:39,745 [trainer.py] => All params: 87431424
2023-09-10 11:16:39,745 [trainer.py] => Trainable params: 1632768
2023-09-10 11:16:39,857 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:17:22,734 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.398, Train_accy 3.81, Test_accy 5.17
2023-09-10 11:18:03,230 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 3.396, Train_accy 4.11, Test_accy 3.33
2023-09-10 11:18:44,335 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 3.396, Train_accy 4.02, Test_accy 3.33
2023-09-10 11:19:24,804 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 3.395, Train_accy 4.33, Test_accy 3.33
2023-09-10 11:20:05,838 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 3.395, Train_accy 4.45, Test_accy 3.33
2023-09-10 11:20:46,124 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:20:46,124 [trainer.py] => prefix:  
2023-09-10 11:20:46,125 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:20:46,125 [trainer.py] => memory_size: 0
2023-09-10 11:20:46,125 [trainer.py] => memory_per_class: 0
2023-09-10 11:20:46,125 [trainer.py] => fixed_memory: False
2023-09-10 11:20:46,125 [trainer.py] => shuffle: True
2023-09-10 11:20:46,125 [trainer.py] => init_cls: 30
2023-09-10 11:20:46,125 [trainer.py] => increment: 30
2023-09-10 11:20:46,125 [trainer.py] => model_name: adam_adapter
2023-09-10 11:20:46,125 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:20:46,125 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:20:46,126 [trainer.py] => seed: 1993
2023-09-10 11:20:46,126 [trainer.py] => tuned_epoch: 20
2023-09-10 11:20:46,126 [trainer.py] => init_lr: 0.02
2023-09-10 11:20:46,126 [trainer.py] => batch_size: 96
2023-09-10 11:20:46,126 [trainer.py] => use_A: False
2023-09-10 11:20:46,126 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:20:46,126 [trainer.py] => min_lr: 0
2023-09-10 11:20:46,126 [trainer.py] => ffn_num: 64
2023-09-10 11:20:46,126 [trainer.py] => optimizer: sgd
2023-09-10 11:20:46,126 [trainer.py] => vpt_type: shallow
2023-09-10 11:20:46,126 [trainer.py] => prompt_token_num: 5
2023-09-10 11:20:46,844 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:22:02,737 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:22:03,247 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:22:03,348 [trainer.py] => All params: 87431424
2023-09-10 11:22:03,360 [trainer.py] => Trainable params: 1632768
2023-09-10 11:22:03,471 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:23:47,183 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:23:47,183 [trainer.py] => prefix:  
2023-09-10 11:23:47,183 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:23:47,183 [trainer.py] => memory_size: 0
2023-09-10 11:23:47,183 [trainer.py] => memory_per_class: 0
2023-09-10 11:23:47,183 [trainer.py] => fixed_memory: False
2023-09-10 11:23:47,183 [trainer.py] => shuffle: True
2023-09-10 11:23:47,183 [trainer.py] => init_cls: 30
2023-09-10 11:23:47,184 [trainer.py] => increment: 30
2023-09-10 11:23:47,184 [trainer.py] => model_name: adam_adapter
2023-09-10 11:23:47,184 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:23:47,184 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:23:47,184 [trainer.py] => seed: 1993
2023-09-10 11:23:47,184 [trainer.py] => tuned_epoch: 20
2023-09-10 11:23:47,184 [trainer.py] => init_lr: 0.02
2023-09-10 11:23:47,184 [trainer.py] => batch_size: 96
2023-09-10 11:23:47,184 [trainer.py] => use_A: False
2023-09-10 11:23:47,184 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:23:47,184 [trainer.py] => min_lr: 0
2023-09-10 11:23:47,185 [trainer.py] => ffn_num: 64
2023-09-10 11:23:47,185 [trainer.py] => optimizer: sgd
2023-09-10 11:23:47,185 [trainer.py] => vpt_type: shallow
2023-09-10 11:23:47,185 [trainer.py] => prompt_token_num: 5
2023-09-10 11:23:47,896 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:23:50,444 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:23:50,967 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:23:51,066 [trainer.py] => All params: 87431424
2023-09-10 11:23:51,079 [trainer.py] => Trainable params: 1632768
2023-09-10 11:23:51,189 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:27:35,099 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:27:35,099 [trainer.py] => prefix:  
2023-09-10 11:27:35,099 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:27:35,099 [trainer.py] => memory_size: 0
2023-09-10 11:27:35,099 [trainer.py] => memory_per_class: 0
2023-09-10 11:27:35,099 [trainer.py] => fixed_memory: False
2023-09-10 11:27:35,099 [trainer.py] => shuffle: True
2023-09-10 11:27:35,099 [trainer.py] => init_cls: 30
2023-09-10 11:27:35,099 [trainer.py] => increment: 30
2023-09-10 11:27:35,099 [trainer.py] => model_name: adam_adapter
2023-09-10 11:27:35,099 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:27:35,099 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:27:35,099 [trainer.py] => seed: 1993
2023-09-10 11:27:35,099 [trainer.py] => tuned_epoch: 20
2023-09-10 11:27:35,099 [trainer.py] => init_lr: 0.02
2023-09-10 11:27:35,099 [trainer.py] => batch_size: 96
2023-09-10 11:27:35,099 [trainer.py] => use_A: False
2023-09-10 11:27:35,099 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:27:35,099 [trainer.py] => min_lr: 0
2023-09-10 11:27:35,099 [trainer.py] => ffn_num: 64
2023-09-10 11:27:35,099 [trainer.py] => optimizer: sgd
2023-09-10 11:27:35,099 [trainer.py] => vpt_type: shallow
2023-09-10 11:27:35,099 [trainer.py] => prompt_token_num: 5
2023-09-10 11:27:35,301 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:27:37,266 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:27:37,777 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:27:37,849 [trainer.py] => All params: 87431424
2023-09-10 11:27:37,849 [trainer.py] => Trainable params: 1632768
2023-09-10 11:27:37,961 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:28:37,026 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.398, Train_accy 3.81, Test_accy 5.17
2023-09-10 11:29:31,039 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:29:31,039 [trainer.py] => prefix:  
2023-09-10 11:29:31,039 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:29:31,039 [trainer.py] => memory_size: 0
2023-09-10 11:29:31,039 [trainer.py] => memory_per_class: 0
2023-09-10 11:29:31,039 [trainer.py] => fixed_memory: False
2023-09-10 11:29:31,039 [trainer.py] => shuffle: True
2023-09-10 11:29:31,039 [trainer.py] => init_cls: 30
2023-09-10 11:29:31,040 [trainer.py] => increment: 30
2023-09-10 11:29:31,040 [trainer.py] => model_name: adam_adapter
2023-09-10 11:29:31,040 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:29:31,040 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:29:31,040 [trainer.py] => seed: 1993
2023-09-10 11:29:31,040 [trainer.py] => tuned_epoch: 20
2023-09-10 11:29:31,040 [trainer.py] => init_lr: 0.02
2023-09-10 11:29:31,040 [trainer.py] => batch_size: 96
2023-09-10 11:29:31,040 [trainer.py] => use_A: False
2023-09-10 11:29:31,040 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:29:31,041 [trainer.py] => min_lr: 0
2023-09-10 11:29:31,041 [trainer.py] => ffn_num: 64
2023-09-10 11:29:31,041 [trainer.py] => optimizer: sgd
2023-09-10 11:29:31,041 [trainer.py] => vpt_type: shallow
2023-09-10 11:29:31,041 [trainer.py] => prompt_token_num: 5
2023-09-10 11:29:31,767 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:29:49,871 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:29:50,399 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:29:50,534 [trainer.py] => All params: 86988288
2023-09-10 11:29:50,551 [trainer.py] => Trainable params: 1189632
2023-09-10 11:29:50,674 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:32:10,820 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:32:10,821 [trainer.py] => prefix:  
2023-09-10 11:32:10,821 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:32:10,821 [trainer.py] => memory_size: 0
2023-09-10 11:32:10,821 [trainer.py] => memory_per_class: 0
2023-09-10 11:32:10,821 [trainer.py] => fixed_memory: False
2023-09-10 11:32:10,821 [trainer.py] => shuffle: True
2023-09-10 11:32:10,821 [trainer.py] => init_cls: 30
2023-09-10 11:32:10,821 [trainer.py] => increment: 30
2023-09-10 11:32:10,821 [trainer.py] => model_name: adam_adapter
2023-09-10 11:32:10,821 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:32:10,821 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:32:10,821 [trainer.py] => seed: 1993
2023-09-10 11:32:10,821 [trainer.py] => tuned_epoch: 20
2023-09-10 11:32:10,821 [trainer.py] => init_lr: 0.02
2023-09-10 11:32:10,821 [trainer.py] => batch_size: 96
2023-09-10 11:32:10,821 [trainer.py] => use_A: False
2023-09-10 11:32:10,821 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:32:10,821 [trainer.py] => min_lr: 0
2023-09-10 11:32:10,821 [trainer.py] => ffn_num: 64
2023-09-10 11:32:10,821 [trainer.py] => optimizer: sgd
2023-09-10 11:32:10,821 [trainer.py] => vpt_type: shallow
2023-09-10 11:32:10,821 [trainer.py] => prompt_token_num: 5
2023-09-10 11:32:11,032 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:32:13,150 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:32:13,662 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:32:13,731 [trainer.py] => All params: 86988288
2023-09-10 11:32:13,731 [trainer.py] => Trainable params: 1189632
2023-09-10 11:32:13,844 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:33:13,005 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.399, Train_accy 3.76, Test_accy 3.33
2023-09-10 11:33:45,898 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:33:45,898 [trainer.py] => prefix:  
2023-09-10 11:33:45,898 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:33:45,898 [trainer.py] => memory_size: 0
2023-09-10 11:33:45,898 [trainer.py] => memory_per_class: 0
2023-09-10 11:33:45,898 [trainer.py] => fixed_memory: False
2023-09-10 11:33:45,898 [trainer.py] => shuffle: True
2023-09-10 11:33:45,899 [trainer.py] => init_cls: 30
2023-09-10 11:33:45,899 [trainer.py] => increment: 30
2023-09-10 11:33:45,899 [trainer.py] => model_name: adam_adapter
2023-09-10 11:33:45,899 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:33:45,899 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:33:45,899 [trainer.py] => seed: 1993
2023-09-10 11:33:45,899 [trainer.py] => tuned_epoch: 20
2023-09-10 11:33:45,899 [trainer.py] => init_lr: 0.02
2023-09-10 11:33:45,899 [trainer.py] => batch_size: 96
2023-09-10 11:33:45,899 [trainer.py] => use_A: False
2023-09-10 11:33:45,899 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:33:45,899 [trainer.py] => min_lr: 0
2023-09-10 11:33:45,899 [trainer.py] => ffn_num: 64
2023-09-10 11:33:45,899 [trainer.py] => optimizer: sgd
2023-09-10 11:33:45,899 [trainer.py] => vpt_type: shallow
2023-09-10 11:33:45,899 [trainer.py] => prompt_token_num: 5
2023-09-10 11:33:46,099 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:33:48,083 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:33:48,599 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:33:48,663 [trainer.py] => All params: 86988288
2023-09-10 11:33:48,663 [trainer.py] => Trainable params: 1189632
2023-09-10 11:33:48,769 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:34:47,659 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 11:40:34,598 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:40:34,598 [trainer.py] => prefix:  
2023-09-10 11:40:34,598 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:40:34,598 [trainer.py] => memory_size: 0
2023-09-10 11:40:34,598 [trainer.py] => memory_per_class: 0
2023-09-10 11:40:34,598 [trainer.py] => fixed_memory: False
2023-09-10 11:40:34,598 [trainer.py] => shuffle: True
2023-09-10 11:40:34,598 [trainer.py] => init_cls: 30
2023-09-10 11:40:34,598 [trainer.py] => increment: 30
2023-09-10 11:40:34,598 [trainer.py] => model_name: adam_adapter
2023-09-10 11:40:34,598 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:40:34,599 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:40:34,599 [trainer.py] => seed: 1993
2023-09-10 11:40:34,599 [trainer.py] => tuned_epoch: 20
2023-09-10 11:40:34,599 [trainer.py] => init_lr: 0.02
2023-09-10 11:40:34,599 [trainer.py] => batch_size: 96
2023-09-10 11:40:34,599 [trainer.py] => use_A: False
2023-09-10 11:40:34,599 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:40:34,599 [trainer.py] => min_lr: 0
2023-09-10 11:40:34,599 [trainer.py] => ffn_num: 64
2023-09-10 11:40:34,599 [trainer.py] => optimizer: sgd
2023-09-10 11:40:34,599 [trainer.py] => vpt_type: shallow
2023-09-10 11:40:34,599 [trainer.py] => prompt_token_num: 5
2023-09-10 11:40:34,801 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:40:37,126 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:40:37,628 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:40:37,696 [trainer.py] => All params: 86988288
2023-09-10 11:40:37,696 [trainer.py] => Trainable params: 1189632
2023-09-10 11:40:37,808 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:41:36,849 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.238, Train_accy 24.32, Test_accy 59.00
2023-09-10 11:44:46,795 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 11:44:46,795 [trainer.py] => prefix:  
2023-09-10 11:44:46,795 [trainer.py] => dataset: omnibenchmark
2023-09-10 11:44:46,795 [trainer.py] => memory_size: 0
2023-09-10 11:44:46,795 [trainer.py] => memory_per_class: 0
2023-09-10 11:44:46,796 [trainer.py] => fixed_memory: False
2023-09-10 11:44:46,796 [trainer.py] => shuffle: True
2023-09-10 11:44:46,796 [trainer.py] => init_cls: 30
2023-09-10 11:44:46,796 [trainer.py] => increment: 30
2023-09-10 11:44:46,796 [trainer.py] => model_name: adam_adapter
2023-09-10 11:44:46,796 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 11:44:46,796 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 11:44:46,796 [trainer.py] => seed: 1993
2023-09-10 11:44:46,796 [trainer.py] => tuned_epoch: 20
2023-09-10 11:44:46,796 [trainer.py] => init_lr: 0.02
2023-09-10 11:44:46,796 [trainer.py] => batch_size: 96
2023-09-10 11:44:46,796 [trainer.py] => use_A: False
2023-09-10 11:44:46,796 [trainer.py] => weight_decay: 0.0005
2023-09-10 11:44:46,796 [trainer.py] => min_lr: 0
2023-09-10 11:44:46,796 [trainer.py] => ffn_num: 64
2023-09-10 11:44:46,796 [trainer.py] => optimizer: sgd
2023-09-10 11:44:46,796 [trainer.py] => vpt_type: shallow
2023-09-10 11:44:46,796 [trainer.py] => prompt_token_num: 5
2023-09-10 11:44:46,990 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 11:44:48,932 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 11:44:49,442 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 11:44:49,509 [trainer.py] => All params: 87431424
2023-09-10 11:44:49,510 [trainer.py] => Trainable params: 1632768
2023-09-10 11:44:49,618 [adam_adapter.py] => Learning on 0-30
2023-09-10 11:45:48,869 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 3.226, Train_accy 29.93, Test_accy 55.00
2023-09-10 11:46:48,095 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 2.053, Train_accy 73.00, Test_accy 79.50
2023-09-10 11:47:47,316 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 1.078, Train_accy 80.49, Test_accy 81.17
2023-09-10 11:48:47,360 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 0.782, Train_accy 82.96, Test_accy 87.67
2023-09-10 11:49:46,157 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 0.665, Train_accy 84.32, Test_accy 86.00
2023-09-10 11:50:45,080 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 0.609, Train_accy 84.46, Test_accy 86.50
2023-09-10 11:51:44,377 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 0.568, Train_accy 85.60, Test_accy 88.17
2023-09-10 11:52:43,267 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 0.542, Train_accy 85.75, Test_accy 86.67
2023-09-10 11:53:42,410 [adam_adapter.py] => Task 0, Epoch 9/20 => Loss 0.521, Train_accy 86.33, Test_accy 89.83
2023-09-10 11:54:41,498 [adam_adapter.py] => Task 0, Epoch 10/20 => Loss 0.499, Train_accy 86.99, Test_accy 86.83
2023-09-10 11:55:40,575 [adam_adapter.py] => Task 0, Epoch 11/20 => Loss 0.494, Train_accy 87.15, Test_accy 89.17
2023-09-10 11:56:39,712 [adam_adapter.py] => Task 0, Epoch 12/20 => Loss 0.478, Train_accy 87.69, Test_accy 89.00
2023-09-10 11:57:38,527 [adam_adapter.py] => Task 0, Epoch 13/20 => Loss 0.470, Train_accy 87.86, Test_accy 89.50
2023-09-10 11:58:37,714 [adam_adapter.py] => Task 0, Epoch 14/20 => Loss 0.471, Train_accy 87.83, Test_accy 90.33
2023-09-10 11:59:37,249 [adam_adapter.py] => Task 0, Epoch 15/20 => Loss 0.463, Train_accy 88.19, Test_accy 90.50
2023-09-10 12:00:36,313 [adam_adapter.py] => Task 0, Epoch 16/20 => Loss 0.459, Train_accy 88.21, Test_accy 89.17
2023-09-10 12:01:35,415 [adam_adapter.py] => Task 0, Epoch 17/20 => Loss 0.459, Train_accy 88.31, Test_accy 90.33
2023-09-10 12:02:34,295 [adam_adapter.py] => Task 0, Epoch 18/20 => Loss 0.450, Train_accy 88.62, Test_accy 90.00
2023-09-10 12:03:33,417 [adam_adapter.py] => Task 0, Epoch 19/20 => Loss 0.453, Train_accy 88.45, Test_accy 90.17
2023-09-10 12:04:32,417 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.449, Train_accy 88.41, Test_accy 90.17
2023-09-10 12:04:33,729 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 12:04:34,237 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 12:04:34,992 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 12:04:35,249 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 12:05:56,653 [trainer.py] => No NME accuracy.
2023-09-10 12:05:56,653 [trainer.py] => CNN: {'total': 86.5, '00-09': 90.0, '10-19': 88.5, '20-29': 81.0, 'old': 0, 'new': 86.5}
2023-09-10 12:05:56,653 [trainer.py] => CNN top1 curve: [86.5]
2023-09-10 12:05:56,653 [trainer.py] => CNN top5 curve: [99.0]

2023-09-10 12:05:56,653 [trainer.py] => Average Accuracy (CNN): 86.5
2023-09-10 12:05:56,655 [trainer.py] => All params: 173276161
2023-09-10 12:05:56,657 [trainer.py] => Trainable params: 87477505
2023-09-10 12:05:56,658 [adam_adapter.py] => Learning on 30-60
2023-09-10 12:07:24,652 [trainer.py] => No NME accuracy.
2023-09-10 12:07:24,652 [trainer.py] => CNN: {'total': 86.91, '00-09': 85.0, '10-19': 87.0, '20-29': 77.5, '30-39': 89.5, '40-49': 88.5, '50-59': 93.97, 'old': 83.17, 'new': 90.65}
2023-09-10 12:07:24,652 [trainer.py] => CNN top1 curve: [86.5, 86.91]
2023-09-10 12:07:24,652 [trainer.py] => CNN top5 curve: [99.0, 97.83]

2023-09-10 12:07:24,652 [trainer.py] => Average Accuracy (CNN): 86.705
2023-09-10 12:07:24,653 [trainer.py] => All params: 173322241
2023-09-10 12:07:24,654 [trainer.py] => Trainable params: 87523585
2023-09-10 12:07:24,655 [adam_adapter.py] => Learning on 60-90
2023-09-10 12:08:55,828 [trainer.py] => No NME accuracy.
2023-09-10 12:08:55,828 [trainer.py] => CNN: {'total': 84.54, '00-09': 81.0, '10-19': 82.0, '20-29': 74.0, '30-39': 87.5, '40-49': 86.0, '50-59': 92.46, '60-69': 91.0, '70-79': 87.0, '80-89': 79.9, 'old': 83.82, 'new': 85.98}
2023-09-10 12:08:55,828 [trainer.py] => CNN top1 curve: [86.5, 86.91, 84.54]
2023-09-10 12:08:55,828 [trainer.py] => CNN top5 curve: [99.0, 97.83, 97.11]

2023-09-10 12:08:55,828 [trainer.py] => Average Accuracy (CNN): 85.98333333333333
2023-09-10 12:08:55,829 [trainer.py] => All params: 173368321
2023-09-10 12:08:55,830 [trainer.py] => Trainable params: 87569665
2023-09-10 12:08:55,832 [adam_adapter.py] => Learning on 90-120
2023-09-10 12:10:33,939 [trainer.py] => No NME accuracy.
2023-09-10 12:10:33,939 [trainer.py] => CNN: {'total': 81.17, '00-09': 79.5, '10-19': 79.5, '20-29': 73.0, '30-39': 86.5, '40-49': 85.0, '50-59': 84.92, '60-69': 87.0, '70-79': 85.0, '80-89': 77.39, '90-99': 79.4, '100-109': 79.9, '110-119': 76.88, 'old': 81.98, 'new': 78.73}
2023-09-10 12:10:33,939 [trainer.py] => CNN top1 curve: [86.5, 86.91, 84.54, 81.17]
2023-09-10 12:10:33,939 [trainer.py] => CNN top5 curve: [99.0, 97.83, 97.11, 95.91]

2023-09-10 12:10:33,939 [trainer.py] => Average Accuracy (CNN): 84.78
2023-09-10 12:10:33,941 [trainer.py] => All params: 173414401
2023-09-10 12:10:33,941 [trainer.py] => Trainable params: 87615745
2023-09-10 12:10:33,943 [adam_adapter.py] => Learning on 120-150
2023-09-10 12:12:17,815 [trainer.py] => No NME accuracy.
2023-09-10 12:12:17,815 [trainer.py] => CNN: {'total': 79.06, '00-09': 77.0, '10-19': 78.0, '20-29': 72.0, '30-39': 86.0, '40-49': 83.0, '50-59': 78.39, '60-69': 81.0, '70-79': 83.5, '80-89': 75.88, '90-99': 78.89, '100-109': 75.88, '110-119': 71.86, '120-129': 85.0, '130-139': 80.5, '140-149': 78.89, 'old': 78.46, 'new': 81.47}
2023-09-10 12:12:17,815 [trainer.py] => CNN top1 curve: [86.5, 86.91, 84.54, 81.17, 79.06]
2023-09-10 12:12:17,815 [trainer.py] => CNN top5 curve: [99.0, 97.83, 97.11, 95.91, 94.59]

2023-09-10 12:12:17,815 [trainer.py] => Average Accuracy (CNN): 83.636
2023-09-10 12:12:17,817 [trainer.py] => All params: 173460481
2023-09-10 12:12:17,818 [trainer.py] => Trainable params: 87661825
2023-09-10 12:12:17,822 [adam_adapter.py] => Learning on 150-180
2023-09-10 12:14:02,890 [trainer.py] => No NME accuracy.
2023-09-10 12:14:02,890 [trainer.py] => CNN: {'total': 76.48, '00-09': 72.5, '10-19': 74.5, '20-29': 70.0, '30-39': 84.0, '40-49': 80.5, '50-59': 77.39, '60-69': 80.5, '70-79': 81.0, '80-89': 73.37, '90-99': 78.89, '100-109': 73.37, '110-119': 70.35, '120-129': 84.0, '130-139': 80.0, '140-149': 76.88, '150-159': 79.4, '160-169': 73.87, '170-179': 66.0, 'old': 77.15, 'new': 73.08}
2023-09-10 12:14:02,890 [trainer.py] => CNN top1 curve: [86.5, 86.91, 84.54, 81.17, 79.06, 76.48]
2023-09-10 12:14:02,890 [trainer.py] => CNN top5 curve: [99.0, 97.83, 97.11, 95.91, 94.59, 93.46]

2023-09-10 12:14:02,890 [trainer.py] => Average Accuracy (CNN): 82.44333333333334
2023-09-10 12:14:02,891 [trainer.py] => All params: 173506561
2023-09-10 12:14:02,892 [trainer.py] => Trainable params: 87707905
2023-09-10 12:14:02,894 [adam_adapter.py] => Learning on 180-210
2023-09-10 12:15:50,531 [trainer.py] => No NME accuracy.
2023-09-10 12:15:50,531 [trainer.py] => CNN: {'total': 75.39, '00-09': 72.5, '10-19': 74.0, '20-29': 69.5, '30-39': 84.0, '40-49': 78.5, '50-59': 76.88, '60-69': 77.5, '70-79': 81.0, '80-89': 72.86, '90-99': 76.38, '100-109': 73.37, '110-119': 67.34, '120-129': 84.0, '130-139': 79.0, '140-149': 76.38, '150-159': 76.38, '160-169': 72.36, '170-179': 66.0, '180-189': 71.72, '190-199': 83.5, '200-209': 69.85, 'old': 75.45, 'new': 75.04}
2023-09-10 12:15:50,531 [trainer.py] => CNN top1 curve: [86.5, 86.91, 84.54, 81.17, 79.06, 76.48, 75.39]
2023-09-10 12:15:50,531 [trainer.py] => CNN top5 curve: [99.0, 97.83, 97.11, 95.91, 94.59, 93.46, 92.62]

2023-09-10 12:15:50,531 [trainer.py] => Average Accuracy (CNN): 81.4357142857143
2023-09-10 12:15:50,532 [trainer.py] => All params: 173552641
2023-09-10 12:15:50,533 [trainer.py] => Trainable params: 87753985
2023-09-10 12:15:50,535 [adam_adapter.py] => Learning on 210-240
2023-09-10 12:17:48,431 [trainer.py] => No NME accuracy.
2023-09-10 12:17:48,431 [trainer.py] => CNN: {'total': 74.02, '00-09': 72.0, '10-19': 72.0, '20-29': 68.5, '30-39': 84.0, '40-49': 77.0, '50-59': 76.88, '60-69': 76.5, '70-79': 81.0, '80-89': 72.36, '90-99': 76.38, '100-109': 65.83, '110-119': 66.33, '120-129': 83.0, '130-139': 77.5, '140-149': 75.88, '150-159': 76.38, '160-169': 72.36, '170-179': 63.5, '180-189': 70.71, '190-199': 83.5, '200-209': 66.83, '210-219': 51.5, '220-229': 85.43, '230-239': 81.0, 'old': 74.22, 'new': 72.62}
2023-09-10 12:17:48,431 [trainer.py] => CNN top1 curve: [86.5, 86.91, 84.54, 81.17, 79.06, 76.48, 75.39, 74.02]
2023-09-10 12:17:48,432 [trainer.py] => CNN top5 curve: [99.0, 97.83, 97.11, 95.91, 94.59, 93.46, 92.62, 91.33]

2023-09-10 12:17:48,432 [trainer.py] => Average Accuracy (CNN): 80.50875
2023-09-10 12:17:48,433 [trainer.py] => All params: 173598721
2023-09-10 12:17:48,434 [trainer.py] => Trainable params: 87800065
2023-09-10 12:17:48,436 [adam_adapter.py] => Learning on 240-270
2023-09-10 12:19:49,733 [trainer.py] => No NME accuracy.
2023-09-10 12:19:49,734 [trainer.py] => CNN: {'total': 73.21, '00-09': 72.0, '10-19': 72.0, '20-29': 68.5, '30-39': 83.5, '40-49': 77.0, '50-59': 75.88, '60-69': 74.5, '70-79': 80.0, '80-89': 72.36, '90-99': 74.37, '100-109': 65.33, '110-119': 63.82, '120-129': 78.5, '130-139': 76.5, '140-149': 72.36, '150-159': 73.37, '160-169': 72.36, '170-179': 63.5, '180-189': 70.71, '190-199': 82.0, '200-209': 64.32, '210-219': 51.5, '220-229': 85.43, '230-239': 81.0, '240-249': 68.5, '250-259': 76.88, '260-269': 80.5, 'old': 72.95, 'new': 75.29}
2023-09-10 12:19:49,734 [trainer.py] => CNN top1 curve: [86.5, 86.91, 84.54, 81.17, 79.06, 76.48, 75.39, 74.02, 73.21]
2023-09-10 12:19:49,734 [trainer.py] => CNN top5 curve: [99.0, 97.83, 97.11, 95.91, 94.59, 93.46, 92.62, 91.33, 91.48]

2023-09-10 12:19:49,734 [trainer.py] => Average Accuracy (CNN): 79.69777777777779
2023-09-10 12:19:49,735 [trainer.py] => All params: 173644801
2023-09-10 12:19:49,736 [trainer.py] => Trainable params: 87846145
2023-09-10 12:19:49,739 [adam_adapter.py] => Learning on 270-300
2023-09-10 12:21:49,126 [trainer.py] => No NME accuracy.
2023-09-10 12:21:49,126 [trainer.py] => CNN: {'total': 73.3, '00-09': 71.5, '10-19': 72.0, '20-29': 67.5, '30-39': 83.5, '40-49': 74.0, '50-59': 75.88, '60-69': 74.0, '70-79': 79.0, '80-89': 72.36, '90-99': 71.36, '100-109': 64.82, '110-119': 62.81, '120-129': 78.5, '130-139': 76.5, '140-149': 72.36, '150-159': 73.37, '160-169': 70.85, '170-179': 63.5, '180-189': 70.2, '190-199': 81.5, '200-209': 63.82, '210-219': 50.5, '220-229': 85.43, '230-239': 81.0, '240-249': 66.5, '250-259': 76.38, '260-269': 79.5, '270-279': 79.4, '280-289': 80.0, '290-299': 80.9, 'old': 72.55, 'new': 80.1}
2023-09-10 12:21:49,126 [trainer.py] => CNN top1 curve: [86.5, 86.91, 84.54, 81.17, 79.06, 76.48, 75.39, 74.02, 73.21, 73.3]
2023-09-10 12:21:49,126 [trainer.py] => CNN top5 curve: [99.0, 97.83, 97.11, 95.91, 94.59, 93.46, 92.62, 91.33, 91.48, 91.45]

2023-09-10 12:21:49,126 [trainer.py] => Average Accuracy (CNN): 79.058
2023-09-10 13:32:56,638 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 13:32:56,639 [trainer.py] => prefix:  
2023-09-10 13:32:56,639 [trainer.py] => dataset: omnibenchmark
2023-09-10 13:32:56,639 [trainer.py] => memory_size: 0
2023-09-10 13:32:56,639 [trainer.py] => memory_per_class: 0
2023-09-10 13:32:56,639 [trainer.py] => fixed_memory: False
2023-09-10 13:32:56,639 [trainer.py] => shuffle: True
2023-09-10 13:32:56,639 [trainer.py] => init_cls: 30
2023-09-10 13:32:56,639 [trainer.py] => increment: 30
2023-09-10 13:32:56,639 [trainer.py] => model_name: adam_adapter
2023-09-10 13:32:56,639 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 13:32:56,640 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 13:32:56,640 [trainer.py] => seed: 1993
2023-09-10 13:32:56,640 [trainer.py] => tuned_epoch: 1
2023-09-10 13:32:56,640 [trainer.py] => init_lr: 0.02
2023-09-10 13:32:56,640 [trainer.py] => batch_size: 96
2023-09-10 13:32:56,640 [trainer.py] => use_A: False
2023-09-10 13:32:56,640 [trainer.py] => weight_decay: 0.0005
2023-09-10 13:32:56,640 [trainer.py] => min_lr: 0
2023-09-10 13:32:56,640 [trainer.py] => ffn_num: 64
2023-09-10 13:32:56,640 [trainer.py] => optimizer: sgd
2023-09-10 13:32:56,640 [trainer.py] => vpt_type: shallow
2023-09-10 13:32:56,640 [trainer.py] => prompt_token_num: 5
2023-09-10 13:32:57,342 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 13:32:59,967 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:33:00,470 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:33:00,570 [trainer.py] => All params: 86988288
2023-09-10 13:33:00,581 [trainer.py] => Trainable params: 1189632
2023-09-10 13:33:00,681 [adam_adapter.py] => Learning on 0-30
2023-09-10 13:34:04,851 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 13:34:06,526 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:34:06,799 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:34:07,629 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:34:07,899 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:36:44,276 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 13:36:44,276 [trainer.py] => prefix:  
2023-09-10 13:36:44,276 [trainer.py] => dataset: omnibenchmark
2023-09-10 13:36:44,276 [trainer.py] => memory_size: 0
2023-09-10 13:36:44,276 [trainer.py] => memory_per_class: 0
2023-09-10 13:36:44,277 [trainer.py] => fixed_memory: False
2023-09-10 13:36:44,277 [trainer.py] => shuffle: True
2023-09-10 13:36:44,277 [trainer.py] => init_cls: 30
2023-09-10 13:36:44,277 [trainer.py] => increment: 30
2023-09-10 13:36:44,277 [trainer.py] => model_name: adam_adapter
2023-09-10 13:36:44,277 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 13:36:44,277 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 13:36:44,277 [trainer.py] => seed: 1993
2023-09-10 13:36:44,277 [trainer.py] => tuned_epoch: 1
2023-09-10 13:36:44,277 [trainer.py] => init_lr: 0.02
2023-09-10 13:36:44,278 [trainer.py] => batch_size: 96
2023-09-10 13:36:44,278 [trainer.py] => use_A: False
2023-09-10 13:36:44,278 [trainer.py] => weight_decay: 0.0005
2023-09-10 13:36:44,278 [trainer.py] => min_lr: 0
2023-09-10 13:36:44,278 [trainer.py] => ffn_num: 64
2023-09-10 13:36:44,278 [trainer.py] => optimizer: sgd
2023-09-10 13:36:44,278 [trainer.py] => vpt_type: shallow
2023-09-10 13:36:44,278 [trainer.py] => prompt_token_num: 5
2023-09-10 13:36:44,996 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 13:36:47,688 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:36:48,205 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:36:48,302 [trainer.py] => All params: 86988288
2023-09-10 13:36:48,314 [trainer.py] => Trainable params: 1189632
2023-09-10 13:36:48,432 [adam_adapter.py] => Learning on 0-30
2023-09-10 13:37:52,559 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 13:37:53,952 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:37:54,227 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:37:54,975 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:37:55,244 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:41:15,113 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 13:41:15,113 [trainer.py] => prefix:  
2023-09-10 13:41:15,113 [trainer.py] => dataset: omnibenchmark
2023-09-10 13:41:15,113 [trainer.py] => memory_size: 0
2023-09-10 13:41:15,113 [trainer.py] => memory_per_class: 0
2023-09-10 13:41:15,113 [trainer.py] => fixed_memory: False
2023-09-10 13:41:15,113 [trainer.py] => shuffle: True
2023-09-10 13:41:15,113 [trainer.py] => init_cls: 30
2023-09-10 13:41:15,113 [trainer.py] => increment: 30
2023-09-10 13:41:15,113 [trainer.py] => model_name: adam_adapter
2023-09-10 13:41:15,113 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 13:41:15,113 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 13:41:15,113 [trainer.py] => seed: 1993
2023-09-10 13:41:15,113 [trainer.py] => tuned_epoch: 1
2023-09-10 13:41:15,113 [trainer.py] => init_lr: 0.02
2023-09-10 13:41:15,113 [trainer.py] => batch_size: 96
2023-09-10 13:41:15,113 [trainer.py] => use_A: False
2023-09-10 13:41:15,113 [trainer.py] => weight_decay: 0.0005
2023-09-10 13:41:15,113 [trainer.py] => min_lr: 0
2023-09-10 13:41:15,113 [trainer.py] => ffn_num: 64
2023-09-10 13:41:15,113 [trainer.py] => optimizer: sgd
2023-09-10 13:41:15,113 [trainer.py] => vpt_type: shallow
2023-09-10 13:41:15,113 [trainer.py] => prompt_token_num: 5
2023-09-10 13:41:15,311 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 13:41:17,326 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:41:17,824 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:41:17,892 [trainer.py] => All params: 86988288
2023-09-10 13:41:17,892 [trainer.py] => Trainable params: 1189632
2023-09-10 13:41:18,006 [adam_adapter.py] => Learning on 0-30
2023-09-10 13:42:17,416 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 13:42:18,948 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:42:19,197 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:42:19,979 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:42:20,242 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:43:58,152 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 13:43:58,152 [trainer.py] => prefix:  
2023-09-10 13:43:58,152 [trainer.py] => dataset: omnibenchmark
2023-09-10 13:43:58,152 [trainer.py] => memory_size: 0
2023-09-10 13:43:58,152 [trainer.py] => memory_per_class: 0
2023-09-10 13:43:58,152 [trainer.py] => fixed_memory: False
2023-09-10 13:43:58,152 [trainer.py] => shuffle: True
2023-09-10 13:43:58,152 [trainer.py] => init_cls: 30
2023-09-10 13:43:58,152 [trainer.py] => increment: 30
2023-09-10 13:43:58,152 [trainer.py] => model_name: adam_adapter
2023-09-10 13:43:58,152 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 13:43:58,152 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 13:43:58,152 [trainer.py] => seed: 1993
2023-09-10 13:43:58,152 [trainer.py] => tuned_epoch: 1
2023-09-10 13:43:58,152 [trainer.py] => init_lr: 0.02
2023-09-10 13:43:58,152 [trainer.py] => batch_size: 96
2023-09-10 13:43:58,152 [trainer.py] => use_A: False
2023-09-10 13:43:58,152 [trainer.py] => weight_decay: 0.0005
2023-09-10 13:43:58,152 [trainer.py] => min_lr: 0
2023-09-10 13:43:58,152 [trainer.py] => ffn_num: 64
2023-09-10 13:43:58,152 [trainer.py] => optimizer: sgd
2023-09-10 13:43:58,152 [trainer.py] => vpt_type: shallow
2023-09-10 13:43:58,152 [trainer.py] => prompt_token_num: 5
2023-09-10 13:43:58,351 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 13:44:00,294 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:44:00,795 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:44:00,857 [trainer.py] => All params: 86988288
2023-09-10 13:44:00,858 [trainer.py] => Trainable params: 1189632
2023-09-10 13:44:00,964 [adam_adapter.py] => Learning on 0-30
2023-09-10 13:44:59,772 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 13:45:01,037 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:45:01,297 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:45:02,114 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:45:02,368 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:48:28,574 [adam_adapter.py] => Task 0, Epoch 6/1 => Loss 2.931, Train_accy 84.80, Test_accy 86.67
2023-09-10 13:51:12,747 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 13:51:12,748 [trainer.py] => prefix:  
2023-09-10 13:51:12,748 [trainer.py] => dataset: omnibenchmark
2023-09-10 13:51:12,748 [trainer.py] => memory_size: 0
2023-09-10 13:51:12,748 [trainer.py] => memory_per_class: 0
2023-09-10 13:51:12,748 [trainer.py] => fixed_memory: False
2023-09-10 13:51:12,748 [trainer.py] => shuffle: True
2023-09-10 13:51:12,748 [trainer.py] => init_cls: 30
2023-09-10 13:51:12,748 [trainer.py] => increment: 30
2023-09-10 13:51:12,748 [trainer.py] => model_name: adam_adapter
2023-09-10 13:51:12,748 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 13:51:12,748 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 13:51:12,748 [trainer.py] => seed: 1993
2023-09-10 13:51:12,748 [trainer.py] => tuned_epoch: 1
2023-09-10 13:51:12,748 [trainer.py] => init_lr: 0.02
2023-09-10 13:51:12,748 [trainer.py] => batch_size: 96
2023-09-10 13:51:12,748 [trainer.py] => use_A: False
2023-09-10 13:51:12,748 [trainer.py] => weight_decay: 0.0005
2023-09-10 13:51:12,748 [trainer.py] => min_lr: 0
2023-09-10 13:51:12,748 [trainer.py] => ffn_num: 64
2023-09-10 13:51:12,748 [trainer.py] => optimizer: sgd
2023-09-10 13:51:12,748 [trainer.py] => vpt_type: shallow
2023-09-10 13:51:12,748 [trainer.py] => prompt_token_num: 5
2023-09-10 13:51:12,954 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 13:51:14,889 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:51:15,397 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:51:15,467 [trainer.py] => All params: 86988288
2023-09-10 13:51:15,467 [trainer.py] => Trainable params: 1189632
2023-09-10 13:51:15,594 [adam_adapter.py] => Learning on 0-30
2023-09-10 13:52:14,502 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 13:52:15,649 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:52:15,910 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:52:16,735 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:52:16,989 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:54:13,229 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 13:54:13,229 [trainer.py] => prefix:  
2023-09-10 13:54:13,229 [trainer.py] => dataset: omnibenchmark
2023-09-10 13:54:13,229 [trainer.py] => memory_size: 0
2023-09-10 13:54:13,229 [trainer.py] => memory_per_class: 0
2023-09-10 13:54:13,229 [trainer.py] => fixed_memory: False
2023-09-10 13:54:13,229 [trainer.py] => shuffle: True
2023-09-10 13:54:13,229 [trainer.py] => init_cls: 30
2023-09-10 13:54:13,230 [trainer.py] => increment: 30
2023-09-10 13:54:13,230 [trainer.py] => model_name: adam_adapter
2023-09-10 13:54:13,230 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 13:54:13,230 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 13:54:13,230 [trainer.py] => seed: 1993
2023-09-10 13:54:13,230 [trainer.py] => tuned_epoch: 20
2023-09-10 13:54:13,230 [trainer.py] => init_lr: 0.02
2023-09-10 13:54:13,230 [trainer.py] => batch_size: 96
2023-09-10 13:54:13,230 [trainer.py] => use_A: False
2023-09-10 13:54:13,230 [trainer.py] => weight_decay: 0.0005
2023-09-10 13:54:13,230 [trainer.py] => min_lr: 0
2023-09-10 13:54:13,230 [trainer.py] => ffn_num: 64
2023-09-10 13:54:13,230 [trainer.py] => optimizer: sgd
2023-09-10 13:54:13,230 [trainer.py] => vpt_type: shallow
2023-09-10 13:54:13,230 [trainer.py] => prompt_token_num: 5
2023-09-10 13:54:13,431 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 13:54:15,504 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 13:54:16,016 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 13:54:16,087 [trainer.py] => All params: 86988288
2023-09-10 13:54:16,088 [trainer.py] => Trainable params: 1189632
2023-09-10 13:54:16,205 [adam_adapter.py] => Learning on 0-30
2023-09-10 13:55:15,455 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 13:56:14,497 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 0.889, Train_accy 82.64, Test_accy 88.67
2023-09-10 13:57:13,876 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 0.554, Train_accy 85.15, Test_accy 89.83
2023-09-10 13:58:12,536 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 0.489, Train_accy 86.31, Test_accy 87.83
2023-09-10 13:59:11,186 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 0.457, Train_accy 86.68, Test_accy 90.33
2023-09-10 14:00:09,709 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 0.433, Train_accy 87.47, Test_accy 91.00
2023-09-10 14:01:08,413 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 0.410, Train_accy 88.13, Test_accy 91.33
2023-09-10 14:02:07,575 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 0.393, Train_accy 88.22, Test_accy 90.83
2023-09-10 14:03:06,601 [adam_adapter.py] => Task 0, Epoch 9/20 => Loss 0.393, Train_accy 88.73, Test_accy 92.17
2023-09-10 14:04:05,502 [adam_adapter.py] => Task 0, Epoch 10/20 => Loss 0.372, Train_accy 89.00, Test_accy 91.83
2023-09-10 14:05:04,258 [adam_adapter.py] => Task 0, Epoch 11/20 => Loss 0.362, Train_accy 89.23, Test_accy 91.83
2023-09-10 14:06:03,326 [adam_adapter.py] => Task 0, Epoch 12/20 => Loss 0.372, Train_accy 89.00, Test_accy 92.50
2023-09-10 14:07:02,439 [adam_adapter.py] => Task 0, Epoch 13/20 => Loss 0.355, Train_accy 89.64, Test_accy 92.50
2023-09-10 14:08:01,340 [adam_adapter.py] => Task 0, Epoch 14/20 => Loss 0.346, Train_accy 89.99, Test_accy 92.00
2023-09-10 14:09:01,234 [adam_adapter.py] => Task 0, Epoch 15/20 => Loss 0.351, Train_accy 89.86, Test_accy 91.83
2023-09-10 14:10:00,454 [adam_adapter.py] => Task 0, Epoch 16/20 => Loss 0.351, Train_accy 89.71, Test_accy 92.17
2023-09-10 14:11:00,085 [adam_adapter.py] => Task 0, Epoch 17/20 => Loss 0.341, Train_accy 90.28, Test_accy 92.50
2023-09-10 14:11:59,408 [adam_adapter.py] => Task 0, Epoch 18/20 => Loss 0.341, Train_accy 90.06, Test_accy 92.50
2023-09-10 14:12:58,701 [adam_adapter.py] => Task 0, Epoch 19/20 => Loss 0.338, Train_accy 90.49, Test_accy 92.50
2023-09-10 14:13:57,714 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.345, Train_accy 90.13, Test_accy 92.50
2023-09-10 14:13:58,892 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:13:59,395 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:14:00,310 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:14:00,570 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:16:41,842 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 2.861, Train_accy 85.19, Test_accy 89.33
2023-09-10 14:18:12,680 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 5.709, Train_accy 85.24, Test_accy 89.33
2023-09-10 14:19:43,289 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 8.554, Train_accy 85.30, Test_accy 89.33
2023-09-10 14:21:13,361 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 11.401, Train_accy 85.22, Test_accy 89.33
2023-09-10 14:26:10,337 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 14:26:10,337 [trainer.py] => prefix:  
2023-09-10 14:26:10,337 [trainer.py] => dataset: omnibenchmark
2023-09-10 14:26:10,337 [trainer.py] => memory_size: 0
2023-09-10 14:26:10,337 [trainer.py] => memory_per_class: 0
2023-09-10 14:26:10,337 [trainer.py] => fixed_memory: False
2023-09-10 14:26:10,337 [trainer.py] => shuffle: True
2023-09-10 14:26:10,337 [trainer.py] => init_cls: 30
2023-09-10 14:26:10,337 [trainer.py] => increment: 30
2023-09-10 14:26:10,337 [trainer.py] => model_name: adam_adapter
2023-09-10 14:26:10,338 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 14:26:10,338 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 14:26:10,338 [trainer.py] => seed: 1993
2023-09-10 14:26:10,338 [trainer.py] => tuned_epoch: 20
2023-09-10 14:26:10,338 [trainer.py] => init_lr: 0.02
2023-09-10 14:26:10,338 [trainer.py] => batch_size: 96
2023-09-10 14:26:10,338 [trainer.py] => use_A: False
2023-09-10 14:26:10,338 [trainer.py] => weight_decay: 0.0005
2023-09-10 14:26:10,338 [trainer.py] => min_lr: 0
2023-09-10 14:26:10,338 [trainer.py] => ffn_num: 64
2023-09-10 14:26:10,338 [trainer.py] => optimizer: sgd
2023-09-10 14:26:10,338 [trainer.py] => vpt_type: shallow
2023-09-10 14:26:10,338 [trainer.py] => prompt_token_num: 5
2023-09-10 14:26:10,537 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 14:26:12,547 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:26:13,064 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:26:13,130 [trainer.py] => All params: 86988288
2023-09-10 14:26:13,131 [trainer.py] => Trainable params: 1189632
2023-09-10 14:26:13,252 [adam_adapter.py] => Learning on 0-30
2023-09-10 14:26:55,419 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 14:26:55,419 [trainer.py] => prefix:  
2023-09-10 14:26:55,419 [trainer.py] => dataset: omnibenchmark
2023-09-10 14:26:55,419 [trainer.py] => memory_size: 0
2023-09-10 14:26:55,419 [trainer.py] => memory_per_class: 0
2023-09-10 14:26:55,419 [trainer.py] => fixed_memory: False
2023-09-10 14:26:55,419 [trainer.py] => shuffle: True
2023-09-10 14:26:55,419 [trainer.py] => init_cls: 30
2023-09-10 14:26:55,419 [trainer.py] => increment: 30
2023-09-10 14:26:55,419 [trainer.py] => model_name: adam_adapter
2023-09-10 14:26:55,419 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 14:26:55,419 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 14:26:55,419 [trainer.py] => seed: 1993
2023-09-10 14:26:55,419 [trainer.py] => tuned_epoch: 1
2023-09-10 14:26:55,419 [trainer.py] => init_lr: 0.02
2023-09-10 14:26:55,419 [trainer.py] => batch_size: 96
2023-09-10 14:26:55,419 [trainer.py] => use_A: False
2023-09-10 14:26:55,419 [trainer.py] => weight_decay: 0.0005
2023-09-10 14:26:55,419 [trainer.py] => min_lr: 0
2023-09-10 14:26:55,419 [trainer.py] => ffn_num: 64
2023-09-10 14:26:55,419 [trainer.py] => optimizer: sgd
2023-09-10 14:26:55,419 [trainer.py] => vpt_type: shallow
2023-09-10 14:26:55,419 [trainer.py] => prompt_token_num: 5
2023-09-10 14:26:55,618 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 14:26:57,564 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:26:58,670 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:26:58,734 [trainer.py] => All params: 86988288
2023-09-10 14:26:58,734 [trainer.py] => Trainable params: 1189632
2023-09-10 14:26:58,838 [adam_adapter.py] => Learning on 0-30
2023-09-10 14:27:58,107 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 14:27:59,249 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:27:59,508 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:28:00,245 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:28:00,495 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:30:42,089 [adam_adapter.py] => Task 0, Epoch 1/5 => Loss 2.952, Train_accy 84.70, Test_accy 87.17
2023-09-10 14:32:12,003 [adam_adapter.py] => Task 0, Epoch 2/5 => Loss 2.942, Train_accy 84.54, Test_accy 87.17
2023-09-10 14:33:42,519 [adam_adapter.py] => Task 0, Epoch 3/5 => Loss 2.940, Train_accy 84.50, Test_accy 87.17
2023-09-10 14:35:12,397 [adam_adapter.py] => Task 0, Epoch 4/5 => Loss 2.942, Train_accy 84.73, Test_accy 87.17
2023-09-10 14:36:42,610 [adam_adapter.py] => Task 0, Epoch 5/5 => Loss 2.941, Train_accy 84.31, Test_accy 87.17
2023-09-10 14:36:51,586 [trainer.py] => No NME accuracy.
2023-09-10 14:36:51,586 [trainer.py] => CNN: {'total': 87.17, '00-09': 90.5, '10-19': 88.0, '20-29': 83.0, 'old': 0, 'new': 87.17}
2023-09-10 14:36:51,586 [trainer.py] => CNN top1 curve: [87.17]
2023-09-10 14:36:51,587 [trainer.py] => CNN top5 curve: [98.83]

2023-09-10 14:36:51,587 [trainer.py] => Average Accuracy (CNN): 87.17
2023-09-10 14:36:51,588 [trainer.py] => All params: 172833025
2023-09-10 14:36:51,590 [trainer.py] => Trainable params: 1235713
2023-09-10 14:36:51,592 [adam_adapter.py] => Learning on 30-60
2023-09-10 14:43:41,324 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 14:43:41,324 [trainer.py] => prefix:  
2023-09-10 14:43:41,324 [trainer.py] => dataset: omnibenchmark
2023-09-10 14:43:41,324 [trainer.py] => memory_size: 0
2023-09-10 14:43:41,324 [trainer.py] => memory_per_class: 0
2023-09-10 14:43:41,324 [trainer.py] => fixed_memory: False
2023-09-10 14:43:41,324 [trainer.py] => shuffle: True
2023-09-10 14:43:41,324 [trainer.py] => init_cls: 30
2023-09-10 14:43:41,324 [trainer.py] => increment: 30
2023-09-10 14:43:41,324 [trainer.py] => model_name: adam_adapter
2023-09-10 14:43:41,324 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 14:43:41,324 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 14:43:41,324 [trainer.py] => seed: 1993
2023-09-10 14:43:41,324 [trainer.py] => tuned_epoch: 1
2023-09-10 14:43:41,324 [trainer.py] => init_lr: 0.02
2023-09-10 14:43:41,324 [trainer.py] => batch_size: 96
2023-09-10 14:43:41,324 [trainer.py] => use_A: False
2023-09-10 14:43:41,324 [trainer.py] => weight_decay: 0.0005
2023-09-10 14:43:41,324 [trainer.py] => min_lr: 0
2023-09-10 14:43:41,324 [trainer.py] => ffn_num: 64
2023-09-10 14:43:41,324 [trainer.py] => optimizer: sgd
2023-09-10 14:43:41,324 [trainer.py] => vpt_type: shallow
2023-09-10 14:43:41,324 [trainer.py] => prompt_token_num: 5
2023-09-10 14:43:41,526 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 14:43:43,501 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:43:43,999 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:43:44,063 [trainer.py] => All params: 86988288
2023-09-10 14:43:44,064 [trainer.py] => Trainable params: 1189632
2023-09-10 14:43:44,182 [adam_adapter.py] => Learning on 0-30
2023-09-10 14:44:43,009 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 14:44:44,232 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:44:44,492 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:44:45,234 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:44:45,484 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:47:06,126 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 14:47:06,126 [trainer.py] => prefix:  
2023-09-10 14:47:06,127 [trainer.py] => dataset: omnibenchmark
2023-09-10 14:47:06,127 [trainer.py] => memory_size: 0
2023-09-10 14:47:06,127 [trainer.py] => memory_per_class: 0
2023-09-10 14:47:06,127 [trainer.py] => fixed_memory: False
2023-09-10 14:47:06,127 [trainer.py] => shuffle: True
2023-09-10 14:47:06,127 [trainer.py] => init_cls: 30
2023-09-10 14:47:06,127 [trainer.py] => increment: 30
2023-09-10 14:47:06,127 [trainer.py] => model_name: adam_adapter
2023-09-10 14:47:06,127 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 14:47:06,127 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 14:47:06,127 [trainer.py] => seed: 1993
2023-09-10 14:47:06,127 [trainer.py] => tuned_epoch: 1
2023-09-10 14:47:06,127 [trainer.py] => init_lr: 0.02
2023-09-10 14:47:06,127 [trainer.py] => batch_size: 96
2023-09-10 14:47:06,127 [trainer.py] => use_A: False
2023-09-10 14:47:06,127 [trainer.py] => weight_decay: 0.0005
2023-09-10 14:47:06,127 [trainer.py] => min_lr: 0
2023-09-10 14:47:06,127 [trainer.py] => ffn_num: 64
2023-09-10 14:47:06,127 [trainer.py] => optimizer: sgd
2023-09-10 14:47:06,127 [trainer.py] => vpt_type: shallow
2023-09-10 14:47:06,127 [trainer.py] => prompt_token_num: 5
2023-09-10 14:47:06,322 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 14:47:08,298 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:47:08,809 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:47:08,877 [trainer.py] => All params: 86988288
2023-09-10 14:47:08,877 [trainer.py] => Trainable params: 1189632
2023-09-10 14:47:08,991 [adam_adapter.py] => Learning on 0-30
2023-09-10 14:48:08,231 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 14:48:09,747 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:48:10,008 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:48:10,901 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:48:11,151 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:50:52,800 [adam_adapter.py] => Task 0, Epoch 1/5 => Loss 2.952, Train_accy 84.70, Test_accy 87.17
2023-09-10 14:52:23,416 [adam_adapter.py] => Task 0, Epoch 2/5 => Loss 2.942, Train_accy 84.54, Test_accy 87.17
2023-09-10 14:56:12,135 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 14:56:12,135 [trainer.py] => prefix:  
2023-09-10 14:56:12,135 [trainer.py] => dataset: omnibenchmark
2023-09-10 14:56:12,135 [trainer.py] => memory_size: 0
2023-09-10 14:56:12,135 [trainer.py] => memory_per_class: 0
2023-09-10 14:56:12,135 [trainer.py] => fixed_memory: False
2023-09-10 14:56:12,135 [trainer.py] => shuffle: True
2023-09-10 14:56:12,135 [trainer.py] => init_cls: 30
2023-09-10 14:56:12,135 [trainer.py] => increment: 30
2023-09-10 14:56:12,135 [trainer.py] => model_name: adam_adapter
2023-09-10 14:56:12,135 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 14:56:12,135 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 14:56:12,135 [trainer.py] => seed: 1993
2023-09-10 14:56:12,135 [trainer.py] => tuned_epoch: 1
2023-09-10 14:56:12,135 [trainer.py] => init_lr: 0.02
2023-09-10 14:56:12,135 [trainer.py] => batch_size: 96
2023-09-10 14:56:12,135 [trainer.py] => use_A: False
2023-09-10 14:56:12,135 [trainer.py] => weight_decay: 0.0005
2023-09-10 14:56:12,135 [trainer.py] => min_lr: 0
2023-09-10 14:56:12,135 [trainer.py] => ffn_num: 64
2023-09-10 14:56:12,135 [trainer.py] => optimizer: sgd
2023-09-10 14:56:12,135 [trainer.py] => vpt_type: shallow
2023-09-10 14:56:12,135 [trainer.py] => prompt_token_num: 5
2023-09-10 14:56:12,327 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 14:56:14,234 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:56:14,841 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:56:14,915 [trainer.py] => All params: 86988288
2023-09-10 14:56:14,915 [trainer.py] => Trainable params: 1189632
2023-09-10 14:56:15,077 [adam_adapter.py] => Learning on 0-30
2023-09-10 14:57:13,887 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 14:57:14,987 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:57:15,245 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:57:15,940 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 14:57:16,197 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 14:59:43,627 [adam_adapter.py] => Task 0, Epoch 1/5 => Loss 2.952, Train_accy 84.70, Test_accy 87.17
2023-09-10 15:00:59,874 [adam_adapter.py] => Task 0, Epoch 2/5 => Loss 2.942, Train_accy 84.54, Test_accy 87.17
2023-09-10 15:02:16,118 [adam_adapter.py] => Task 0, Epoch 3/5 => Loss 2.940, Train_accy 84.50, Test_accy 87.17
2023-09-10 15:03:42,549 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 15:03:42,549 [trainer.py] => prefix:  
2023-09-10 15:03:42,549 [trainer.py] => dataset: omnibenchmark
2023-09-10 15:03:42,549 [trainer.py] => memory_size: 0
2023-09-10 15:03:42,549 [trainer.py] => memory_per_class: 0
2023-09-10 15:03:42,549 [trainer.py] => fixed_memory: False
2023-09-10 15:03:42,549 [trainer.py] => shuffle: True
2023-09-10 15:03:42,550 [trainer.py] => init_cls: 30
2023-09-10 15:03:42,550 [trainer.py] => increment: 30
2023-09-10 15:03:42,550 [trainer.py] => model_name: adam_adapter
2023-09-10 15:03:42,550 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 15:03:42,550 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 15:03:42,550 [trainer.py] => seed: 1993
2023-09-10 15:03:42,550 [trainer.py] => tuned_epoch: 1
2023-09-10 15:03:42,550 [trainer.py] => init_lr: 0.02
2023-09-10 15:03:42,550 [trainer.py] => batch_size: 96
2023-09-10 15:03:42,550 [trainer.py] => use_A: False
2023-09-10 15:03:42,550 [trainer.py] => weight_decay: 0.0005
2023-09-10 15:03:42,551 [trainer.py] => min_lr: 0
2023-09-10 15:03:42,551 [trainer.py] => ffn_num: 64
2023-09-10 15:03:42,551 [trainer.py] => optimizer: sgd
2023-09-10 15:03:42,551 [trainer.py] => vpt_type: shallow
2023-09-10 15:03:42,551 [trainer.py] => prompt_token_num: 5
2023-09-10 15:03:43,242 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 15:03:45,675 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:03:46,183 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:03:46,274 [trainer.py] => All params: 86988288
2023-09-10 15:03:46,285 [trainer.py] => Trainable params: 1189632
2023-09-10 15:03:46,436 [adam_adapter.py] => Learning on 0-30
2023-09-10 15:04:50,256 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 15:04:51,486 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:04:51,755 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:04:52,527 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:04:52,794 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:11:25,811 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 15:11:25,812 [trainer.py] => prefix:  
2023-09-10 15:11:25,812 [trainer.py] => dataset: omnibenchmark
2023-09-10 15:11:25,812 [trainer.py] => memory_size: 0
2023-09-10 15:11:25,812 [trainer.py] => memory_per_class: 0
2023-09-10 15:11:25,812 [trainer.py] => fixed_memory: False
2023-09-10 15:11:25,812 [trainer.py] => shuffle: True
2023-09-10 15:11:25,812 [trainer.py] => init_cls: 30
2023-09-10 15:11:25,812 [trainer.py] => increment: 30
2023-09-10 15:11:25,812 [trainer.py] => model_name: adam_adapter
2023-09-10 15:11:25,812 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 15:11:25,812 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 15:11:25,812 [trainer.py] => seed: 1993
2023-09-10 15:11:25,812 [trainer.py] => tuned_epoch: 20
2023-09-10 15:11:25,812 [trainer.py] => init_lr: 0.02
2023-09-10 15:11:25,812 [trainer.py] => batch_size: 96
2023-09-10 15:11:25,812 [trainer.py] => use_A: False
2023-09-10 15:11:25,812 [trainer.py] => weight_decay: 0.0005
2023-09-10 15:11:25,812 [trainer.py] => min_lr: 0
2023-09-10 15:11:25,812 [trainer.py] => ffn_num: 64
2023-09-10 15:11:25,812 [trainer.py] => optimizer: sgd
2023-09-10 15:11:25,812 [trainer.py] => vpt_type: shallow
2023-09-10 15:11:25,812 [trainer.py] => prompt_token_num: 5
2023-09-10 15:11:26,003 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 15:11:27,877 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:11:28,370 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:11:28,441 [trainer.py] => All params: 86988288
2023-09-10 15:11:28,441 [trainer.py] => Trainable params: 1189632
2023-09-10 15:11:28,597 [adam_adapter.py] => Learning on 0-30
2023-09-10 15:12:26,822 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 15:13:26,071 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 0.889, Train_accy 82.64, Test_accy 88.67
2023-09-10 15:14:24,384 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 0.554, Train_accy 85.15, Test_accy 89.83
2023-09-10 15:15:23,073 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 0.489, Train_accy 86.31, Test_accy 87.83
2023-09-10 15:16:22,184 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 0.457, Train_accy 86.68, Test_accy 90.33
2023-09-10 15:17:21,580 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 0.433, Train_accy 87.47, Test_accy 91.00
2023-09-10 15:18:19,945 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 0.410, Train_accy 88.13, Test_accy 91.33
2023-09-10 15:19:18,553 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 0.393, Train_accy 88.22, Test_accy 90.83
2023-09-10 15:20:17,640 [adam_adapter.py] => Task 0, Epoch 9/20 => Loss 0.393, Train_accy 88.73, Test_accy 92.17
2023-09-10 15:21:16,156 [adam_adapter.py] => Task 0, Epoch 10/20 => Loss 0.372, Train_accy 89.00, Test_accy 91.83
2023-09-10 15:22:14,939 [adam_adapter.py] => Task 0, Epoch 11/20 => Loss 0.362, Train_accy 89.23, Test_accy 91.83
2023-09-10 15:23:13,546 [adam_adapter.py] => Task 0, Epoch 12/20 => Loss 0.372, Train_accy 89.00, Test_accy 92.50
2023-09-10 15:24:11,390 [adam_adapter.py] => Task 0, Epoch 13/20 => Loss 0.355, Train_accy 89.64, Test_accy 92.50
2023-09-10 15:25:10,277 [adam_adapter.py] => Task 0, Epoch 14/20 => Loss 0.346, Train_accy 89.99, Test_accy 92.00
2023-09-10 15:26:09,121 [adam_adapter.py] => Task 0, Epoch 15/20 => Loss 0.351, Train_accy 89.86, Test_accy 91.83
2023-09-10 15:27:08,157 [adam_adapter.py] => Task 0, Epoch 16/20 => Loss 0.351, Train_accy 89.71, Test_accy 92.17
2023-09-10 15:28:06,943 [adam_adapter.py] => Task 0, Epoch 17/20 => Loss 0.341, Train_accy 90.28, Test_accy 92.50
2023-09-10 15:29:05,761 [adam_adapter.py] => Task 0, Epoch 18/20 => Loss 0.341, Train_accy 90.06, Test_accy 92.50
2023-09-10 15:30:05,395 [adam_adapter.py] => Task 0, Epoch 19/20 => Loss 0.338, Train_accy 90.49, Test_accy 92.50
2023-09-10 15:31:03,971 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.345, Train_accy 90.13, Test_accy 92.50
2023-09-10 15:31:05,086 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:31:05,583 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:31:06,308 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:31:06,568 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:33:33,867 [adam_adapter.py] => Task 0, Epoch 1/2 => Loss 2.042, Train_accy 85.09, Test_accy 89.17
2023-09-10 15:34:50,295 [adam_adapter.py] => Task 0, Epoch 2/2 => Loss 1.273, Train_accy 85.28, Test_accy 89.17
2023-09-10 15:34:59,439 [trainer.py] => No NME accuracy.
2023-09-10 15:34:59,439 [trainer.py] => CNN: {'total': 89.17, '00-09': 89.0, '10-19': 90.5, '20-29': 88.0, 'old': 0, 'new': 89.17}
2023-09-10 15:34:59,439 [trainer.py] => CNN top1 curve: [89.17]
2023-09-10 15:34:59,440 [trainer.py] => CNN top5 curve: [99.0]

2023-09-10 15:34:59,440 [trainer.py] => Average Accuracy (CNN): 89.17
2023-09-10 15:34:59,442 [trainer.py] => All params: 172833025
2023-09-10 15:34:59,443 [trainer.py] => Trainable params: 1235713
2023-09-10 15:34:59,445 [adam_adapter.py] => Learning on 30-60
2023-09-10 15:40:48,272 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 15:40:48,272 [trainer.py] => prefix:  
2023-09-10 15:40:48,272 [trainer.py] => dataset: omnibenchmark
2023-09-10 15:40:48,272 [trainer.py] => memory_size: 0
2023-09-10 15:40:48,272 [trainer.py] => memory_per_class: 0
2023-09-10 15:40:48,272 [trainer.py] => fixed_memory: False
2023-09-10 15:40:48,272 [trainer.py] => shuffle: True
2023-09-10 15:40:48,272 [trainer.py] => init_cls: 30
2023-09-10 15:40:48,272 [trainer.py] => increment: 30
2023-09-10 15:40:48,272 [trainer.py] => model_name: adam_adapter
2023-09-10 15:40:48,272 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 15:40:48,272 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 15:40:48,272 [trainer.py] => seed: 1993
2023-09-10 15:40:48,272 [trainer.py] => tuned_epoch: 1
2023-09-10 15:40:48,272 [trainer.py] => init_lr: 0.02
2023-09-10 15:40:48,272 [trainer.py] => batch_size: 96
2023-09-10 15:40:48,272 [trainer.py] => use_A: False
2023-09-10 15:40:48,272 [trainer.py] => weight_decay: 0.0005
2023-09-10 15:40:48,272 [trainer.py] => min_lr: 0
2023-09-10 15:40:48,272 [trainer.py] => ffn_num: 64
2023-09-10 15:40:48,272 [trainer.py] => optimizer: sgd
2023-09-10 15:40:48,272 [trainer.py] => vpt_type: shallow
2023-09-10 15:40:48,272 [trainer.py] => prompt_token_num: 5
2023-09-10 15:40:48,465 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 15:40:50,329 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:40:50,833 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:40:50,901 [trainer.py] => All params: 86988288
2023-09-10 15:40:50,902 [trainer.py] => Trainable params: 1189632
2023-09-10 15:40:51,054 [adam_adapter.py] => Learning on 0-30
2023-09-10 15:41:49,587 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 15:41:50,679 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:41:50,939 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:41:51,678 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:41:51,940 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:44:32,598 [adam_adapter.py] => Task 0, Epoch 1/2 => Loss 2.330, Train_accy 84.72, Test_accy 87.17
2023-09-10 15:46:02,906 [adam_adapter.py] => Task 0, Epoch 2/2 => Loss 1.636, Train_accy 84.30, Test_accy 87.17
2023-09-10 15:46:12,041 [trainer.py] => No NME accuracy.
2023-09-10 15:46:12,041 [trainer.py] => CNN: {'total': 87.17, '00-09': 90.5, '10-19': 88.5, '20-29': 82.5, 'old': 0, 'new': 87.17}
2023-09-10 15:46:12,041 [trainer.py] => CNN top1 curve: [87.17]
2023-09-10 15:46:12,041 [trainer.py] => CNN top5 curve: [98.83]

2023-09-10 15:46:12,041 [trainer.py] => Average Accuracy (CNN): 87.17
2023-09-10 15:46:12,043 [trainer.py] => All params: 172833025
2023-09-10 15:46:12,044 [trainer.py] => Trainable params: 1235713
2023-09-10 15:46:12,046 [adam_adapter.py] => Learning on 30-60
2023-09-10 15:48:18,428 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 15:48:18,428 [trainer.py] => prefix:  
2023-09-10 15:48:18,428 [trainer.py] => dataset: omnibenchmark
2023-09-10 15:48:18,428 [trainer.py] => memory_size: 0
2023-09-10 15:48:18,428 [trainer.py] => memory_per_class: 0
2023-09-10 15:48:18,428 [trainer.py] => fixed_memory: False
2023-09-10 15:48:18,428 [trainer.py] => shuffle: True
2023-09-10 15:48:18,428 [trainer.py] => init_cls: 30
2023-09-10 15:48:18,428 [trainer.py] => increment: 30
2023-09-10 15:48:18,428 [trainer.py] => model_name: adam_adapter
2023-09-10 15:48:18,428 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 15:48:18,428 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 15:48:18,428 [trainer.py] => seed: 1993
2023-09-10 15:48:18,428 [trainer.py] => tuned_epoch: 1
2023-09-10 15:48:18,428 [trainer.py] => init_lr: 0.02
2023-09-10 15:48:18,428 [trainer.py] => batch_size: 96
2023-09-10 15:48:18,428 [trainer.py] => use_A: False
2023-09-10 15:48:18,428 [trainer.py] => weight_decay: 0.0005
2023-09-10 15:48:18,428 [trainer.py] => min_lr: 0
2023-09-10 15:48:18,428 [trainer.py] => ffn_num: 64
2023-09-10 15:48:18,429 [trainer.py] => optimizer: sgd
2023-09-10 15:48:18,429 [trainer.py] => vpt_type: shallow
2023-09-10 15:48:18,429 [trainer.py] => prompt_token_num: 5
2023-09-10 15:48:18,625 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 15:48:20,523 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:48:21,022 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:48:21,096 [trainer.py] => All params: 86988288
2023-09-10 15:48:21,096 [trainer.py] => Trainable params: 1189632
2023-09-10 15:48:21,257 [adam_adapter.py] => Learning on 0-30
2023-09-10 15:49:19,979 [adam_adapter.py] => Task 0, Epoch 1/1 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 15:49:21,324 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:49:21,575 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:49:22,304 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 15:49:22,571 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 15:51:49,783 [adam_adapter.py] => Task 0, Epoch 1/2 => Loss 2.330, Train_accy 84.73, Test_accy 87.17
2023-09-10 15:53:06,021 [adam_adapter.py] => Task 0, Epoch 2/2 => Loss 1.636, Train_accy 84.30, Test_accy 87.33
2023-09-10 15:53:15,069 [trainer.py] => No NME accuracy.
2023-09-10 15:53:15,069 [trainer.py] => CNN: {'total': 87.33, '00-09': 90.5, '10-19': 88.5, '20-29': 83.0, 'old': 0, 'new': 87.33}
2023-09-10 15:53:15,069 [trainer.py] => CNN top1 curve: [87.33]
2023-09-10 15:53:15,069 [trainer.py] => CNN top5 curve: [98.83]

2023-09-10 15:53:15,070 [trainer.py] => Average Accuracy (CNN): 87.33
2023-09-10 15:53:15,071 [trainer.py] => All params: 172833025
2023-09-10 15:53:15,073 [trainer.py] => Trainable params: 1235713
2023-09-10 15:53:15,075 [adam_adapter.py] => Learning on 30-60
2023-09-10 15:55:49,943 [adam_adapter.py] => Task 1, Epoch 1/2 => Loss 1.606, Train_accy 88.26, Test_accy 81.15
2023-09-10 15:57:13,143 [adam_adapter.py] => Task 1, Epoch 2/2 => Loss 1.150, Train_accy 88.84, Test_accy 81.23
2023-09-10 15:57:29,285 [trainer.py] => No NME accuracy.
2023-09-10 15:57:29,286 [trainer.py] => CNN: {'total': 81.23, '00-09': 70.5, '10-19': 81.5, '20-29': 56.5, '30-39': 90.5, '40-49': 94.5, '50-59': 93.97, 'old': 69.5, 'new': 92.99}
2023-09-10 15:57:29,286 [trainer.py] => CNN top1 curve: [87.33, 81.23]
2023-09-10 15:57:29,286 [trainer.py] => CNN top5 curve: [98.83, 98.08]

2023-09-10 15:57:29,286 [trainer.py] => Average Accuracy (CNN): 84.28
2023-09-10 15:57:29,287 [trainer.py] => All params: 172879105
2023-09-10 15:57:29,289 [trainer.py] => Trainable params: 1281793
2023-09-10 15:57:29,291 [adam_adapter.py] => Learning on 60-90
2023-09-10 16:00:05,905 [trainer.py] => config: ./exps/adam_adapter.json
2023-09-10 16:00:05,905 [trainer.py] => prefix:  
2023-09-10 16:00:05,905 [trainer.py] => dataset: omnibenchmark
2023-09-10 16:00:05,905 [trainer.py] => memory_size: 0
2023-09-10 16:00:05,905 [trainer.py] => memory_per_class: 0
2023-09-10 16:00:05,905 [trainer.py] => fixed_memory: False
2023-09-10 16:00:05,905 [trainer.py] => shuffle: True
2023-09-10 16:00:05,905 [trainer.py] => init_cls: 30
2023-09-10 16:00:05,905 [trainer.py] => increment: 30
2023-09-10 16:00:05,905 [trainer.py] => model_name: adam_adapter
2023-09-10 16:00:05,905 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_adapter
2023-09-10 16:00:05,905 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-09-10 16:00:05,905 [trainer.py] => seed: 1993
2023-09-10 16:00:05,905 [trainer.py] => tuned_epoch: 20
2023-09-10 16:00:05,905 [trainer.py] => init_lr: 0.02
2023-09-10 16:00:05,905 [trainer.py] => batch_size: 96
2023-09-10 16:00:05,905 [trainer.py] => use_A: False
2023-09-10 16:00:05,905 [trainer.py] => weight_decay: 0.0005
2023-09-10 16:00:05,905 [trainer.py] => min_lr: 0
2023-09-10 16:00:05,905 [trainer.py] => ffn_num: 64
2023-09-10 16:00:05,905 [trainer.py] => optimizer: sgd
2023-09-10 16:00:05,905 [trainer.py] => vpt_type: shallow
2023-09-10 16:00:05,905 [trainer.py] => prompt_token_num: 5
2023-09-10 16:00:06,096 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-09-10 16:00:07,998 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 16:00:08,510 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 16:00:08,578 [trainer.py] => All params: 86988288
2023-09-10 16:00:08,578 [trainer.py] => Trainable params: 1189632
2023-09-10 16:00:08,717 [adam_adapter.py] => Learning on 0-30
2023-09-10 16:00:48,647 [adam_adapter.py] => Task 0, Epoch 1/20 => Loss 2.867, Train_accy 59.45, Test_accy 80.83
2023-09-10 16:01:29,709 [adam_adapter.py] => Task 0, Epoch 2/20 => Loss 0.889, Train_accy 82.64, Test_accy 88.67
2023-09-10 16:02:08,961 [adam_adapter.py] => Task 0, Epoch 3/20 => Loss 0.554, Train_accy 85.15, Test_accy 89.83
2023-09-10 16:02:48,447 [adam_adapter.py] => Task 0, Epoch 4/20 => Loss 0.489, Train_accy 86.31, Test_accy 87.83
2023-09-10 16:03:28,266 [adam_adapter.py] => Task 0, Epoch 5/20 => Loss 0.457, Train_accy 86.68, Test_accy 90.33
2023-09-10 16:04:07,966 [adam_adapter.py] => Task 0, Epoch 6/20 => Loss 0.433, Train_accy 87.47, Test_accy 91.00
2023-09-10 16:04:47,522 [adam_adapter.py] => Task 0, Epoch 7/20 => Loss 0.410, Train_accy 88.13, Test_accy 91.33
2023-09-10 16:05:27,136 [adam_adapter.py] => Task 0, Epoch 8/20 => Loss 0.393, Train_accy 88.22, Test_accy 90.83
2023-09-10 16:06:06,831 [adam_adapter.py] => Task 0, Epoch 9/20 => Loss 0.393, Train_accy 88.73, Test_accy 92.17
2023-09-10 16:06:46,535 [adam_adapter.py] => Task 0, Epoch 10/20 => Loss 0.372, Train_accy 89.00, Test_accy 91.83
2023-09-10 16:07:26,115 [adam_adapter.py] => Task 0, Epoch 11/20 => Loss 0.362, Train_accy 89.23, Test_accy 91.83
2023-09-10 16:08:05,741 [adam_adapter.py] => Task 0, Epoch 12/20 => Loss 0.372, Train_accy 89.00, Test_accy 92.50
2023-09-10 16:08:45,396 [adam_adapter.py] => Task 0, Epoch 13/20 => Loss 0.355, Train_accy 89.64, Test_accy 92.50
2023-09-10 16:09:25,064 [adam_adapter.py] => Task 0, Epoch 14/20 => Loss 0.346, Train_accy 89.99, Test_accy 92.00
2023-09-10 16:10:04,733 [adam_adapter.py] => Task 0, Epoch 15/20 => Loss 0.351, Train_accy 89.86, Test_accy 91.83
2023-09-10 16:10:44,381 [adam_adapter.py] => Task 0, Epoch 16/20 => Loss 0.351, Train_accy 89.71, Test_accy 92.17
2023-09-10 16:11:24,123 [adam_adapter.py] => Task 0, Epoch 17/20 => Loss 0.341, Train_accy 90.28, Test_accy 92.50
2023-09-10 16:12:03,717 [adam_adapter.py] => Task 0, Epoch 18/20 => Loss 0.341, Train_accy 90.06, Test_accy 92.50
2023-09-10 16:12:43,327 [adam_adapter.py] => Task 0, Epoch 19/20 => Loss 0.338, Train_accy 90.49, Test_accy 92.50
2023-09-10 16:13:23,027 [adam_adapter.py] => Task 0, Epoch 20/20 => Loss 0.345, Train_accy 90.13, Test_accy 92.50
2023-09-10 16:13:24,128 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 16:13:24,620 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 16:13:25,324 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-09-10 16:13:25,586 [_hub.py] => [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-09-10 16:15:18,198 [adam_adapter.py] => Task 0, Epoch 1/2 => Loss 2.042, Train_accy 85.09, Test_accy 89.17
2023-09-10 16:16:21,006 [adam_adapter.py] => Task 0, Epoch 2/2 => Loss 1.273, Train_accy 85.28, Test_accy 89.17
2023-09-10 16:16:25,390 [trainer.py] => No NME accuracy.
2023-09-10 16:16:25,390 [trainer.py] => CNN: {'total': 89.17, '00-09': 89.0, '10-19': 90.5, '20-29': 88.0, 'old': 0, 'new': 89.17}
2023-09-10 16:16:25,390 [trainer.py] => CNN top1 curve: [89.17]
2023-09-10 16:16:25,390 [trainer.py] => CNN top5 curve: [99.0]

2023-09-10 16:16:25,390 [trainer.py] => Average Accuracy (CNN): 89.17
2023-09-10 16:16:25,392 [trainer.py] => All params: 172833025
2023-09-10 16:16:25,393 [trainer.py] => Trainable params: 1235713
2023-09-10 16:16:25,395 [adam_adapter.py] => Learning on 30-60
2023-09-10 16:18:08,375 [adam_adapter.py] => Task 1, Epoch 1/2 => Loss 1.352, Train_accy 88.58, Test_accy 82.57
2023-09-10 16:19:13,553 [adam_adapter.py] => Task 1, Epoch 2/2 => Loss 1.000, Train_accy 89.00, Test_accy 82.57
2023-09-10 16:19:20,996 [trainer.py] => No NME accuracy.
2023-09-10 16:19:20,997 [trainer.py] => CNN: {'total': 82.57, '00-09': 70.5, '10-19': 83.5, '20-29': 58.0, '30-39': 92.5, '40-49': 95.5, '50-59': 95.48, 'old': 70.67, 'new': 94.49}
2023-09-10 16:19:20,997 [trainer.py] => CNN top1 curve: [89.17, 82.57]
2023-09-10 16:19:20,997 [trainer.py] => CNN top5 curve: [99.0, 98.58]

2023-09-10 16:19:20,997 [trainer.py] => Average Accuracy (CNN): 85.87
2023-09-10 16:19:20,999 [trainer.py] => All params: 172879105
2023-09-10 16:19:21,000 [trainer.py] => Trainable params: 1281793
2023-09-10 16:19:21,002 [adam_adapter.py] => Learning on 60-90
2023-09-10 16:21:04,143 [adam_adapter.py] => Task 2, Epoch 1/2 => Loss 1.040, Train_accy 88.23, Test_accy 75.75
2023-09-10 16:22:10,511 [adam_adapter.py] => Task 2, Epoch 2/2 => Loss 0.858, Train_accy 88.76, Test_accy 75.86
2023-09-10 16:22:21,140 [trainer.py] => No NME accuracy.
2023-09-10 16:22:21,140 [trainer.py] => CNN: {'total': 75.86, '00-09': 58.0, '10-19': 60.0, '20-29': 54.5, '30-39': 70.5, '40-49': 74.5, '50-59': 84.42, '60-69': 98.0, '70-79': 95.0, '80-89': 87.94, 'old': 66.97, 'new': 93.66}
2023-09-10 16:22:21,140 [trainer.py] => CNN top1 curve: [89.17, 82.57, 75.86]
2023-09-10 16:22:21,140 [trainer.py] => CNN top5 curve: [99.0, 98.58, 91.38]

2023-09-10 16:22:21,140 [trainer.py] => Average Accuracy (CNN): 82.53333333333335
2023-09-10 16:22:21,141 [trainer.py] => All params: 172925185
2023-09-10 16:22:21,142 [trainer.py] => Trainable params: 1327873
2023-09-10 16:22:21,143 [adam_adapter.py] => Learning on 90-120
2023-09-10 16:24:10,177 [adam_adapter.py] => Task 3, Epoch 1/2 => Loss 0.967, Train_accy 86.76, Test_accy 66.89
2023-09-10 16:25:20,959 [adam_adapter.py] => Task 3, Epoch 2/2 => Loss 0.851, Train_accy 86.88, Test_accy 66.97
2023-09-10 16:25:34,736 [trainer.py] => No NME accuracy.
2023-09-10 16:25:34,736 [trainer.py] => CNN: {'total': 66.97, '00-09': 51.0, '10-19': 47.0, '20-29': 53.5, '30-39': 54.0, '40-49': 53.0, '50-59': 64.32, '60-69': 69.5, '70-79': 75.0, '80-89': 67.34, '90-99': 88.44, '100-109': 88.94, '110-119': 91.96, 'old': 59.4, 'new': 89.78}
2023-09-10 16:25:34,736 [trainer.py] => CNN top1 curve: [89.17, 82.57, 75.86, 66.97]
2023-09-10 16:25:34,736 [trainer.py] => CNN top5 curve: [99.0, 98.58, 91.38, 90.02]

2023-09-10 16:25:34,736 [trainer.py] => Average Accuracy (CNN): 78.64250000000001
2023-09-10 16:25:34,737 [trainer.py] => All params: 172971265
2023-09-10 16:25:34,738 [trainer.py] => Trainable params: 1373953
2023-09-10 16:25:34,739 [adam_adapter.py] => Learning on 120-150
2023-09-10 16:27:26,308 [adam_adapter.py] => Task 4, Epoch 1/2 => Loss 0.895, Train_accy 87.00, Test_accy 60.39
2023-09-10 16:28:40,037 [adam_adapter.py] => Task 4, Epoch 2/2 => Loss 0.795, Train_accy 87.19, Test_accy 60.45
2023-09-10 16:28:57,689 [trainer.py] => No NME accuracy.
2023-09-10 16:28:57,690 [trainer.py] => CNN: {'total': 60.45, '00-09': 38.0, '10-19': 41.5, '20-29': 47.5, '30-39': 44.0, '40-49': 46.0, '50-59': 49.75, '60-69': 43.0, '70-79': 66.0, '80-89': 52.26, '90-99': 79.9, '100-109': 67.84, '110-119': 57.29, '120-129': 90.5, '130-139': 92.0, '140-149': 91.46, 'old': 52.73, 'new': 91.32}
2023-09-10 16:28:57,690 [trainer.py] => CNN top1 curve: [89.17, 82.57, 75.86, 66.97, 60.45]
2023-09-10 16:28:57,690 [trainer.py] => CNN top5 curve: [99.0, 98.58, 91.38, 90.02, 85.0]

2023-09-10 16:28:57,690 [trainer.py] => Average Accuracy (CNN): 75.004
2023-09-10 16:28:57,691 [trainer.py] => All params: 173017345
2023-09-10 16:28:57,693 [trainer.py] => Trainable params: 1420033
2023-09-10 16:28:57,695 [adam_adapter.py] => Learning on 150-180
2023-09-10 16:30:48,928 [adam_adapter.py] => Task 5, Epoch 1/2 => Loss 0.875, Train_accy 84.42, Test_accy 55.51
2023-09-10 16:32:03,334 [adam_adapter.py] => Task 5, Epoch 2/2 => Loss 0.809, Train_accy 84.18, Test_accy 55.48
2023-09-10 16:32:23,937 [trainer.py] => No NME accuracy.
2023-09-10 16:32:23,937 [trainer.py] => CNN: {'total': 55.48, '00-09': 38.0, '10-19': 31.5, '20-29': 51.0, '30-39': 50.0, '40-49': 39.0, '50-59': 39.7, '60-69': 47.0, '70-79': 52.0, '80-89': 35.18, '90-99': 70.35, '100-109': 45.73, '110-119': 29.15, '120-129': 71.0, '130-139': 76.5, '140-149': 62.31, '150-159': 84.42, '160-169': 85.93, '170-179': 90.0, 'old': 49.23, 'new': 86.79}
2023-09-10 16:32:23,937 [trainer.py] => CNN top1 curve: [89.17, 82.57, 75.86, 66.97, 60.45, 55.48]
2023-09-10 16:32:23,938 [trainer.py] => CNN top5 curve: [99.0, 98.58, 91.38, 90.02, 85.0, 80.85]

2023-09-10 16:32:23,938 [trainer.py] => Average Accuracy (CNN): 71.75000000000001
2023-09-10 16:32:23,939 [trainer.py] => All params: 173063425
2023-09-10 16:32:23,939 [trainer.py] => Trainable params: 1466113
2023-09-10 16:32:23,941 [adam_adapter.py] => Learning on 180-210
2023-09-10 16:34:17,277 [adam_adapter.py] => Task 6, Epoch 1/2 => Loss 0.732, Train_accy 86.13, Test_accy 52.45
2023-09-10 16:35:33,936 [adam_adapter.py] => Task 6, Epoch 2/2 => Loss 0.683, Train_accy 85.86, Test_accy 52.59
2023-09-10 16:35:57,954 [trainer.py] => No NME accuracy.
2023-09-10 16:35:57,954 [trainer.py] => CNN: {'total': 52.59, '00-09': 43.5, '10-19': 52.0, '20-29': 58.0, '30-39': 47.0, '40-49': 40.5, '50-59': 50.25, '60-69': 36.0, '70-79': 40.5, '80-89': 32.66, '90-99': 40.7, '100-109': 32.16, '110-119': 15.58, '120-129': 49.0, '130-139': 55.5, '140-149': 54.77, '150-159': 66.83, '160-169': 59.8, '170-179': 62.5, '180-189': 93.94, '190-199': 90.5, '200-209': 82.91, 'old': 46.52, 'new': 89.11}
2023-09-10 16:35:57,954 [trainer.py] => CNN top1 curve: [89.17, 82.57, 75.86, 66.97, 60.45, 55.48, 52.59]
2023-09-10 16:35:57,954 [trainer.py] => CNN top5 curve: [99.0, 98.58, 91.38, 90.02, 85.0, 80.85, 76.46]

2023-09-10 16:35:57,954 [trainer.py] => Average Accuracy (CNN): 69.01285714285714
2023-09-10 16:35:57,955 [trainer.py] => All params: 173109505
2023-09-10 16:35:57,956 [trainer.py] => Trainable params: 1512193
2023-09-10 16:35:57,958 [adam_adapter.py] => Learning on 210-240
2023-09-10 16:37:58,212 [adam_adapter.py] => Task 7, Epoch 1/2 => Loss 0.783, Train_accy 82.14, Test_accy 45.36
2023-09-10 16:39:20,089 [adam_adapter.py] => Task 7, Epoch 2/2 => Loss 0.751, Train_accy 82.30, Test_accy 44.99
2023-09-10 16:40:07,521 [trainer.py] => No NME accuracy.
2023-09-10 16:40:07,521 [trainer.py] => CNN: {'total': 44.99, '00-09': 60.0, '10-19': 51.0, '20-29': 60.0, '30-39': 51.5, '40-49': 31.5, '50-59': 43.22, '60-69': 32.0, '70-79': 36.5, '80-89': 25.63, '90-99': 33.67, '100-109': 15.58, '110-119': 7.54, '120-129': 29.0, '130-139': 25.0, '140-149': 29.65, '150-159': 60.8, '160-169': 25.13, '170-179': 15.0, '180-189': 55.05, '190-199': 74.0, '200-209': 55.78, '210-219': 72.5, '220-229': 98.99, '230-239': 90.5, 'old': 38.94, 'new': 87.31}
2023-09-10 16:40:07,521 [trainer.py] => CNN top1 curve: [89.17, 82.57, 75.86, 66.97, 60.45, 55.48, 52.59, 44.99]
2023-09-10 16:40:07,521 [trainer.py] => CNN top5 curve: [99.0, 98.58, 91.38, 90.02, 85.0, 80.85, 76.46, 73.04]

2023-09-10 16:40:07,521 [trainer.py] => Average Accuracy (CNN): 66.01
2023-09-10 16:40:07,523 [trainer.py] => All params: 173155585
2023-09-10 16:40:07,524 [trainer.py] => Trainable params: 1558273
2023-09-10 16:40:07,530 [adam_adapter.py] => Learning on 240-270
2023-09-10 16:43:54,382 [adam_adapter.py] => Task 8, Epoch 1/2 => Loss 0.521, Train_accy 89.37, Test_accy 43.62
2023-09-10 16:45:18,154 [adam_adapter.py] => Task 8, Epoch 2/2 => Loss 0.484, Train_accy 89.55, Test_accy 43.62
2023-09-10 16:45:47,725 [trainer.py] => No NME accuracy.
2023-09-10 16:45:47,725 [trainer.py] => CNN: {'total': 43.62, '00-09': 60.0, '10-19': 42.5, '20-29': 59.0, '30-39': 53.5, '40-49': 43.5, '50-59': 52.76, '60-69': 31.5, '70-79': 44.5, '80-89': 34.67, '90-99': 37.69, '100-109': 12.56, '110-119': 5.03, '120-129': 12.5, '130-139': 13.5, '140-149': 18.59, '150-159': 24.62, '160-169': 11.56, '170-179': 9.0, '180-189': 43.94, '190-199': 51.0, '200-209': 35.18, '210-219': 54.0, '220-229': 70.35, '230-239': 76.5, '240-249': 87.5, '250-259': 95.48, '260-269': 96.5, 'old': 37.43, 'new': 93.16}
2023-09-10 16:45:47,725 [trainer.py] => CNN top1 curve: [89.17, 82.57, 75.86, 66.97, 60.45, 55.48, 52.59, 44.99, 43.62]
2023-09-10 16:45:47,725 [trainer.py] => CNN top5 curve: [99.0, 98.58, 91.38, 90.02, 85.0, 80.85, 76.46, 73.04, 70.69]

2023-09-10 16:45:47,725 [trainer.py] => Average Accuracy (CNN): 63.522222222222226
2023-09-10 16:45:47,727 [trainer.py] => All params: 173201665
2023-09-10 16:45:47,728 [trainer.py] => Trainable params: 1604353
2023-09-10 16:45:47,732 [adam_adapter.py] => Learning on 270-300
2023-09-10 16:47:45,801 [adam_adapter.py] => Task 9, Epoch 1/2 => Loss 0.493, Train_accy 90.00, Test_accy 42.34
2023-09-10 16:49:08,039 [adam_adapter.py] => Task 9, Epoch 2/2 => Loss 0.461, Train_accy 90.15, Test_accy 42.17
2023-09-10 16:49:41,583 [trainer.py] => No NME accuracy.
2023-09-10 16:49:41,583 [trainer.py] => CNN: {'total': 42.17, '00-09': 51.0, '10-19': 41.0, '20-29': 43.5, '30-39': 51.5, '40-49': 40.0, '50-59': 59.3, '60-69': 43.5, '70-79': 46.5, '80-89': 47.24, '90-99': 34.67, '100-109': 18.59, '110-119': 25.63, '120-129': 15.5, '130-139': 25.5, '140-149': 15.08, '150-159': 15.58, '160-169': 2.51, '170-179': 4.0, '180-189': 18.18, '190-199': 16.0, '200-209': 14.57, '210-219': 24.0, '220-229': 65.33, '230-239': 50.0, '240-249': 54.5, '250-259': 77.89, '260-269': 80.5, '270-279': 96.48, '280-289': 92.5, '290-299': 94.47, 'old': 36.37, 'new': 94.48}
2023-09-10 16:49:41,583 [trainer.py] => CNN top1 curve: [89.17, 82.57, 75.86, 66.97, 60.45, 55.48, 52.59, 44.99, 43.62, 42.17]
2023-09-10 16:49:41,583 [trainer.py] => CNN top5 curve: [99.0, 98.58, 91.38, 90.02, 85.0, 80.85, 76.46, 73.04, 70.69, 70.36]

2023-09-10 16:49:41,583 [trainer.py] => Average Accuracy (CNN): 61.387
